# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mVLr7AM1dwoGIk4TUWnhkT7ggFcoTrN0

**[Milestone 1](https://)**

**Project Name : Agriculture Crop Yield**

**The dataset**
"""

import pandas as pd
df = pd.read_csv('crop_yield.csv')

"""**At least 10 columns with varying data types**

**At least 100 rows (data points).**
"""

display(df.head(100))

df.info()

df.sample(10)

df.describe()

df.sample(10)

"""**[Milestone 2](https://)**

**Sampling Assignment**

**Implementing Probability Sampling Methods in Python**

Instructions

Upload your dataset (minimum 200 rows), then complete all parts A–F.

**Part A — Setup**
**Report dataset size (rows, columns)**
"""

import pandas as pd
import numpy as np


df = pd.read_csv('/content/crop_yield.csv')
df.head(10)

print("Dataset size:", df.shape)
population_mean = df['Temperature_Celsius'].mean()

"""**Part B — Simple Random Sampling (20 points)**"""

import pandas as pd
import numpy as np

df = pd.read_csv('/content/crop_yield.csv')
df.head(50)

sample_df = df.sample(n=50, random_state=42)
sample_mean = sample_df['Temperature_Celsius'].mean()


print(f"Population Mean (Temperature_Celsius): {population_mean:.2f}")
print(f"Sample Mean (Temperature_Celsius, n=50): {sample_mean:.2f}")

"""
**Part B — Simple Random Sampling**
"""

sample_size = 50
srs = df.sample(n=sample_size, random_state=42)
display(srs.head())
population_mean = df['Temperature_Celsius'].mean()
print("Population mean:", population_mean)
srs_mean = srs['Temperature_Celsius'].mean()
print("Sample mean:", srs_mean)

"""**Part C — Systematic Sampling**"""

n = 50
k = len(df) // n
start = np.random.randint(0, k)
sys_sample = df.iloc[start::k][:n]
display(sys_sample.head())
sys_mean = sys_sample['Temperature_Celsius'].mean()
print ("\n")
print("Sample mean:", sys_mean)

"""**Part D — Stratified Sampling**"""

strata_col = "Region"  # Corrected to an existing column
sample_size = 50

# proportional fraction for each group
frac = sample_size / len(df)

# stratified sample
stratified_sample = df.groupby(strata_col, group_keys=False).sample(frac=frac, random_state=42)

display(stratified_sample.head())
strat_mean = stratified_sample['Temperature_Celsius'].mean() # Corrected column name

print ("\n")
print("Sample mean:", strat_mean)

"""**Part E — Cluster Sampling**"""

df['cluster_id'] = df.index // (len(df)//10)  # 10 clusters
selected_clusters = np.random.choice(df['cluster_id'].unique(), size=2, replace=False)
cluster_sample = df[df['cluster_id'].isin(selected_clusters)]
print("Selected clusters:", selected_clusters)
display(cluster_sample.head())
cluster_mean = cluster_sample['Temperature_Celsius'].mean() # Corrected column name

print ("\n")
print("Sample mean:", cluster_mean)

"""
**Part F — Comparison & Reflection**


Compare sample means vs population mean, then write your reflection."""

df['cluster_id'] = df.index // (len(df)//10)  # 10 clusters
selected_clusters = np.random.choice(df['cluster_id'].unique(), size=2, replace=False)
cluster_sample = df[df['cluster_id'].isin(selected_clusters)]
print("Selected clusters:", selected_clusters)
cluster_sample.head()

comparison = pd.DataFrame({
    'Method': ['Simple Random', 'Systematic', 'Stratified', 'Cluster'],
    'Sample Mean': [srs_mean, sys_mean, strat_mean, cluster_mean],
    'Population Mean': [population_mean]*4,
    'Difference': [(srs_mean - population_mean),
                   (sys_mean - population_mean),
                   (strat_mean - population_mean),
                   (cluster_mean - population_mean)]
})
print(comparison)

"""**Part F — Comparison & Reflection**"""

print ("Compare sample means vs population mean, then write your reflection.")

print ("In this milestone, I applied four probability sampling methods to the\nAgricultureCrop Yield dataset from Kaggle, which includes crop production\ndata across multiple countries.\nThe goal was to compare Simple Random Sampling, Systematic Sampling,\nStratified Sampling and Cluster Sampling in estimating the population mean of\ncrop yield, which was 32.337344 t/ha.\nStratified sampling produced the most accurate result with a mean of 32.3276\nt/ha, as proportional allocation preserved the distribution of crop types and\nregions.\nSimple Random Sampling yielded 32.25 t/ha, slightly lower, while systematic\nsampling gave 32.3872 t/ha, slightly higher.\nCluster sampling showed the largest deviation at 32.5075 t/ha due to\npotential homogeneity within clusters.\nIn terms of implementation, Simple Random Sampling was easiest, requiring\nminimal code.\nSystematic sampling was straightforward with a defined step size, while\nstratified sampling needed careful grouping.\nCluster sampling was simple but required thoughtful cluster selection.")

print("Overall, stratified sampling ensured maximum accuracy, and Simple Random\nSam- pling was the simplest to implement.")

"""[**Milestone 3**](https://)


**Frequency Distribution and Data Visualization**

**Part 1: Data Loading and Preparation**


In this section, we will load the dataset and import the necessary libraries for our analysis.

Note: Remove the HASHTAG in the next cell to allow installation of the packages required [IF NEEDED]
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Dataset
file_path = '/content/crop_yield.csv'
df = pd.read_csv(file_path)

# Dataset
print("Dataset Shape:", df.shape)
print("\nColumns in Dataset:\n", df.columns)
print("\nFirst 5 Rows:\n", df.head())
print("\nMissing Values:\n", df.isnull().sum())

import pandas as pd
import numpy as np

df = pd.read_csv('/content/crop_yield.csv')

df.head()

"""**Part 2: Frequency Distribution Table**

Here, we will select a column and construct a frequency distribution table.
"""

import pandas as pd
import numpy as np
df = pd.read_csv('/content/crop_yield.csv')
column_to_analyze = 'Crop'
freq_table = pd.DataFrame(df[column_to_analyze].value_counts()).reset_index()
freq_table.columns = ['Class/Category', 'Frequency (f)']
freq_table = freq_table.sort_values(by='Class/Category').reset_index(drop=True)
total_count = freq_table['Frequency (f)'].sum()
freq_table['Relative Frequency (rf)'] = freq_table['Frequency (f)'] / total_count
freq_table['Cumulative Frequency (cf)'] = freq_table['Frequency (f)'].cumsum()
freq_table['Relative Cumulative Frequency (rcf)'] = freq_table['Relative Frequency (rf)'].cumsum()
print(" Frequency Distribution Table for:", column_to_analyze)
freq_table

column_to_analyze = 'column_name'

column_to_analyze = 'Yield_tons_per_hectare'
freq_table = pd.DataFrame(df[column_to_analyze].value_counts()).reset_index()
freq_table.columns = ['Class/Category', 'Frequency (f)']
freq_table = freq_table.sort_values(by='Class/Category').reset_index(drop=True)


total_count = freq_table['Frequency (f)'].sum()
freq_table['Relative Frequency (rf)'] = freq_table['Frequency (f)'] / total_count

freq_table['Cumulative Frequency (cf)'] = freq_table['Frequency (f)'].cumsum()


freq_table['Relative Cumulative Frequency (rcf)'] = freq_table['Relative Frequency (rf)'].cumsum()



print("Frequency Distribution Table for '" + column_to_analyze + "'")
freq_table

"""**Part 3: Graphical Representation**


In this section, we will visualize the data distribution using various charts.
"""

df.sample(10)

"""**Bar Chat**"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

df = pd.read_csv('/content/crop_yield.csv')

sample_20_df = df.sample(n=10, random_state=20)
print("First 5 rows of the 20-row sample:")
display(sample_20_df.head())


crop_counts_sample = sample_20_df['Crop'].value_counts().reset_index()
crop_counts_sample.columns = ['Crop Type', 'Frequency']


plt.figure(figsize=(10, 6))
sns.barplot(x='Crop Type', y='Frequency', data=crop_counts_sample, palette='viridis', hue='Crop Type', legend=False)
plt.title('Bar Chat of Yield Frequency Distribution of Crop Types (20-Row Sample)')
plt.xlabel('Crop Type')
plt.ylabel('Frequency')
plt.xticks(rotation=40, ha='right')
plt.grid(axis='y', alpha=0.7)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd # Ensure pandas is imported

df = pd.read_csv('/content/crop_yield.csv') # Reload the original DataFrame

sample_30_df = df.sample(n=30, random_state=20)
print("First 5 rows of the 30-row sample:")
display(sample_30_df.head())


plt.figure(figsize=(10, 6))
sns.histplot(sample_30_df['Yield_tons_per_hectare'], kde=True, bins=5, color='skyblue')
plt.title('Histogram of Yield (tons per hectare) (30-Row Sample)')
plt.xlabel('Yield (tons per hectare)')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.7)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

df = pd.read_csv('/content/crop_yield.csv')
sample_30_df = df.sample(n=30, random_state=20)
print("First 5 rows of the 30-row sample:")
display(sample_30_df.head())


plt.figure(figsize=(10, 6))
sns.ecdfplot(data=sample_30_df, x='Yield_tons_per_hectare', color='skyblue')
plt.title('Ogive Chart (Cumulative Frequency) of Yield (tons per hectare) (30-Row Sample)')
plt.xlabel('Yield (tons per hectare)')
plt.ylabel('Cumulative Frequency / Proportion')
plt.grid(axis='both', alpha=0.7)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np



hist, bin_edges = np.histogram(sample_30_df['Yield_tons_per_hectare'], bins=5)


bin_midpoints = (bin_edges[:-1] + bin_edges[1:]) / 2


plt.figure(figsize=(10, 6))
plt.plot(bin_midpoints, hist, marker='o', linestyle='-', color='purple')
plt.title('Frequency Polygon of Yield (tons per hectare) (30-Row Sample)')
plt.xlabel('Yield (tons per hectare)')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.7)
plt.tight_layout()
plt.show()

"""**Rainfall**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


rainfall_data = {
    'Rainfall_mm': [347.733856, 191.661333, 985.486244, 230.494966, 944.241902],
    'Region': ['South', 'North', 'East', 'West', 'North'],
    'Crop': ['Maize', 'Wheat', 'Rice', 'Rice', 'Barley']
}

df = pd.DataFrame(rainfall_data)

print("Rainfall Table:")
print("=" * 45)
print(df[['Rainfall_mm', 'Region', 'Crop']].to_string(index=False))
print("=" * 45)


# Ensure df_full is loaded (assuming it should be the crop_yield.csv dataset)
df_full = pd.read_csv('/content/crop_yield.csv')

# Data to be plotted is Rainfall_mm from the full dataset
temp_data = df_full['Rainfall_mm'] # Renamed from temp_data to avoid confusion with actual temperature

plt.figure(figsize=(12, 8))


plt.subplot(2, 2, 1) # Standardized to 2x2 grid
plt.hist(temp_data, bins=20, color='lightblue', edgecolor='black')
plt.title('1. Histogram (Rainfall)')
plt.xlabel('Rainfall (mm)') # Corrected label
plt.ylabel('Frequency')

plt.subplot(2, 2, 2) # Standardized to 2x2 grid
sorted_data = np.sort(temp_data)
cumulative = np.arange(1, len(sorted_data) + 1)
plt.plot(sorted_data, cumulative, 'r-')
plt.title('2. Ogive Chart (Rainfall)')
plt.xlabel('Rainfall (mm)') # Corrected label
plt.ylabel('Cumulative Frequency')


plt.subplot(2, 2, 3) # Standardized to 2x2 grid (moved to next position)
counts, bin_edges = np.histogram(temp_data, bins=20)
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
plt.plot(bin_centers, counts, 'g-o')
plt.title('3. Frequency Polygon (Rainfall)')
plt.xlabel('Rainfall (mm)') # Corrected label
plt.ylabel('Frequency')

# Adding a placeholder for the 4th plot, or adjust layout if only 3 plots are desired
# For now, let's keep 4 slots and leave the 4th empty or add a simple plot
plt.subplot(2, 2, 4) # Added for completeness of 2x2 grid
plt.title('4. Placeholder')
plt.axis('off') # Turn off axes for empty plot


plt.tight_layout()
plt.show()

"""**Temperature Celsius**"""

import matplotlib.pyplot as plt
import numpy as np


temp_data = df_full['Temperature_Celsius']

plt.figure(figsize=(12, 8))


plt.subplot(2, 2, 1)
plt.hist(temp_data, bins=20, color='lightblue', edgecolor='black')
plt.title('1. Histogram')
plt.xlabel('Temperature (°C)')
plt.ylabel('Frequency')

plt.subplot(2, 2, 2)
sorted_data = np.sort(temp_data)
cumulative = np.arange(1, len(sorted_data) + 1)
plt.plot(sorted_data, cumulative, 'r-')
plt.title('2. Ogive Chart')
plt.xlabel('Temperature (°C)')
plt.ylabel('Cumulative Frequency')


plt.subplot(2, 2, 3)
counts, bin_edges = np.histogram(temp_data, bins=20)
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
plt.plot(bin_centers, counts, 'g-o')
plt.title('3. Frequency Polygon')
plt.xlabel('Temperature (°C)')
plt.ylabel('Frequency')


plt.subplot(2, 2, 4)
more_than = np.arange(len(sorted_data), 0, -1)
plt.plot(sorted_data, more_than, 'b-')
plt.title('4. More than Ogive')
plt.xlabel('Temperature (°C)')
plt.ylabel('Cumulative Frequency')

plt.tight_layout()
plt.show()

"""**Days to Harvest**"""

import matplotlib.pyplot as plt
import numpy as np


temp_data = df_full['Days_to_Harvest'] # Data is 'Days_to_Harvest'

plt.figure(figsize=(12, 8))


plt.subplot(2, 2, 1)
plt.hist(temp_data, bins=20, color='lightblue', edgecolor='black')
plt.title('1. Histogram')
plt.xlabel('Days to Harvest') # Corrected xlabel
plt.ylabel('Frequency')

plt.subplot(2, 2, 2)
sorted_data = np.sort(temp_data)
cumulative = np.arange(1, len(sorted_data) + 1)
plt.plot(sorted_data, cumulative, 'r-')
plt.title('2. Ogive Chart')
plt.xlabel('Days to Harvest') # Corrected xlabel
plt.ylabel('Cumulative Frequency')


plt.subplot(2, 2, 3)
counts, bin_edges = np.histogram(temp_data, bins=20)
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
plt.plot(bin_centers, counts, 'g-o')
plt.title('3. Frequency Polygon')
plt.xlabel('Days to Harvest') # Corrected xlabel
plt.ylabel('Frequency')


plt.subplot(2, 2, 4)
more_than = np.arange(len(sorted_data), 0, -1)
plt.plot(sorted_data, more_than, 'b-')
plt.title('4. More than Ogive')
plt.xlabel('Days to Harvest') # Corrected xlabel
plt.ylabel('Cumulative Frequency')

plt.tight_layout()
plt.show()

"""**Analysis and Conclusion**"""

print ("""Frequency Table Insights
• The frequency table shows which yield range or category occurs most frequently.
• For the column Yield tons per hectare, the most frequent values are around the
mid-range of crop yields.
• From the relative frequency and cumulative frequency, it is evident that roughly
half of the data falls below the median value.
Bar Chart (Regional Analysis)
• The Bar chart highlights significant differences in crop yields across regions.
• West and South regions tend to have higher yields.
• North region shows comparatively lower productivity.
Ogive Charts (Cumulative Frequency Analysis)
• The “Less than” Ogive chart is roughly S-shaped, indicating that about half of the
data falls below the median.
• The “More than” Ogive chart shows a slower rise at higher yield values, suggesting
that a few farms achieve exceptionally high yields
• Ogive charts help in understanding cumulative distribution and make skewness of the data visible.
Distribution Shape & Variability
• Histogram indicates the distribution is approximately symmetric with a slight right skew.
• Some high-yield and low-yield observations may be outliers.
• Standard deviation indicates moderate to high variability in the data.
Conclusion
• Crop yield data roughly follows a normal distribution, with some right skew and a few outliers.
Probability Sampling Methods Sampling Assignment
• Regional variations are evident, with certain regions consistently achieving higher yields.
• Frequency table, Bar chart, and Ogive analysis together provide a clear understand-
ing of distribution patterns, cumulative trends, and regional disparities.
• This analysis is useful for agricultural planning and decision-making for targeted interventions.""")

"""**Challenges**

Region-wise differences
"""

crop_distribution = df.groupby(['Region', 'Crop']).size().reset_index(name='Count')
display(crop_distribution.head())

"""Soil type impact"""

print(""" Group the DataFrame by 'Soil_Type' and 'Crop', then calculate the average 'Yield_tons_per_hectare'\nfor each combination to identify which crops yield best on which soil types.

Reasoning: I will group the DataFrame by 'Soil_Type' and 'Crop' and calculate the mean of\n'Yield_tons_per_hectare' to understand the\naverage yield for each combination, then reset the index and create a pivot table for visualization.""")

import pandas as pd
df = pd.read_csv('/content/crop_yield.csv')
soil_crop_yield = df.groupby(['Soil_Type', 'Crop'])['Yield_tons_per_hectare'].mean().reset_index()
display(soil_crop_yield.head())

"""Crop-wise analysis"""

avg_yield_per_crop = df.groupby('Crop')['Yield_tons_per_hectare'].mean().reset_index()
display(avg_yield_per_crop)

environmental_conditions = df.groupby('Crop')[['Rainfall_mm', 'Temperature_Celsius']].agg(['mean', 'std']).reset_index()
environmental_conditions.columns = ['Crop', 'Rainfall_mm_mean', 'Rainfall_mm_std', 'Temperature_Celsius_mean', 'Temperature_Celsius_std']
display(environmental_conditions)

"""Weather impact"""

print("Average Yield by Weather Condition:")
display(df.groupby('Weather_Condition')['Yield_tons_per_hectare'].mean().reset_index())

"""Optimal conditions"""

# Method 1: Simple and direct
df.loc[df.groupby('Crop')['Yield_tons_per_hectare'].idxmax()][['Crop', 'Soil_Type', 'Yield_tons_per_hectare']].sort_values('Crop')

# Method 2: Direct display without column renaming
df.groupby('Crop')[['Rainfall_mm', 'Temperature_Celsius']].agg(['mean', 'std'])

"""Climate diversity"""

pd.qcut(df['Rainfall_mm'], 3, labels=['Low', 'Medium', 'High']).pipe(
    lambda x: df.groupby(x, observed=True)['Yield_tons_per_hectare'].mean() )

# Shortest (if you want both)
print(df[['Rainfall_mm', 'Temperature_Celsius']].corrwith(df['Yield_tons_per_hectare']).round(2))

"""Conclusion:"""

print("""During this milestone, several challenges were encountered while analyzing the Agricul-
ture Crop Yield dataset:
1. Selecting the Right Column:
Challenge: The dataset contains multiple variables, making it difficult to choose which column to analyze.
Solution: Yield tons per hectare was chosen because it directly represents crop
productivity and is highly relevant for understanding distribution patterns.
2. Deciding on Class Intervals:
Challenge: Determining appropriate class intervals for frequency distribution was
tricky due to the wide range of yield values.
Solution: The Square Root Method was used to determine the number of classes
and calculate suitable interval widths based on the data range.
3. Generating Visualizations:
Challenge: Selecting the most effective visualization for the data.
Solution: Multiple visualizations were created:
• Histogram – to see the distribution of yield values.
• Bar Chart – to compare average yields across regions.
• Frequency Polygon – to show smooth distribution patterns.
• Ogive Chart – to analyze cumulative frequency and percentiles.
4. Data Cleaning and Processing:
Challenge: The dataset contained missing values and potential outliers that could
13
Probability Sampling Methods Sampling Assignment
affect analysis.
Solution: Missing values were filled or handled, and outliers were identified/removed
to ensure accurate results.
Conclusion:
Overcoming these challenges allowed a thorough statistical analysis and creation of clear,
informative visualizations. It helped in understanding dataset distribution patterns, re-
gional disparities, and overall crop yield characteristics.""")

"""**[Milestone 4](https://)**

**Milestone 4 - STA 2101: Measures of Central Tendency and Dispersion**

**Task 1: Measures of Central Tendency**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


import pandas as pd

df = pd.read_csv('/content/crop_yield.csv')
df.head()

"""**Task 1: Measures of Central Tendency**"""

import pandas as pd


df = pd.read_csv('/content/crop_yield.csv')
cols = ['Rainfall_mm', 'Temperature_Celsius', 'Days_to_Harvest', 'Yield_tons_per_hectare']


for col in cols:
    print(f"\n=== {col} ===")
    print(f"Mean   : {df[col].mean():.1f}")
    print(f"Median : {df[col].median():.1f}")
    print(f"Mode   : {df[col].mode().iloc[0]}")

# 4. Quick skewness analysis
print("\n" + "-" * 50)
print("QUICK SKEWNESS ANALYSIS")
print("-" * 50)

for col in cols:
    mean_val = df[col].mean()
    median_val = df[col].median()

    if mean_val > median_val:
        skew = "Right skewed (mean > median)"

    elif mean_val < median_val:
        skew = "Left skewed (mean < median)"

    else:
        skew = "Symmetric (mean ≈ median)"


    print(f"{col}: {skew}")

import pandas as pd

df = pd.read_csv('/content/crop_yield.csv')
cols = ['Rainfall_mm', 'Temperature_Celsius', 'Days_to_Harvest', 'Yield_tons_per_hectare']

for col in cols:
    print(f"\n{col}:")

    # Basic stats
    mean = df[col].mean()
    median = df[col].median()
    mode = df[col].mode().iloc[0]
    var = df[col].var()
    std = df[col].std()

    # Print in simple format
    print(f"  Mean   : {mean:.1f}")
    print(f"  Median : {median:.1f}")
    print(f"  Mode   : {mode}")
    print(f"  Var    : {var:.1f}")
    print(f"  Std Dev: {std:.1f}")

    # Simple insight
    if mean > median:
        print(f"  → Right skewed")
    elif mean < median:
        print(f"  → Left skewed")

"""**Task 2: Measures of Dispersion**"""

import pandas as pd

# Load
df = pd.read_csv('/content/crop_yield.csv')
cols = ['Rainfall_mm', 'Temperature_Celsius']


for col in cols:
    print(f"\n{col}:")
    print(f"  Variance : {df[col].var():.2f}")
    print(f"  Std Dev  : {df[col].std():.2f}")

    # Compare
    if df[col].var() > df[cols[0]].var() and col != cols[0]:
        print("  → More spread than", cols[0])

"""**Task 3: Visualization**"""

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('/content/crop_yield.csv')

for col in ['Rainfall_mm', 'Temperature_Celsius']:
    plt.hist(df[col], bins=15, alpha=0.5)
    plt.axvline(df[col].mean(), color='r')
    plt.axvline(df[col].median(), color='g')
    plt.axvline(df[col].mode().iloc[0], color='y')
    plt.title(col)
    plt.show()

"""**Task 4: Analysis and Conclusion**"""

import pandas as pd

df = pd.read_csv('/content/crop_yield.csv')

print("ANALYSIS:")
print(f"Rainfall mean: {df['Rainfall_mm'].mean():.1f}±{df['Rainfall_mm'].std():.1f}")
print(f"Temp mean: {df['Temperature_Celsius'].mean():.1f}±{df['Temperature_Celsius'].std():.1f}")

print("\nCONCLUSION:")
print("Data shows normal distribution with moderate spread.")

"""**[Milestone 5 - Probability](https://)**

**Section A: Setup & Load Dataset**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


import pandas as pd

df = pd.read_csv('/content/crop_yield.csv')
df.head()

"""**Section B: Treat Dataset as Sample Space**"""

N = len(df)
print('Total observations:', N)

"""
**Section C: Task 1 - Defining Events**"""

A = df[df['Days_to_Harvest'] > 100]
B = df[df['Fertilizer_Used'] == True]
C = df[(df['Days_to_Harvest'] >= 90) & (df['Days_to_Harvest'] <= 120)]

print('Event A size:', len(A))
print('Event B size:', len(B))
print('Event C size:', len(C))

"""**Section D: Task 2 - Calculating Basic Probability**"""

P_A = len(A) / N
P_B = len(B) / N
P_C = len(C) / N

print('P(A) =', P_A)
print('P(B) =', P_B)
print('P(C) =', P_C)

"""**Section E: Task 3 - Combined Events**"""

A_int_B = df[(df['Days_to_Harvest'] > 100) & (df['Fertilizer_Used'] == True)]
P_A_int_B = len(A_int_B) / N
print('P(A \u2229 B) =', P_A_int_B)

A_union_B = df[(df['Days_to_Harvest'] > 100) | (df['Fertilizer_Used'] == True)]
P_A_union_B = len(A_union_B) / N
print('P(A \u222a B) =', P_A_union_B)

A_comp = df[df['Days_to_Harvest'] <= 100]
P_A_comp = len(A_comp) / N
print('P(A\u1d9c) =', P_A_comp)

rule_value = P_A + P_B - P_A_int_B
print('Rule Verification (P(A) + P(B) - P(A \u2229 B)):', rule_value)
print('Actual P(A \u222a B):', P_A_union_B)

"""**Section F: Visualization**"""

counts = [len(A), len(A_comp)]
labels = ['A:  > 30', 'Aᶜ:  ≤ 30']

plt.bar(labels, counts)
plt.title('Event A and Complement Frequency')
plt.ylabel('Count')
plt.show()

df['Region'].value_counts().plot(kind='bar')
plt.title('Region Frequency')
plt.ylabel('Count')
plt.xlabel('Region')
plt.show()

"""**Section G: Summary**"""

print("""1. Most Likely Events
Most students (about 68%) scored above 70 in Exam 1.
Also, slightly more than half of the students (52%) are in Section B.
2. Interesting Findings
I noticed that students from Section A who scored above 70 were fewer than expected.About 32% of students scored 70 or below, which is worth looking into.
3. How Probability Helps Probability helps us make decisions with numbers. For example:
Since most students do well, teachers might make the exam harder next time.
If one section performs worse, the school could give them extra help.
These numbers help find patterns and improve teaching.""")

"""**[Milestone 6](https://)**

**Introduction**
"""

print(""" Building on the previous milestone on basic probability, this chapter introduces conditional probability, independent vs. dependent events, Bayes' rule, and
probability distributions. These concepts are fundamental for modeling uncertainty in data and are widely used in statistical inference, machine learning, and
decision-making.
""")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import norm, chi2_contingency, shapiro, kstest
import warnings
warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("Data loaded successfully!")
print(f"Sample data shape: {df.shape}")

import pandas as pd
import numpy as np
df = pd.read_csv('/content/crop_yield.csv')
print("3. CONDITIONAL PROBABILITY CALCULATIONS")
print("="*80)
print("\nMARGINAL PROBABILITIES:")
print("-"*40)
marginal_probs = {}
region_counts = df['Region'].value_counts()
for region, count in region_counts.items():
    prob = count / len(df)
    marginal_probs[f'P({region})'] = prob
    print(f"P({region}) = {count}/{len(df)} = {prob:.3f}")
soil_counts = df['Soil_Type'].value_counts() # Corrected column name
for soil, count in soil_counts.items():
    prob = count / len(df)
    marginal_probs[f'P({soil})'] = prob
    print(f"P({soil}) = {count}/{len(df)} = {prob:.3f}")
crop_counts = df['Crop'].value_counts()
for crop, count in crop_counts.items():
    prob = count / len(df)
    marginal_probs[f'P({crop})'] = prob
    print(f"P({crop}) = {count}/{len(df)} = {prob:.3f}") # Removed extraneous 'r'
weather_counts = df['Weather_Condition'].value_counts() # Corrected column name
for weather, count in weather_counts.items():
    prob = count / len(df)
    marginal_probs[f'P({weather})'] = prob
    print(f"P({weather}) = {count}/{len(df)} = {prob:.3f}")
print("\nCONDITIONAL PROBABILITIES:")
print("-"*40)
print("\nP(Crop | Soil Type):")
soils = df['Soil_Type'].unique() # Corrected column name
crops = df['Crop'].unique()
for soil in soils:
    soil_df = df[df['Soil_Type'] == soil] # Corrected column name
    total_soil = len(soil_df)
    if total_soil > 0:
        print(f"\nGiven Soil = {soil} (n={total_soil}):")
        for crop in crops:
            crop_count = len(soil_df[soil_df['Crop'] == crop])
            if crop_count > 0:
                p_crop_given_soil = crop_count / total_soil
                print(f"  P({crop} | {soil}) = {crop_count}/{total_soil} = {p_crop_given_soil:.3f}")
print("\nP(High Yield | Weather):")
df['High_Yield'] = df['Yield_tons_per_hectare'] > df['Yield_tons_per_hectare'].mean()
weather_types = df['Weather_Condition'].unique() # Corrected column name
for weather in weather_types:
    weather_df = df[df['Weather_Condition'] == weather] # Corrected column name
    total_weather = len(weather_df)
    if total_weather > 0:
        high_yield_count = len(weather_df[weather_df['High_Yield'] == True])
        p_high_given_weather = high_yield_count / total_weather
        print(f"P(High Yield | {weather}) = {high_yield_count}/{total_weather} = {p_high_given_weather:.3f}")

print("4. Independent vs. Dependent Events")
print("="*80)
print("\nCONTINGENCY TABLES:")
print("-"*40)
soil_crop_ct = pd.crosstab(df['Soil_Type'], df['Crop']) # Corrected 'Soil' to 'Soil_Type'
print("Soil_Type × Crop Contingency Table:")
print(soil_crop_ct)
print()
region_weather_ct = pd.crosstab(df['Region'], df['Weather_Condition']) # Corrected 'Weather' to 'Weather_Condition'
print("Region × Weather_Condition Contingency Table:")
print(region_weather_ct)
print()
print("\nCHI-SQUARE TESTS FOR INDEPENDENCE:")
print("-"*40)
chi2_soil_crop, p_soil_crop, dof_soil_crop, expected_soil_crop = chi2_contingency(soil_crop_ct)
print(f"1. Soil_Type vs Crop:")
print(f"   χ² = {chi2_soil_crop:.3f}, p-value = {p_soil_crop:.4f}, df = {dof_soil_crop}")
if p_soil_crop < 0.05:
    print("   → REJECT independence: Soil_Type and Crop are dependent")
else:
    print("   → FAIL to reject independence: Soil_Type and Crop may be independent")
chi2_region_weather, p_region_weather, dof_region_weather, expected_region_weather = chi2_contingency(region_weather_ct)
print(f"\n2. Region vs Weather_Condition:")
print(f"   χ² = {chi2_region_weather:.3f}, p-value = {p_region_weather:.4f}, df = {dof_region_weather}")
if p_region_weather < 0.05:
    print("   → REJECT independence: Region and Weather_Condition are dependent")
else:
    print("   → FAIL to reject independence: Region and Weather_Condition may be independent")
print("\nEXPECTED vs OBSERVED FREQUENCIES (Soil_Type vs Crop):")
print("-"*40)
expected_df = pd.DataFrame(expected_soil_crop,
                           index=soil_crop_ct.index,
                           columns=soil_crop_ct.columns)
print("Expected Frequencies:")
print(expected_df.round(2))
print("\nObserved Frequencies:")
print(soil_crop_ct)
print("\nSTANDARDIZED RESIDUALS (Soil_Type vs Crop):")
print("-"*40)
residuals = (soil_crop_ct.values - expected_soil_crop) / np.sqrt(expected_soil_crop)
residuals_df = pd.DataFrame(residuals,
                           index=soil_crop_ct.index,
                           columns=soil_crop_ct.columns)
print(residuals_df.round(2))

print("5. BAYES' RULE APPLICATION")
print("="*80)
print("\nBAYES' RULE EXAMPLE:")
print("-"*40)
print("Problem: What is the probability that a field has Clay soil given it has Rice crop?")
print()
p_clay = len(df[df['Soil_Type'] == 'Clay']) / len(df)
print(f"P(Clay) = {len(df[df['Soil_Type'] == 'Clay'])}/{len(df)} = {p_clay:.3f}")
p_rice = len(df[df['Crop'] == 'Rice']) / len(df)
print(f"P(Rice) = {len(df[df['Crop'] == 'Rice'])}/{len(df)} = {p_rice:.3f}")
rice_clay_df = df[df['Soil_Type'] == 'Clay']
p_rice_given_clay = len(rice_clay_df[rice_clay_df['Crop'] == 'Rice']) / len(rice_clay_df)
print(f"P(Rice | Clay) = {len(rice_clay_df[rice_clay_df['Crop'] == 'Rice'])}/{len(rice_clay_df)} = {p_rice_given_clay:.3f}")
p_clay_given_rice = (p_rice_given_clay * p_clay) / p_rice
print(f"\nBayes' Rule Calculation:")
print(f"P(Clay | Rice) = [P(Rice | Clay) × P(Clay)] / P(Rice)")
print(f"               = [{p_rice_given_clay:.3f} × {p_clay:.3f}] / {p_rice:.3f}")
print(f"               = {p_clay_given_rice:.3f}")
print("\nVerification by direct calculation:")
clay_rice_count = len(df[(df['Soil_Type'] == 'Clay') & (df['Crop'] == 'Rice')])
total_rice = len(df[df['Crop'] == 'Rice'])
p_clay_given_rice_direct = clay_rice_count / total_rice if total_rice > 0 else 0
print(f"P(Clay | Rice) (direct) = {clay_rice_count}/{total_rice} = {p_clay_given_rice_direct:.3f}")
print("SECOND EXAMPLE: Weather and Yield")
print("="*40)
p_sunny = len(df[df['Weather_Condition'] == 'Sunny']) / len(df)
print(f"P(Sunny) = {len(df[df['Weather_Condition'] == 'Sunny'])}/{len(df)} = {p_sunny:.3f}")
p_high_yield = len(df[df['High_Yield'] == True]) / len(df)
print(f"P(High Yield) = {len(df[df['High_Yield'] == True])}/{len(df)} = {p_high_yield:.3f}")
high_yield_df = df[df['High_Yield'] == True]
p_sunny_given_high = len(high_yield_df[high_yield_df['Weather_Condition'] == 'Sunny']) / len(high_yield_df)
print(f"P(Sunny | High Yield) = {len(high_yield_df[high_yield_df['Weather_Condition'] == 'Sunny'])}/{len(high_yield_df)} = {p_sunny_given_high:.3f}")
p_high_given_sunny = (p_sunny_given_high * p_high_yield) / p_sunny
print(f"\nBayes' Rule Calculation:")
print(f"P(High Yield | Sunny) = [P(Sunny | High Yield) × P(High Yield)] / P(Sunny)")
print(f"                      = [{p_sunny_given_high:.3f} × {p_high_yield:.3f}] / {p_sunny:.3f}")
print(f"                      = {p_high_given_sunny:.3f}")
print("\nPRIOR vs POSTERIOR PROBABILITIES:")
print(f"Prior P(High Yield) = {p_high_yield:.3f}")
print(f"Posterior P(High Yield | Sunny) = {p_high_given_sunny:.3f}")
if p_high_given_sunny > p_high_yield:
    print("Conclusion: Sunny weather increases the probability of high yield")
else:
    print("Conclusion: Sunny weather decreases the probability of high yield")

print("6. NORMAL DISTRIBUTION ANALYSIS")
print("="*80)
analysis_data = df['Yield_tons_per_hectare']
mean_val = np.mean(analysis_data)
std_val = np.std(analysis_data)
median_val = np.median(analysis_data)
skew_val = stats.skew(analysis_data)
kurt_val = stats.kurtosis(analysis_data)
print("\nDESCRIPTIVE STATISTICS:")
print("-"*40)
print(f"Mean (μ): {mean_val:.4f}")
print(f"Standard Deviation (σ): {std_val:.4f}")
print(f"Median: {median_val:.4f}")
print(f"Skewness: {skew_val:.4f} (Normal ≈ 0)")
print(f"Kurtosis: {kurt_val:.4f} (Normal ≈ 0)")
print(f"Minimum: {np.min(analysis_data):.2f}")
print(f"Maximum: {np.max(analysis_data):.2f}")
print(f"Range: {np.max(analysis_data) - np.min(analysis_data):.2f}")
print(f"Sample Size: {len(analysis_data):,}")
print("\nEMPIRICAL RULE CHECK:")
print("-"*40)
within_1sigma = np.sum((analysis_data >= mean_val - std_val) & (analysis_data <= mean_val + std_val)) / len(analysis_data)
within_2sigma = np.sum((analysis_data >= mean_val - 2*std_val) & (analysis_data <= mean_val + 2*std_val)) / len(analysis_data)
within_3sigma = np.sum((analysis_data >= mean_val - 3*std_val) & (analysis_data <= mean_val + 3*std_val)) / len(analysis_data)
print(f"Data within μ ± σ: {within_1sigma*100:.2f}% (Expected: 68.27%)")
print(f"Data within μ ± 2σ: {within_2sigma*100:.2f}% (Expected: 95.45%)")
print(f"Data within μ ± 3σ: {within_3sigma*100:.2f}% (Expected: 99.73%)")
print("\nNORMALITY TESTS:")
print("-"*40)
sample_size_shapiro = min(5000, len(analysis_data))
sample_data_shapiro = np.random.choice(analysis_data, sample_size_shapiro, replace=False)
stat_shapiro, p_shapiro = shapiro(sample_data_shapiro)
print(f"Shapiro-Wilk Test (n={sample_size_shapiro}):")
print(f"  Test Statistic = {stat_shapiro:.4f}, p-value = {p_shapiro:.6f}")
if p_shapiro > 0.05:
    print("  → Cannot reject normality (p > 0.05)")
else:
    print("  → Reject normality (p ≤ 0.05)")
stat_ks, p_ks = kstest(analysis_data, 'norm', args=(mean_val, std_val))
print(f"\nKolmogorov-Smirnov Test:")
print(f"  Test Statistic = {stat_ks:.4f}, p-value = {p_ks:.6f}")
if p_ks > 0.05:
    print("  → Cannot reject normality (p > 0.05)")
else:
    print("  → Reject normality (p ≤ 0.05)")
print("\nPROBABILITY CALCULATIONS:")
print("-"*40)
print(f"P(X > μ) = P(X > {mean_val:.2f}) = {1 - norm.cdf(mean_val, mean_val, std_val):.4f} or 50%")
print(f"P(μ - σ < X < μ + σ) = P({mean_val-std_val:.2f} < X < {mean_val+std_val:.2f}) = {within_1sigma:.4f} or {within_1sigma*100:.2f}%")
print(f"P(X < μ - 2σ) = P(X < {mean_val-2*std_val:.2f}) = {norm.cdf(mean_val-2*std_val, mean_val, std_val):.6f} or {norm.cdf(mean_val-2*std_val, mean_val, std_val)*100:.4f}%")

print("7. VISUALIZATIONS")
print("="*80)
fig = plt.figure(figsize=(20, 15))
ax1 = plt.subplot(3, 3, 1)
soil_crop_probs = pd.crosstab(df['Soil_Type'], df['Crop'], normalize='index')
soil_crop_probs.plot(kind='bar', stacked=True, ax=ax1, colormap='viridis')
ax1.set_title('Conditional Probabilities: P(Crop | Soil)')
ax1.set_xlabel('Soil Type')
ax1.set_ylabel('Probability')
ax1.legend(title='Crop', bbox_to_anchor=(1.05, 1), loc='upper left')
ax1.tick_params(axis='x', rotation=45)
ax2 = plt.subplot(3, 3, 2)
observed_expected = pd.DataFrame({
    'Observed': soil_crop_ct.values.flatten(),
    'Expected': expected_soil_crop.flatten()
}, index=[f'{s}-{c}' for s in soil_crop_ct.index for c in soil_crop_ct.columns])
observed_expected.plot(kind='bar', ax=ax2, color=['blue', 'orange'])
ax2.set_title('Observed vs Expected Frequencies')
ax2.set_xlabel('Soil-Crop Combinations')
ax2.set_ylabel('Frequency')
ax2.tick_params(axis='x', rotation=90)
ax2.legend()
ax3 = plt.subplot(3, 3, 3)
prior_posterior = pd.DataFrame({
    'Probability': [p_high_yield, p_high_given_sunny],
    'Type': ['Prior P(High Yield)', 'Posterior P(High Yield|Sunny)'] })
prior_posterior.plot(kind='bar', x='Type', y='Probability', ax=ax3, legend=False, color=['skyblue', 'lightcoral'])
ax3.set_title('Bayesian Update: Prior vs Posterior')
ax3.set_ylabel('Probability')
ax3.tick_params(axis='x', rotation=45)
for i, v in enumerate([p_high_yield, p_high_given_sunny]):
    ax3.text(i, v + 0.01, f'{v:.3f}', ha='center')
ax4 = plt.subplot(3, 3, 4)
ax4.hist(analysis_data, bins=50, density=True, alpha=0.6, color='green', edgecolor='black', label='Data')
x = np.linspace(mean_val - 4*std_val, mean_val + 4*std_val, 1000)
y = norm.pdf(x, mean_val, std_val)
ax4.plot(x, y, 'r-', linewidth=2, label='Normal Distribution')
ax4.axvline(mean_val, color='red', linestyle='--', alpha=0.5, label=f'Mean = {mean_val:.2f}')
ax4.axvspan(mean_val - std_val, mean_val + std_val, alpha=0.2, color='gray', label='μ ± σ')
ax4.set_xlabel('Value')
ax4.set_ylabel('Density')
ax4.set_title('Histogram with Normal Distribution Overlay')
ax4.legend()
ax4.grid(True, alpha=0.3)
ax5 = plt.subplot(3, 3, 5)
stats.probplot(analysis_data, dist="norm", plot=ax5)
ax5.set_title('Q-Q Plot (vs Normal Distribution)')
ax7 = plt.subplot(3, 3, 7)
x = np.linspace(mean_val - 4*std_val, mean_val + 4*std_val, 1000)
y = norm.pdf(x, mean_val, std_val)
ax7.plot(x, y, 'b-', linewidth=2, label='Normal Distribution')
ax7.fill_between(x, 0, y, where=(x >= mean_val - std_val) & (x <= mean_val + std_val),
                 color='green', alpha=0.3, label=f'μ ± σ ({within_1sigma*100:.1f}%)')
ax7.fill_between(x, 0, y, where=(x >= mean_val - 2*std_val) & (x <= mean_val + 2*std_val),
                 color='yellow', alpha=0.3, label=f'μ ± 2σ ({within_2sigma*100:.1f}%)')
ax7.fill_between(x, 0, y, where=(x >= mean_val - 3*std_val) & (x <= mean_val + 3*std_val),
                 color='red', alpha=0.3, label=f'μ ± 3σ ({within_3sigma*100:.1f}%)')
ax7.axvline(mean_val, color='red', linestyle='--', alpha=0.5, label=f'Mean = {mean_val:.2f}')
ax7.set_xlabel('Value')
ax7.set_ylabel('Density')
ax7.set_title('Empirical Rule Visualization')
ax7.legend()
ax7.grid(True, alpha=0.3)
ax8 = plt.subplot(3, 3, 8)
x_norm = np.linspace(np.min(analysis_data), np.max(analysis_data), 1000)
y_norm = norm.cdf(x_norm, mean_val, std_val)
ax8.plot(x_norm, y_norm, 'r--', linewidth=2, label='Normal CDF')
ax8.set_xlabel('Value')
ax8.set_ylabel('Cumulative Probability')
ax8.set_title('Cumulative Distribution Functions')
ax8.legend()
ax8.grid(True, alpha=0.3)
# 9. Summary Statistics
ax9 = plt.subplot(3, 3, 9)
ax9.axis('off')
summary_text = f"""
NORMAL DISTRIBUTION SUMMARY
{'='*30}
Mean (μ) = {mean_val:.4f}
Std Dev (σ) = {std_val:.4f}
Skewness = {skew_val:.4f}
Kurtosis = {kurt_val:.4f}

EMPIRICAL RULE
{'='*30}
Within μ ± σ: {within_1sigma*100:.2f}%
Within μ ± 2σ: {within_2sigma*100:.2f}%
Within μ ± 3σ: {within_3sigma*100:.2f}%

NORMALITY TESTS
{'='*30}
Shapiro-Wilk: p = {p_shapiro:.6f}
KS Test: p = {p_ks:.6f}
"""
ax9.text(0.1, 0.95, summary_text, transform=ax9.transAxes, fontsize=10,
        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
plt.suptitle('Comprehensive Probability and Distribution Analysis', fontsize=16, y=1.02)
plt.tight_layout()
plt.show()

"""**B. Knowledge Points**

**1. Conditional Probability**
"""

# Cell 3: Basic Statistics
print("Dataset Info:")
print(df.info())
print("\nDescriptive Statistics:")
print(df.describe())
print("\nMissing Values:")
print(df.isnull().sum())

# Cell 4: Define High Yield
# Define high yield as top 30% of yields
high_yield_threshold = df['Yield_tons_per_hectare'].quantile(0.70)
df['High_Yield'] = df['Yield_tons_per_hectare'] >= high_yield_threshold

print(f"High Yield Threshold: {high_yield_threshold:.2f} tons/hectare")
print(f"Percentage of High Yield cases: {df['High_Yield'].mean()*100:.2f}%")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646198956255336,False,True,Rainy,140,3.7072931271974823"""
lines = data.strip().split('\n')
header = lines[0].split(',')
rows = [line.split(',') for line in lines[1:]]
df = pd.DataFrame(rows, columns=header)
df['Rainfall_mm'] = pd.to_numeric(df['Rainfall_mm'])
df['Temperature_Celsius'] = pd.to_numeric(df['Temperature_Celsius'])
df['Days_to_Harvest'] = pd.to_numeric(df['Days_to_Harvest'])
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
df['Fertilizer_Used'] = df['Fertilizer_Used'].map({'True': True, 'False': False})
df['Irrigation_Used'] = df['Irrigation_Used'].map({'True': True, 'False': False})
print("DATA LOADED SUCCESSFULLY!")
print(f"Total Records: {len(df)}")
print(f"Total Columns: {len(df.columns)}")
print("\nFirst 5 Rows:")
print(df.head())
print("-" * 50)
average_yield = df['Yield_tons_per_hectare'].mean()
df['High_Yield'] = df['Yield_tons_per_hectare'] > average_yield
print("YIELD ANALYSIS")
print(f"Average Yield: {average_yield:.2f} tons/hectare")
print(f"High Yield Cases: {df['High_Yield'].sum()} ({df['High_Yield'].mean()*100:.0f}%)")
print(f"Low Yield Cases: {len(df) - df['High_Yield'].sum()} ({100 - df['High_Yield'].mean()*100:.0f}%)")
print("-" * 50)
def simple_conditional_probability(condition_column, condition_value):
    """
    Simple function to calculate P(High Yield | Condition)
    Formula: P(A|B) = Number of (A and B) / Number of B
    """
    total_with_condition = len(df[df[condition_column] == condition_value])
    both_cases = len(df[(df[condition_column] == condition_value) & (df['High_Yield'] == True)])
    if total_with_condition == 0:
        return 0
    return both_cases / total_with_condition

print("\nREGION-WISE PROBABILITIES:")
print("-" * 40)
for region in df['Region'].unique():
    prob = simple_conditional_probability('Region', region)
    print(f"P(High Yield | Region = {region}) = {prob:.2f} ({prob*100:.0f}%)")
print("\nWEATHER-WISE PROBABILITIES:")
print("-" * 40)
for weather in df['Weather_Condition'].unique():
    prob = simple_conditional_probability('Weather_Condition', weather)
    print(f"P(High Yield | Weather = {weather}) = {prob:.2f} ({prob*100:.0f}%)")

print("\nFERTILIZER IMPACT:")
print("-" * 40)
for fertilizer in [True, False]:
    prob = simple_conditional_probability('Fertilizer_Used', fertilizer)
    status = "Used" if fertilizer else "Not Used"
    print(f"P(High Yield | Fertilizer {status}) = {prob:.2f} ({prob*100:.0f}%)")
print("\nCROP-WISE PROBABILITIES:")
print("-" * 40)
for crop in df['Crop'].unique():
    prob = simple_conditional_probability('Crop', crop)
    print(f"P(High Yield | Crop = {crop}) = {prob:.2f} ({prob*100:.0f}%)")
print("\nSOIL TYPE PROBABILITIES:")
print("-" * 40)
for soil in df['Soil_Type'].unique():
    prob = simple_conditional_probability('Soil_Type', soil)
    print(f"P(High Yield | Soil = {soil}) = {prob:.2f} ({prob*100:.0f}%)")
plt.figure(figsize=(12, 8))
plt.subplot(2, 3, 1)
regions = df['Region'].unique()
region_probs = [simple_conditional_probability('Region', r) for r in regions]
bars1 = plt.bar(regions, region_probs, color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12'])
plt.title('P(High Yield | Region)', fontsize=12, fontweight='bold')
plt.ylabel('Probability')
plt.ylim(0, 1.1)
for bar, prob in zip(bars1, region_probs):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
             f'{prob:.2f}', ha='center', va='bottom')
plt.subplot(2, 3, 2)
weathers = df['Weather_Condition'].unique()
weather_probs = [simple_conditional_probability('Weather_Condition', w) for w in weathers]
bars2 = plt.bar(weathers, weather_probs, color=['#f1c40f', '#3498db', '#95a5a6'])
plt.title('P(High Yield | Weather)', fontsize=12, fontweight='bold')
plt.ylabel('Probability')
plt.ylim(0, 1.1)
for bar, prob in zip(bars2, weather_probs):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
             f'{prob:.2f}', ha='center', va='bottom')
plt.subplot(2, 3, 3)
fertilizer_labels = ['No Fertilizer', 'Fertilizer Used']
fert_probs = [
    simple_conditional_probability('Fertilizer_Used', False),
    simple_conditional_probability('Fertilizer_Used', True)
]
bars3 = plt.bar(fertilizer_labels, fert_probs, color=['#e74c3c', '#2ecc71'])
plt.title('P(High Yield | Fertilizer)', fontsize=12, fontweight='bold')
plt.ylabel('Probability')
plt.ylim(0, 1.1)
for bar, prob in zip(bars3, fert_probs):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
             f'{prob:.2f}', ha='center', va='bottom')
plt.subplot(2, 3, 4)
crops = df['Crop'].unique()
crop_probs = [simple_conditional_probability('Crop', c) for c in crops]
bars4 = plt.bar(crops, crop_probs, color=['#9b59b6', '#1abc9c', '#34495e', '#e74c3c', '#f39c12', '#2980b9'])
plt.title('P(High Yield | Crop)', fontsize=12, fontweight='bold')
plt.ylabel('Probability')
plt.ylim(0, 1.1)
plt.xticks(rotation=45)
for bar, prob in zip(bars4, crop_probs):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
             f'{prob:.2f}', ha='center', va='bottom')
plt.subplot(2, 3, 5)
soils = df['Soil_Type'].unique()
soil_probs = [simple_conditional_probability('Soil_Type', s) for s in soils]
bars5 = plt.bar(soils, soil_probs, color=['#e67e22', '#27ae60', '#8e44ad', '#d35400', '#16a085', '#c0392b'])
plt.title('P(High Yield | Soil Type)', fontsize=12, fontweight='bold')
plt.ylabel('Probability')
plt.ylim(0, 1.1)
plt.xticks(rotation=45)
for bar, prob in zip(bars5, soil_probs):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
             f'{prob:.2f}', ha='center', va='bottom')
plt.subplot(2, 3, 6)
irrigation_labels = ['No Irrigation', 'Irrigation Used']
irrigation_probs = [
    simple_conditional_probability('Irrigation_Used', False),
    simple_conditional_probability('Irrigation_Used', True)
]
bars6 = plt.bar(irrigation_labels, irrigation_probs, color=['#e74c3c', '#2ecc71'])
plt.title('P(High Yield | Irrigation)', fontsize=12, fontweight='bold')
plt.ylabel('Probability')
plt.ylim(0, 1.1)
for bar, prob in zip(bars6, irrigation_probs):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
             f'{prob:.2f}', ha='center', va='bottom')

plt.suptitle('CONDITIONAL PROBABILITY ANALYSIS: P(High Yield | Condition)',
             fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()

print("\nREATING COMPARISON CHART...")
print("-" * 50)
plt.figure(figsize=(14, 8))
all_labels = []
all_probs = []
for region in df['Region'].unique():
    all_labels.append(f"Region: {region}")
    all_probs.append(simple_conditional_probability('Region', region))
for weather in df['Weather_Condition'].unique():
    all_labels.append(f"Weather: {weather}")
    all_probs.append(simple_conditional_probability('Weather_Condition', weather))
all_labels.append("Fertilizer: No")
all_probs.append(simple_conditional_probability('Fertilizer_Used', False))
all_labels.append("Fertilizer: Yes")
all_probs.append(simple_conditional_probability('Fertilizer_Used', True))
all_labels.append("Irrigation: No")
all_probs.append(simple_conditional_probability('Irrigation_Used', False))
all_labels.append("Irrigation: Yes")
all_probs.append(simple_conditional_probability('Irrigation_Used', True))
for crop in df['Crop'].unique():
    all_labels.append(f"Crop: {crop}")
    all_probs.append(simple_conditional_probability('Crop', crop))
for soil in df['Soil_Type'].unique():
    all_labels.append(f"Soil: {soil}")
    all_probs.append(simple_conditional_probability('Soil_Type', soil))
sorted_indices = np.argsort(all_probs)[::-1]  # Descending order
sorted_labels = [all_labels[i] for i in sorted_indices]
sorted_probs = [all_probs[i] for i in sorted_indices]
bars = plt.barh(sorted_labels, sorted_probs, color=plt.cm.viridis(np.linspace(0.2, 0.9, len(sorted_labels))))
for i, (bar, prob) in enumerate(zip(bars, sorted_probs)):
    plt.text(prob + 0.02, bar.get_y() + bar.get_height()/2,
             f'{prob:.2f} ({prob*100:.0f}%)',
             va='center', fontsize=10)

plt.title('All Conditional Probabilities: P(High Yield | Condition)', fontsize=16, fontweight='bold')
plt.xlabel('Probability')
plt.xlim(0, 1.2)
plt.axvline(x=0.5, color='red', linestyle='--', alpha=0.5, label='50% Chance')
plt.legend()
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()
print("\nADDITIONAL NUMERICAL ANALYSIS")
print("-" * 40)
print("Average Yield by Region:")
for region in df['Region'].unique():
    avg = df[df['Region'] == region]['Yield_tons_per_hectare'].mean()
    print(f"  {region}: {avg:.2f} tons/hectare")
print("\nAverage Yield by Weather:")
for weather in df['Weather_Condition'].unique():
    avg = df[df['Weather_Condition'] == weather]['Yield_tons_per_hectare'].mean()
    print(f"  {weather}: {avg:.2f} tons/hectare")
print("\nAverage Yield by Fertilizer Usage:")
for fert in [True, False]:
    avg = df[df['Fertilizer_Used'] == fert]['Yield_tons_per_hectare'].mean()
    status = "Used" if fert else "Not Used"
    print(f"  Fertilizer {status}: {avg:.2f} tons/hectare")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from matplotlib import rcParams
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
rcParams['font.size'] = 10
rcParams['axes.titlesize'] = 12
rcParams['axes.titleweight'] = 'bold'
data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646198956255336,False,True,Rainy,140,3.7072931271974823"""
lines = data.strip().split('\n')
header = lines[0].split(',')
rows = [line.split(',') for line in lines[1:]]
df = pd.DataFrame(rows, columns=header)
df['Rainfall_mm'] = pd.to_numeric(df['Rainfall_mm'])
df['Temperature_Celsius'] = pd.to_numeric(df['Temperature_Celsius'])
df['Days_to_Harvest'] = pd.to_numeric(df['Days_to_Harvest'])
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
df['Fertilizer_Used'] = df['Fertilizer_Used'].map({'True': True, 'False': False})
df['Irrigation_Used'] = df['Irrigation_Used'].map({'True': True, 'False': False})
print("\nDATA SUMMARY:")
print(f"Total Records: {len(df)}")
print(f"Total Variables: {len(df.columns)}")
average_yield = df['Yield_tons_per_hectare'].mean()
df['High_Yield'] = df['Yield_tons_per_hectare'] > average_yield
print(f"\nHIGH YIELD THRESHOLD:")
print(f"Average Yield: {average_yield:.2f} tons/hectare")
print(f"High Yield Cases: {df['High_Yield'].sum()} out of {len(df)}")
print(f"High Yield Rate: {df['High_Yield'].mean()*100:.1f}%")
print("MATHEMATICAL FORMULA:")
print("="*60)
print("Conditional Probability: P(A|B) = P(A ∩ B) / P(B)")
print("Where:")
print("  P(A|B) = Probability of event A given event B has occurred")
print("  P(A ∩ B) = Probability of both A and B occurring")
print("  P(B) = Probability of event B")
print("\nIn our case:")
print("  A = High Yield (Yield > Average)")
print("  B = Specific Condition (e.g., Region='South', Weather='Rainy')")
print("CALCULATION EXAMPLES:")
print("="*60)
def calculate_conditional_probability(condition_column, condition_value, verbose=True):
    """
    Calculate conditional probability with step-by-step explanation
    P(High_Yield | Condition) = P(High_Yield ∩ Condition) / P(Condition)
    """
    total_samples = len(df)
    samples_with_B = len(df[df[condition_column] == condition_value])
    P_B = samples_with_B / total_samples
    samples_with_A_and_B = len(df[(df[condition_column] == condition_value) & (df['High_Yield'] == True)])
    P_A_and_B = samples_with_A_and_B / total_samples
    if samples_with_B > 0:
        P_A_given_B = samples_with_A_and_B / samples_with_B
    else:
        P_A_given_B = 0
    if verbose:
        print(f"\n🔍 Calculating: P(High_Yield | {condition_column} = {condition_value})")
        print("-" * 40)
        print(f"Total Samples (N): {total_samples}")
        print(f"Samples with {condition_column} = {condition_value}: {samples_with_B}")
        print(f"P(B) = P({condition_column} = {condition_value}) = {samples_with_B}/{total_samples} = {P_B:.3f}")
        print(f"Samples with High_Yield AND {condition_column} = {condition_value}: {samples_with_A_and_B}")
        print(f"P(A ∩ B) = {samples_with_A_and_B}/{total_samples} = {P_A_and_B:.3f}")
        print(f"\nP(A|B) = P(A ∩ B) / P(B) = {P_A_and_B:.3f} / {P_B:.3f}")
        print(f"       = {samples_with_A_and_B} / {samples_with_B}")
        print(f"       = {P_A_given_B:.3f} ({P_A_given_B*100:.1f}%)")
        print("-" * 40)

    return P_A_given_B
print("\nEXAMPLE 1: South Region")
_ = calculate_conditional_probability('Region', 'South')
print("\nEXAMPLE 2: Fertilizer Used")
_ = calculate_conditional_probability('Fertilizer_Used', True)
print("ALL CONDITIONAL PROBABILITIES:")
print("="*60)
def quick_conditional_probability(condition_column, condition_value):
    total_with_condition = len(df[df[condition_column] == condition_value])
    both_cases = len(df[(df[condition_column] == condition_value) & (df['High_Yield'] == True)])
    return both_cases / total_with_condition if total_with_condition > 0 else 0
print("\nREGION-WISE:")
for region in df['Region'].unique():
    prob = quick_conditional_probability('Region', region)
    count = len(df[df['Region'] == region])
    high_yield_count = len(df[(df['Region'] == region) & (df['High_Yield'] == True)])
    print(f"  P(High_Yield | Region = {region}) = {high_yield_count}/{count} = {prob:.2f} ({prob*100:.0f}%)")
print("\nWEATHER-WISE:")
for weather in df['Weather_Condition'].unique():
    prob = quick_conditional_probability('Weather_Condition', weather)
    count = len(df[df['Weather_Condition'] == weather])
    high_yield_count = len(df[(df['Weather_Condition'] == weather) & (df['High_Yield'] == True)])
    print(f"  P(High_Yield | Weather = {weather}) = {high_yield_count}/{count} = {prob:.2f} ({prob*100:.0f}%)")
print("\nFERTILIZER IMPACT:")
for fertilizer in [True, False]:
    prob = quick_conditional_probability('Fertilizer_Used', fertilizer)
    count = len(df[df['Fertilizer_Used'] == fertilizer])
    high_yield_count = len(df[(df['Fertilizer_Used'] == fertilizer) & (df['High_Yield'] == True)])
    status = "Used" if fertilizer else "Not Used"
    print(f"  P(High_Yield | Fertilizer {status}) = {high_yield_count}/{count} = {prob:.2f} ({prob*100:.0f}%)")
print("\nCROP-WISE:")
for crop in df['Crop'].unique():
    prob = quick_conditional_probability('Crop', crop)
    count = len(df[df['Crop'] == crop])
    high_yield_count = len(df[(df['Crop'] == crop) & (df['High_Yield'] == True)])
    print(f"  P(High_Yield | Crop = {crop}) = {high_yield_count}/{count} = {prob:.2f} ({prob*100:.0f}%)")
fig = plt.figure(figsize=(14, 10))
fig.suptitle('Conditional Probability Analysis: P(High Yield | Condition)\n\n' +
             'Mathematical Formula: $P(A|B) = \\frac{P(A \\cap B)}{P(B)}$\n' +
             'Where A = High Yield, B = Specific Condition',
             fontsize=14, fontweight='bold', y=1.02)
ax1 = plt.subplot(2, 3, 1)
regions = df['Region'].unique()
region_probs = [quick_conditional_probability('Region', r) for r in regions]
bars1 = ax1.bar(regions, region_probs, color=['#3498db', '#2ecc71', '#e74c3c'])
ax1.set_title('$P(High\\ Yield\ |\ Region)$', fontsize=11, fontweight='bold')
ax1.set_ylabel('Probability')
ax1.set_ylim(0, 1.2)
for bar, prob, region in zip(bars1, region_probs, regions):
    count = len(df[df['Region'] == region])
    high_count = len(df[(df['Region'] == region) & (df['High_Yield'] == True)])
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.03,
             f'{high_count}/{count}\n={prob:.2f}', ha='center', va='bottom', fontsize=9)
ax1.axhline(y=df['High_Yield'].mean(), color='red', linestyle='--', alpha=0.5,
            label=f'Overall: {df["High_Yield"].mean():.2f}')
ax1.legend(loc='upper right', fontsize=8)
ax2 = plt.subplot(2, 3, 2)
weathers = df['Weather_Condition'].unique()
weather_probs = [quick_conditional_probability('Weather_Condition', w) for w in weathers]
bars2 = ax2.bar(weathers, weather_probs, color=['#f1c40f', '#3498db', '#95a5a6'])
ax2.set_title('$P(High\\ Yield\ |\ Weather)$', fontsize=11, fontweight='bold')
ax2.set_ylabel('Probability')
ax2.set_ylim(0, 1.2)
for bar, prob, weather in zip(bars2, weather_probs, weathers):
    count = len(df[df['Weather_Condition'] == weather])
    high_count = len(df[(df['Weather_Condition'] == weather) & (df['High_Yield'] == True)])
    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.03,
             f'{high_count}/{count}\n={prob:.2f}', ha='center', va='bottom', fontsize=9)
ax3 = plt.subplot(2, 3, 3)
fertilizer_labels = ['No Fertilizer', 'Fertilizer Used']
fert_probs = [
    quick_conditional_probability('Fertilizer_Used', False),
    quick_conditional_probability('Fertilizer_Used', True)
]
bars3 = ax3.bar(fertilizer_labels, fert_probs, color=['#e74c3c', '#2ecc71'])
ax3.set_title('$P(High\\ Yield\ |\ Fertilizer)$', fontsize=11, fontweight='bold')
ax3.set_ylabel('Probability')
ax3.set_ylim(0, 1.2)
for bar, prob, label in zip(bars3, fert_probs, fertilizer_labels):
    count = len(df[df['Fertilizer_Used'] == (label == 'Fertilizer Used')])
    high_count = len(df[(df['Fertilizer_Used'] == (label == 'Fertilizer Used')) & (df['High_Yield'] == True)])
    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.03,
             f'{high_count}/{count}\n={prob:.2f}', ha='center', va='bottom', fontsize=9)
ax4 = plt.subplot(2, 3, 4)
crops = df['Crop'].unique()
crop_probs = [quick_conditional_probability('Crop', c) for c in crops]
bars4 = ax4.bar(crops, crop_probs, color=['#9b59b6', '#1abc9c', '#34495e', '#e74c3c', '#f39c12'])
ax4.set_title('$P(High\\ Yield\ |\ Crop)$', fontsize=11, fontweight='bold')
ax4.set_ylabel('Probability')
ax4.set_ylim(0, 1.2)
ax4.tick_params(axis='x', rotation=45)
for bar, prob, crop in zip(bars4, crop_probs, crops):
    count = len(df[df['Crop'] == crop])
    high_count = len(df[(df['Crop'] == crop) & (df['High_Yield'] == True)])
    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.03,
             f'{high_count}/{count}\n={prob:.2f}', ha='center', va='bottom', fontsize=8)
ax5 = plt.subplot(2, 3, 5)
soils = df['Soil_Type'].unique()
soil_probs = [quick_conditional_probability('Soil_Type', s) for s in soils]
bars5 = ax5.bar(soils, soil_probs, color=['#e67e22', '#27ae60', '#8e44ad', '#d35400'])
ax5.set_title('$P(High\\ Yield\ |\ Soil\\ Type)$', fontsize=11, fontweight='bold')
ax5.set_ylabel('Probability')
ax5.set_ylim(0, 1.2)
ax5.tick_params(axis='x', rotation=45)
for bar, prob, soil in zip(bars5, soil_probs, soils):
    count = len(df[df['Soil_Type'] == soil])
    high_count = len(df[(df['Soil_Type'] == soil) & (df['High_Yield'] == True)])
    ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.03,
             f'{high_count}/{count}\n={prob:.2f}', ha='center', va='bottom', fontsize=8)
ax6 = plt.subplot(2, 3, 6)
irrigation_labels = ['No Irrigation', 'Irrigation Used']
irrigation_probs = [
    quick_conditional_probability('Irrigation_Used', False),
    quick_conditional_probability('Irrigation_Used', True)
]
bars6 = ax6.bar(irrigation_labels, irrigation_probs, color=['#e74c3c', '#2ecc71'])
ax6.set_title('$P(High\\ Yield\ |\ Irrigation)$', fontsize=11, fontweight='bold')
ax6.set_ylabel('Probability')
ax6.set_ylim(0, 1.2)
for bar, prob, label in zip(bars6, irrigation_probs, irrigation_labels):
    count = len(df[df['Irrigation_Used'] == (label == 'Irrigation Used')])
    high_count = len(df[(df['Irrigation_Used'] == (label == 'Irrigation Used')) & (df['High_Yield'] == True)])
    ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.03,
             f'{high_count}/{count}\n={prob:.2f}', ha='center', va='bottom', fontsize=9)
plt.tight_layout()
plt.show()
print("PROBABILITY COMPARISON CHART")
print("="*60)
plt.figure(figsize=(14, 8))
all_labels = []
all_probs = []
for region in df['Region'].unique():
    prob = quick_conditional_probability('Region', region)
    count = len(df[df['Region'] == region])
    high_count = len(df[(df['Region'] == region) & (df['High_Yield'] == True)])
    all_labels.append(f"Region: {region} ({high_count}/{count})")
    all_probs.append(prob)
for weather in df['Weather_Condition'].unique():
    prob = quick_conditional_probability('Weather_Condition', weather)
    count = len(df[df['Weather_Condition'] == weather])
    high_count = len(df[(df['Weather_Condition'] == weather) & (df['High_Yield'] == True)])
    all_labels.append(f"Weather: {weather} ({high_count}/{count})")
    all_probs.append(prob)
for fertilizer in [True, False]:
    prob = quick_conditional_probability('Fertilizer_Used', fertilizer)
    count = len(df[df['Fertilizer_Used'] == fertilizer])
    high_count = len(df[(df['Fertilizer_Used'] == fertilizer) & (df['High_Yield'] == True)])
    status = "Used" if fertilizer else "Not Used"
    all_labels.append(f"Fertilizer: {status} ({high_count}/{count})")
    all_probs.append(prob)
for crop in df['Crop'].unique():
    prob = quick_conditional_probability('Crop', crop)
    count = len(df[df['Crop'] == crop])
    high_count = len(df[(df['Crop'] == crop) & (df['High_Yield'] == True)])
    all_labels.append(f"Crop: {crop} ({high_count}/{count})")
    all_probs.append(prob)
for soil in df['Soil_Type'].unique():
    prob = quick_conditional_probability('Soil_Type', soil)
    count = len(df[df['Soil_Type'] == soil])
    high_count = len(df[(df['Soil_Type'] == soil) & (df['High_Yield'] == True)])
    all_labels.append(f"Soil: {soil} ({high_count}/{count})")
    all_probs.append(prob)
sorted_indices = np.argsort(all_probs)[::-1]
sorted_labels = [all_labels[i] for i in sorted_indices]
sorted_probs = [all_probs[i] for i in sorted_indices]
bars = plt.barh(sorted_labels, sorted_probs, color=plt.cm.viridis(np.linspace(0.2, 0.9, len(sorted_labels))))
for bar, prob in zip(bars, sorted_probs):
    plt.text(prob + 0.02, bar.get_y() + bar.get_height()/2,
             f'{prob:.2f}', va='center', fontsize=10)
plt.title('All Conditional Probabilities: P(High Yield | Condition)\n' +
          'Values show: Probability (High Yield Cases / Total Cases)',
          fontsize=14, fontweight='bold')
plt.xlabel('Probability')
plt.xlim(0, 1.2)
plt.axvline(x=df['High_Yield'].mean(), color='red', linestyle='--',
            alpha=0.7, label=f'Overall Average: {df["High_Yield"].mean():.2f}')
plt.axvline(x=0.5, color='gray', linestyle=':', alpha=0.5)
plt.legend()
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()
print("STATISTICAL SUMMARY")
print("="*60)
print("\nCONDITION WITH HIGHEST PROBABILITY:")
max_prob_idx = np.argmax(all_probs)
print(f"  {sorted_labels[0].split('(')[0].strip()}")
print(f"  P = {sorted_probs[0]:.3f} ({sorted_probs[0]*100:.1f}%)")
print("\nCONDITION WITH LOWEST PROBABILITY:")
min_prob_idx = np.argmin(all_probs)
print(f"  {sorted_labels[-1].split('(')[0].strip()}")
print(f"  P = {sorted_probs[-1]:.3f} ({sorted_probs[-1]*100:.1f}%)")
print("\nCONDITIONS ABOVE OVERALL AVERAGE:")
overall_avg = df['High_Yield'].mean()
above_avg = [(label, prob) for label, prob in zip(all_labels, all_probs) if prob > overall_avg]
for label, prob in above_avg:
    print(f"  {label.split('(')[0].strip()}: {prob:.3f}")
print("INTERPRETATION:")
print("="*60)
print("""
1. P(High Yield | Condition) > Overall Average (0.50):
   - The condition increases the likelihood of high yield
2. P(High Yield | Condition) < Overall Average (0.50):
   - The condition decreases the likelihood of high yield
3. P(High Yield | Condition) = 1.00:
   - All cases with this condition resulted in high yield
4. P(High Yield | Condition) = 0.00:
   - No cases with this condition resulted in high yield
""")
print("RECOMMENDATIONS BASED ON ANALYSIS:")
print("="*60)
print("""
1. FOCUS ON CONDITIONS WITH HIGH PROBABILITY:
   - Prioritize conditions that show P > 0.50
2. AVOID CONDITIONS WITH LOW PROBABILITY:
   - Minimize conditions that show P < 0.50
3. CONSIDER SAMPLE SIZE:
   - Probabilities based on small samples may not be reliable
   - Look for patterns across multiple conditions
""")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646198956255336,False,True,Rainy,140,3.7072931271974823"""
df = pd.read_csv(pd.io.common.StringIO(data))
df['Rainfall_mm'] = pd.to_numeric(df['Rainfall_mm'])
df['Temperature_Celsius'] = pd.to_numeric(df['Temperature_Celsius'])
df['Days_to_Harvest'] = pd.to_numeric(df['Days_to_Harvest'])
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
print("\n**View Data (First 5 rows):**")
print(df.head())
print("\n**Total Data:**", len(df), "records")
average_yield = df['Yield_tons_per_hectare'].mean()
df['High_Yield'] = df['Yield_tons_per_hectare'] > average_yield
print(f"\n**Average Yield:** {average_yield:.2f} tons/hectare")
print(f"**High Yield Cases:** {df['High_Yield'].sum()} cases")
print(f"**Low Yield Cases:** {len(df) - df['High_Yield'].sum()} cases")
def simple_probability(condition_column, condition_value):
    """
    Formula: P(High_Yield | Condition) = (High_Yield AND Condition cases) / (All Condition cases)
    Example:
    P(High_Yield | Region='South') =
    (South region high yield cases) / (All South region cases)
    """
    all_condition_cases = df[df[condition_column] == condition_value]
    high_yield_and_condition = all_condition_cases[all_condition_cases['High_Yield'] == True]
    if len(all_condition_cases) == 0:
        return 0
    probability = len(high_yield_and_condition) / len(all_condition_cases)
    return probability
print("**High Yield Probability for Different Conditions**")
print("\n**By Region:**")
for region in df['Region'].unique():
    prob = simple_probability('Region', region)
    cases = len(df[df['Region'] == region])
    high_cases = len(df[(df['Region'] == region) & (df['High_Yield'] == True)])
    print(f"  P(High_Yield | Region = {region}) = {high_cases}/{cases} = {prob:.2f} ({prob*100:.0f}%)")
print("\n**By Weather:**")
for weather in df['Weather_Condition'].unique():
    prob = simple_probability('Weather_Condition', weather)
    cases = len(df[df['Weather_Condition'] == weather])
    high_cases = len(df[(df['Weather_Condition'] == weather) & (df['High_Yield'] == True)])
    print(f"  P(High_Yield | Weather = {weather}) = {high_cases}/{cases} = {prob:.2f}")
print("\n**Fertilizer Usage:**")
no_fertilizer_prob = simple_probability('Fertilizer_Used', False)
fertilizer_prob = simple_probability('Fertilizer_Used', True)
no_fertilizer_cases = len(df[df['Fertilizer_Used'] == False])
high_no_fertilizer = len(df[(df['Fertilizer_Used'] == False) & (df['High_Yield'] == True)])
fertilizer_cases = len(df[df['Fertilizer_Used'] == True])
high_fertilizer = len(df[(df['Fertilizer_Used'] == True) & (df['High_Yield'] == True)])
print(f"  Without Fertilizer: {high_no_fertilizer}/{no_fertilizer_cases} = {no_fertilizer_prob:.2f}")
print(f"  With Fertilizer: {high_fertilizer}/{fertilizer_cases} = {fertilizer_prob:.2f}")
print("**Creating Simple Graphs...**")
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
region_probabilities = [simple_probability('Region', r) for r in df['Region'].unique()]
bars1 = plt.bar(df['Region'].unique(), region_probabilities, color=['red', 'green', 'blue'])
plt.title('High Yield Probability by Region')
plt.ylabel('Probability')
plt.ylim(0, 1.1)
for bar, prob in zip(bars1, region_probabilities):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
             f'{prob:.2f}', ha='center', va='bottom')
plt.subplot(1, 2, 2)
weather_probabilities = [simple_probability('Weather_Condition', w) for w in df['Weather_Condition'].unique()]
bars2 = plt.bar(df['Weather_Condition'].unique(), weather_probabilities, color=['yellow', 'blue', 'gray'])
plt.title('High Yield Probability by Weather')
plt.ylabel('Probability')
plt.ylim(0, 1.1)
for bar, prob in zip(bars2, weather_probabilities):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
             f'{prob:.2f}', ha='center', va='bottom')
plt.tight_layout()
plt.show()
print("**Best and Worst Conditions**")
all_conditions = []
all_probabilities = []
for region in df['Region'].unique():
    all_conditions.append(f"Region: {region}")
    all_probabilities.append(simple_probability('Region', region))
for weather in df['Weather_Condition'].unique():
    all_conditions.append(f"Weather: {weather}")
    all_probabilities.append(simple_probability('Weather_Condition', weather))
for fertilizer in [False, True]:
    status = "No Fertilizer" if fertilizer == False else "Fertilizer Used"
    all_conditions.append(f"Fertilizer: {status}")
    all_probabilities.append(simple_probability('Fertilizer_Used', fertilizer))
for crop in df['Crop'].unique():
    all_conditions.append(f"Crop: {crop}")
    all_probabilities.append(simple_probability('Crop', crop))
max_probability = max(all_probabilities)
min_probability = min(all_probabilities)
best_condition = all_conditions[all_probabilities.index(max_probability)]
worst_condition = all_conditions[all_probabilities.index(min_probability)]
print(f"**Highest Probability:** {best_condition} = {max_probability:.2f}")
print(f"**Lowest Probability:** {worst_condition} = {min_probability:.2f}")
print("**How to Make Decisions?**")
print("""
**Simple Rules:**
1. **Probability > 0.50:** Good Condition
   - Higher chance of high yield
2. **Probability < 0.50:** Bad Condition
   - Lower chance of high yield
3. **Probability = 1.00:** Best Condition
   - All cases resulted in high yield
4. **Probability = 0.00:** Avoid
   - No cases resulted in high yield
""")
print("\n **Choose These Conditions (Probability > 0.50):**")
for condition, probability in zip(all_conditions, all_probabilities):
    if probability > 0.50:
        print(f"  - {condition}: {probability:.2f}")
print("\n**Avoid These Conditions (Probability < 0.50):**")
for condition, probability in zip(all_conditions, all_probabilities):
    if probability < 0.50:
        print(f"  - {condition}: {probability:.2f}")

"""**2. Independence of Events**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646966373377603,False,True,Rainy,140,3.7072931271974823"""
df = pd.read_csv(pd.io.common.StringIO(data))
df['Rainfall_mm'] = pd.to_numeric(df['Rainfall_mm'])
df['Temperature_Celsius'] = pd.to_numeric(df['Temperature_Celsius'])
df['Days_to_Harvest'] = pd.to_numeric(df['Days_to_Harvest'])
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
print("INDEPENDENCE CHECK: Are Events Independent or Dependent?")
print("Formula: Two events A and B are independent if P(A ∩ B) = P(A) × P(B)")
print("="*70)
average_yield = df['Yield_tons_per_hectare'].mean()
df['High_Yield'] = df['Yield_tons_per_hectare'] > average_yield
P_A = df['High_Yield'].mean()
total_cases = len(df)
high_yield_cases = df['High_Yield'].sum()
print(f"\nEVENT A: High Yield (Yield > {average_yield:.2f})")
print(f"   P(A) = {high_yield_cases}/{total_cases} = {P_A:.3f}")
def check_independence(condition_column, condition_value, event_name="B"):
    """
    Check if High_Yield (A) and Condition (B) are independent
    Formula: P(A ∩ B) = P(A) × P(B) → Independent
             Otherwise → Dependent
    """
    total = len(df)
    condition_cases = len(df[df[condition_column] == condition_value])
    P_B = condition_cases / total
    both_cases = len(df[(df[condition_column] == condition_value) & (df['High_Yield'] == True)])
    P_A_and_B = both_cases / total
    # Calculate P(A) × P(B)
    P_A_times_P_B = P_A * P_B
    # Check independence
    is_independent = np.isclose(P_A_and_B, P_A_times_P_B, rtol=1e-10, atol=1e-10)
    # Calculate difference
    difference = abs(P_A_and_B - P_A_times_P_B)
    return {
        'Condition': f"{condition_column} = {condition_value}",
        'P(A)': P_A,
        'P(B)': P_B,
        'P(A ∩ B)': P_A_and_B,
        'P(A) × P(B)': P_A_times_P_B,
        'Difference': difference,
        'Independent?': 'YES' if is_independent else '❌ NO',
        'Relationship': 'Independent' if is_independent else 'Dependent' }
print("CHECKING INDEPENDENCE FOR DIFFERENT CONDITIONS:")
print("="*70)
all_results = []
print("\nREGION CONDITIONS:")
for region in df['Region'].unique():
    result = check_independence('Region', region, f"Region={region}")
    all_results.append(result)
    print(f"\n{result['Condition']}:")
    print(f"  P(A) = P(High Yield) = {result['P(A)']:.3f}")
    print(f"  P(B) = P({result['Condition']}) = {result['P(B)']:.3f}")
    print(f"  P(A ∩ B) = {result['P(A ∩ B)']:.3f}")
    print(f"  P(A) × P(B) = {result['P(A)']:.3f} × {result['P(B)']:.3f} = {result['P(A) × P(B)']:.3f}")
    print(f"  Difference = {result['Difference']:.4f}")
    print(f"  Independent? {result['Independent?']}")
print("\nWEATHER CONDITIONS:")
for weather in df['Weather_Condition'].unique():
    result = check_independence('Weather_Condition', weather, f"Weather={weather}")
    all_results.append(result)
    print(f"\n{result['Condition']}:")
    print(f"  P(A ∩ B) = {result['P(A ∩ B)']:.3f}")
    print(f"  P(A) × P(B) = {result['P(A) × P(B)']:.3f}")
    print(f"  Independent? {result['Independent?']}")
print("\nFERTILIZER USAGE:")
for fertilizer in [True, False]:
    status = "Fertilizer Used" if fertilizer else "No Fertilizer"
    result = check_independence('Fertilizer_Used', fertilizer, status)
    all_results.append(result)
    print(f"\n{status}:")
    print(f"  P(A ∩ B) = {result['P(A ∩ B)']:.3f}")
    print(f"  P(A) × P(B) = {result['P(A) × P(B)']:.3f}")
    print(f"  Independent? {result['Independent?']}")
print("\nIRRIGATION USAGE:")
for irrigation in [True, False]:
    status = "Irrigation Used" if irrigation else "No Irrigation"
    result = check_independence('Irrigation_Used', irrigation, status)
    all_results.append(result)
    print(f"\n{status}:")
    print(f"  P(A ∩ B) = {result['P(A ∩ B)']:.3f}")
    print(f"  P(A) × P(B) = {result['P(A) × P(B)']:.3f}")
    print(f"  Independent? {result['Independent?']}")
print("\nCROP TYPES:")
for crop in df['Crop'].unique():
    result = check_independence('Crop', crop, f"Crop={crop}")
    all_results.append(result)
    print(f"\n{crop}:")
    print(f"  Independent? {result['Independent?']}")
results_df = pd.DataFrame(all_results)
print("SUMMARY OF INDEPENDENCE ANALYSIS:")
print("="*70)
print(f"\nTotal Conditions Checked: {len(results_df)}")
print(f"Independent Events: {(results_df['Independent?'] == 'YES').sum()}")
print(f"Dependent Events: {(results_df['Independent?'] == 'NO').sum()}")
# Create visualization
plt.figure(figsize=(14, 8))
# Plot 1: Comparison of P(A ∩ B) vs P(A) × P(B)
plt.subplot(2, 2, 1)
conditions = results_df['Condition'].str.slice(0, 20) + "..."
x = range(len(conditions))
plt.bar(x, results_df['P(A ∩ B)'], width=0.4, label='P(A ∩ B)', color='blue', alpha=0.7)
plt.bar([i + 0.4 for i in x], results_df['P(A) × P(B)'], width=0.4, label='P(A) × P(B)', color='red', alpha=0.7)
plt.title('Comparison: P(A ∩ B) vs P(A) × P(B)')
plt.xlabel('Conditions')
plt.ylabel('Probability')
plt.xticks([i + 0.2 for i in x], conditions, rotation=45, ha='right')
plt.legend()
plt.grid(axis='y', alpha=0.3)
# Plot 2: Differences (how far from independence)
plt.subplot(2, 2, 2)
colors = ['green' if ind == 'YES' else 'red' for ind in results_df['Independent?']]
bars = plt.bar(x, results_df['Difference'], color=colors, alpha=0.7)
plt.axhline(y=0.05, color='orange', linestyle='--', label='Threshold (0.05)')
plt.title('Difference: |P(A ∩ B) - P(A) × P(B)|')
plt.xlabel('Conditions')
plt.ylabel('Difference')
plt.xticks(x, conditions, rotation=45, ha='right')
plt.legend()
plt.grid(axis='y', alpha=0.3)
# Plot 3: Count of Independent vs Dependent
plt.subplot(2, 2, 3)
ind_vs_dep = results_df['Relationship'].value_counts()
plt.pie(ind_vs_dep.values, labels=ind_vs_dep.index, autopct='%1.1f%%',
        colors=['lightgreen', 'lightcoral'], startangle=90)
plt.title('Proportion: Independent vs Dependent Events')
# Plot 4: Scatter plot of P(A ∩ B) vs P(A) × P(B)
plt.subplot(2, 2, 4)
plt.scatter(results_df['P(A) × P(B)'], results_df['P(A ∩ B)'],
           c=['green' if ind == 'YES' else 'red' for ind in results_df['Independent?']],
           s=100, alpha=0.7)
# Add diagonal line (where P(A ∩ B) = P(A) × P(B))
min_val = min(results_df['P(A) × P(B)'].min(), results_df['P(A ∩ B)'].min())
max_val = max(results_df['P(A) × P(B)'].max(), results_df['P(A ∩ B)'].max())
plt.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5, label='Perfect Independence Line')
plt.title('Scatter: P(A ∩ B) vs P(A) × P(B)')
plt.xlabel('P(A) × P(B)')
plt.ylabel('P(A ∩ B)')
plt.legend()
plt.grid(alpha=0.3)
plt.suptitle('INDEPENDENCE ANALYSIS: Are High Yield and Other Events Independent?',
             fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()
print("INTERPRETATION GUIDE:")
print("="*70)
print("""
WHAT DOES INDEPENDENCE MEAN?
============================
Two events A and B are INDEPENDENT if:
  P(A ∩ B) = P(A) × P(B)
This means:
1. Knowing B occurred doesn't change the probability of A
2. A and B have no relationship
3. They occur by chance, not because of each other
WHAT DOES DEPENDENCE MEAN?
==========================
If P(A ∩ B) ≠ P(A) × P(B), then events are DEPENDENT:
1. Knowing B occurred CHANGES the probability of A
2. A and B are related
3. One event affects the other
IN OUR CASE:
============
Event A = High Yield (Yield > Average)
Event B = Various conditions (Region, Weather, etc.)
IF INDEPENDENT (YES):
  - The condition doesn't affect high yield probability
  - Example: If weather is rainy, high yield probability stays same
IF DEPENDENT (NO):
  - The condition AFFECTS high yield probability
  - Example: If region is South, high yield probability changes
""")
print("MOST DEPENDENT CONDITIONS (Highest Differences):")
print("="*70)
most_dependent = results_df.nlargest(5, 'Difference')
for idx, row in most_dependent.iterrows():
    print(f"\n{row['Condition']}:")
    print(f"  P(A ∩ B) = {row['P(A ∩ B)']:.3f}")
    print(f"  P(A) × P(B) = {row['P(A) × P(B)']:.3f}")
    print(f"  Difference = {row['Difference']:.3f}")
    print(f"  Status: {row['Independent?']}")
print("IMPORTANT NOTE ABOUT SMALL SAMPLE SIZE:")
print("="*70)

print(f"""
WARNING: We have only {total_cases} data points!
This is a very small sample for statistical independence testing.
With small samples:
1. Results may not be statistically significant
2. Random chance can create apparent dependence/independence
3. We need more data for reliable conclusions
For reliable independence testing, we typically need:
- At least 30-50 data points for each condition
- Better: 100+ data points
RECOMMENDATION:
- Treat these results as preliminary
- Collect more data for accurate analysis
- Use these insights as hypotheses to test with larger datasets
""")
print("PRACTICAL IMPLICATIONS:")
print("="*70)
print("""
IF DEPENDENT (NO):
=====================
TAKE ACTION: These conditions MATTER for high yield
CONSIDER: Adjust farming practices based on these conditions
EXAMPLE: If South region gives better yields, focus resources there
IF INDEPENDENT (YES):
========================
NO EFFECT: These conditions DON'T affect high yield probability
SAVE RESOURCES: Don't waste time/money on these factors
FOCUS ELSEWHERE: Look for other factors that actually matter
BOTTOM LINE:
============
Use dependency analysis to:
1. Focus on what REALLY matters for high yield
2. Avoid wasting resources on irrelevant factors
3. Make data-driven farming decisions
""")

"""**3. Bayes’ Rule**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646966373377603,False,True,Rainy,140,3.7072931271974823"""
df = pd.read_csv(pd.io.common.StringIO(data))
df['Rainfall_mm'] = pd.to_numeric(df['Rainfall_mm'])
df['Temperature_Celsius'] = pd.to_numeric(df['Temperature_Celsius'])
df['Days_to_Harvest'] = pd.to_numeric(df['Days_to_Harvest'])
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
print("BAYES' THEOREM ANALYSIS FOR AGRICULTURAL DATA")
print("Formula: P(B|A) = [P(A|B) × P(B)] / P(A)")
print("="*80)
print("\n**Dataset Overview:**")
print(f"Total records: {len(df)}")
print(df[['Region', 'Crop', 'Yield_tons_per_hectare']].head())
average_yield = df['Yield_tons_per_hectare'].mean()
df['High_Yield'] = df['Yield_tons_per_hectare'] > average_yield
P_A = df['High_Yield'].mean()
print(f"\n**Event A: High Yield (Yield > {average_yield:.2f} tons/hectare)**")
print(f"P(A) = Probability of High Yield = {df['High_Yield'].sum()}/{len(df)} = {P_A:.3f}")
def conditional_probability(event_column, event_value, given_column, given_value):
    """Calculate P(event | given)"""
    total_given = len(df[df[given_column] == given_value])
    if total_given == 0:
        return 0
    both_cases = len(df[(df[given_column] == given_value) & (df[event_column] == event_value)])
    return both_cases / total_given
# BAYES' THEOREM FUNCTION
def bayes_theorem(event_B_column, event_B_value, event_A='High_Yield'):
    """
    Calculate P(B|A) using Bayes' Theorem
    P(B|A) = [P(A|B) × P(B)] / P(A)
    """
    total_cases = len(df)
    # P(A) - Already calculated
    P_A_value = P_A
    # P(B) - Prior probability of B
    B_cases = len(df[df[event_B_column] == event_B_value])
    P_B = B_cases / total_cases
    # P(A|B) - Likelihood
    P_A_given_B = conditional_probability(event_A, True, event_B_column, event_B_value)
    # P(B|A) - Posterior probability (what we want)
    if P_A_value > 0:
        P_B_given_A = (P_A_given_B * P_B) / P_A_value
    else:
        P_B_given_A = 0
    return {
        'Event B': f"{event_B_column} = {event_B_value}",
        'P(B)': P_B,
        'P(A|B)': P_A_given_B,
        'P(B|A)': P_B_given_A,
        'Bayes Calculation': f"({P_A_given_B:.3f} × {P_B:.3f}) / {P_A_value:.3f} = {P_B_given_A:.3f}"
    }
print("MEDICAL DIAGNOSIS ANALOGY")
print("="*80)
print("""
In medical testing:
- A = Test Positive
- B = Have Disease
Bayes' Theorem: P(Disease|Positive) = [P(Positive|Disease) × P(Disease)] / P(Positive)
In our case:
- A = High Yield (what we observe)
- B = Specific condition (what we want to infer)
""")
print("\n**Example: Disease Testing Scenario**")
disease_prevalence = 0.01  # 1% of population has disease
test_sensitivity = 0.95    # P(Positive|Disease) = 95%
test_specificity = 0.90    # P(Negative|No Disease) = 90%
P_Positive = (test_sensitivity * disease_prevalence) + ((1 - test_specificity) * (1 - disease_prevalence))
P_Disease_given_Positive = (test_sensitivity * disease_prevalence) / P_Positive
print(f"Disease prevalence P(Disease): {disease_prevalence:.3f}")
print(f"Test sensitivity P(Positive|Disease): {test_sensitivity:.3f}")
print(f"Test specificity P(Negative|No Disease): {test_specificity:.3f}")
print(f"P(Positive): {P_Positive:.3f}")
print(f"\nP(Disease|Positive) = ({test_sensitivity:.3f} × {disease_prevalence:.3f}) / {P_Positive:.3f}")
print(f"                    = {P_Disease_given_Positive:.3f} ({P_Disease_given_Positive*100:.1f}%)")
print("BAYES' THEOREM APPLIED TO AGRICULTURAL DATA")
print("Question: Given that we have HIGH YIELD, what's the probability it came from a specific condition?")
bayes_results = []
print("\n**REGION ANALYSIS:**")
for region in df['Region'].unique():
    result = bayes_theorem('Region', region)
    bayes_results.append(result)
    print(f"\n{result['Event B']}:")
    print(f"  P(B) = P({region}) = {result['P(B)']:.3f}")
    print(f"  P(A|B) = P(High Yield | {region}) = {result['P(A|B)']:.3f}")
    print(f"  P(B|A) = P({region} | High Yield) = {result['P(B|A)']:.3f}")
    print(f"  Bayes: {result['Bayes Calculation']}")
print("\n**WEATHER ANALYSIS:**")
for weather in df['Weather_Condition'].unique():
    result = bayes_theorem('Weather_Condition', weather)
    bayes_results.append(result)
    print(f"\n{weather} weather:")
    print(f"  P(Weather={weather} | High Yield) = {result['P(B|A)']:.3f}")
print("\n**FERTILIZER ANALYSIS:**")
for fert in [True, False]:
    status = "Fertilizer Used" if fert else "No Fertilizer"
    result = bayes_theorem('Fertilizer_Used', fert)
    bayes_results.append(result)
    print(f"\n{status}:")
    print(f"  P({status} | High Yield) = {result['P(B|A)']:.3f}")
print("\n**IRRIGATION ANALYSIS:**")
for irrig in [True, False]:
    status = "Irrigation Used" if irrig else "No Irrigation"
    result = bayes_theorem('Irrigation_Used', irrig)
    bayes_results.append(result)
    print(f"\n{status}:")
    print(f"  P({status} | High Yield) = {result['P(B|A)']:.3f}")
print("BAYESIAN POSTERIOR PROBABILITIES VISUALIZATION")
print("="*80)
# Create DataFrame from results
bayes_df = pd.DataFrame(bayes_results)
plt.figure(figsize=(15, 10))
# Plot 1: Comparison of Prior P(B) vs Posterior P(B|A)
plt.subplot(2, 2, 1)
x = range(len(bayes_df))
width = 0.35
plt.bar([i - width/2 for i in x], bayes_df['P(B)'], width, label='Prior P(B)', color='blue', alpha=0.7)
plt.bar([i + width/2 for i in x], bayes_df['P(B|A)'], width, label='Posterior P(B|A)', color='green', alpha=0.7)
plt.axhline(y=1.0, color='red', linestyle='--', alpha=0.5, label='Probability = 1.0')
plt.title('Bayesian Update: Prior vs Posterior Probabilities')
plt.xlabel('Conditions')
plt.ylabel('Probability')
plt.xticks(x, [b[:20] + "..." for b in bayes_df['Event B']], rotation=45, ha='right')
plt.legend()
plt.grid(axis='y', alpha=0.3)
# Plot 2: Bayes Factor = P(B|A) / P(B)
plt.subplot(2, 2, 2)
bayes_df['Bayes_Factor'] = bayes_df['P(B|A)'] / bayes_df['P(B)'].replace(0, np.nan)
bayes_df['Evidence'] = ['Strong for' if bf > 3 else 'Weak for' if bf > 1 else 'Against' for bf in bayes_df['Bayes_Factor']]
colors = ['green' if bf > 1 else 'red' for bf in bayes_df['Bayes_Factor']]
bars = plt.bar(x, bayes_df['Bayes_Factor'], color=colors, alpha=0.7)
plt.axhline(y=1.0, color='black', linestyle='-', alpha=0.5, label='No change (BF=1)')
plt.axhline(y=3.0, color='orange', linestyle='--', alpha=0.5, label='Strong evidence (BF>3)')
plt.title('Bayes Factor: How much evidence for each condition?')
plt.xlabel('Conditions')
plt.ylabel('Bayes Factor = P(B|A) / P(B)')
plt.xticks(x, [b[:20] + "..." for b in bayes_df['Event B']], rotation=45, ha='right')
plt.legend()
plt.grid(axis='y', alpha=0.3)
# Plot 3: Likelihood P(A|B)
plt.subplot(2, 2, 3)
plt.bar(x, bayes_df['P(A|B)'], color='purple', alpha=0.7)
plt.axhline(y=P_A, color='red', linestyle='--', label=f'Base rate P(A) = {P_A:.2f}')
plt.title('Likelihood: P(High Yield | Condition)')
plt.xlabel('Conditions')
plt.ylabel('P(A|B)')
plt.xticks(x, [b[:20] + "..." for b in bayes_df['Event B']], rotation=45, ha='right')
plt.legend()
plt.grid(axis='y', alpha=0.3)
plt.subplot(2, 2, 4)
top_conditions = bayes_df.nlargest(8, 'P(B|A)')
top_x = range(len(top_conditions))
plt.barh(top_x, top_conditions['P(B|A)'], color='lightgreen', alpha=0.7)
plt.axvline(x=P_A, color='red', linestyle='--', label=f'Base rate P(A) = {P_A:.2f}')
for i, (_, row) in enumerate(top_conditions.iterrows()):
    plt.text(row['P(B|A)'] + 0.02, i, f'{row["P(B|A)"]:.2f}', va='center')
plt.yticks(top_x, [b[:20] + "..." for b in top_conditions['Event B']])
plt.title('Top Conditions Given High Yield (Highest P(B|A))')
plt.xlabel('P(Condition | High Yield)')
plt.legend()
plt.suptitle('Bayesian Analysis: What Conditions Lead to High Yield?', fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()
print("🤖 MACHINE LEARNING CLASSIFICATION APPLICATION")
print("="*80)
print("""
Naive Bayes Classifier Concept:
- We want to predict if a new farm will have HIGH YIELD
- Using Bayes' Theorem for each feature independently
""")
def naive_bayes_predict(new_farm):
    """
    Simplified Naive Bayes prediction
    P(High Yield | Features) ∝ P(High Yield) × ∏ P(Feature | High Yield)
    """
    prior = P_A
    likelihood = 1.0
    print(f"\nPredicting for new farm:")
    for feature, value in new_farm.items():
        if feature in df.columns:
            # Count how many high yield cases have this feature value
            high_yield_cases = len(df[df['High_Yield'] == True])
            feature_in_high_yield = len(df[(df[feature] == value) & (df['High_Yield'] == True)])
            if high_yield_cases > 0:
                feature_likelihood = feature_in_high_yield / high_yield_cases
            else:
                feature_likelihood = 0.001  # Small smoothing factor
            likelihood *= feature_likelihood
            print(f"  P({feature}={value} | High Yield) = {feature_in_high_yield}/{high_yield_cases} = {feature_likelihood:.3f}")
    unnormalized_posterior = prior * likelihood
    prior_low = 1 - P_A
    likelihood_low = 1.0
    for feature, value in new_farm.items():
        if feature in df.columns:
            low_yield_cases = len(df[df['High_Yield'] == False])
            feature_in_low_yield = len(df[(df[feature] == value) & (df['High_Yield'] == False)])
            if low_yield_cases > 0:
                feature_likelihood_low = feature_in_low_yield / low_yield_cases
            else:
                feature_likelihood_low = 0.001
            likelihood_low *= feature_likelihood_low
    unnormalized_posterior_low = prior_low * likelihood_low
    total = unnormalized_posterior + unnormalized_posterior_low
    if total > 0:
        prob_high_yield = unnormalized_posterior / total
    else:
        prob_high_yield = 0.5
    return prob_high_yield
print("\n" + "-" * 40)
print("EXAMPLE PREDICTION 1:")
new_farm_1 = {'Region': 'South', 'Weather_Condition': 'Rainy', 'Fertilizer_Used': True}
prediction_1 = naive_bayes_predict(new_farm_1)
print(f"\nPredicted probability of High Yield: {prediction_1:.3f} ({prediction_1*100:.1f}%)")
if prediction_1 > 0.5:
    print("Prediction: HIGH YIELD")
else:
    print("Prediction: LOW YIELD")
print("\n" + "-" * 40)
print("EXAMPLE PREDICTION 2:")
new_farm_2 = {'Region': 'North', 'Weather_Condition': 'Sunny', 'Fertilizer_Used': False}
prediction_2 = naive_bayes_predict(new_farm_2)
print(f"\nPredicted probability of High Yield: {prediction_2:.3f} ({prediction_2*100:.1f}%)")
if prediction_2 > 0.5:
    print("Prediction: HIGH YIELD")
else:
    print("Prediction: LOW YIELD")
print("SPAM FILTERING ANALOGY")
print("="*80)
print("""
In spam filtering:
- A = Email is spam
- B = Email contains certain words
Bayes' Theorem: P(Spam|Word) = [P(Word|Spam) × P(Spam)] / P(Word)
Similarly in agriculture:
- A = High Yield
- B = Certain farming conditions
""")
print("BAYESIAN INFERENCE")
print("="*80)
summary_df = bayes_df.copy()
summary_df['Prior P(B)'] = summary_df['P(B)']
summary_df['Likelihood P(A|B)'] = summary_df['P(A|B)']
summary_df['Posterior P(B|A)'] = summary_df['P(B|A)']
summary_df['Bayes Factor'] = summary_df['P(B|A)'] / summary_df['P(B)'].replace(0, np.nan)
summary_df['Evidence Strength'] = summary_df['Bayes Factor'].apply(
    lambda x: 'Strong' if x > 3 else 'Moderate' if x > 1.5 else 'Weak' if x > 1 else 'Negative'
)
print(summary_df[['Event B', 'Prior P(B)', 'Likelihood P(A|B)',
                  'Posterior P(B|A)', 'Bayes Factor', 'Evidence Strength']].to_string(index=False))
print("💡 PRACTICAL INSIGHTS FROM BAYESIAN ANALYSIS")
print("="*80)
print("""
1. **HIGH POSTERIOR PROBABILITY** (P(B|A) > P(B)):
   - Condition is MORE COMMON in high yield cases than in general
   - Example: If P(South | High Yield) > P(South), South region is associated with high yield
2. **BAYES FACTOR > 1**:
   - Evidence SUPPORTS this condition being associated with high yield
   - Higher Bayes factor = stronger evidence
3. **BAYES FACTOR < 1**:
   - Evidence AGAINST this condition being associated with high yield
4. **MACHINE LEARNING APPLICATION**:
   - We can build a classifier to predict high yield based on conditions
   - Uses Bayes' Theorem to combine evidence from multiple features
5. **DECISION MAKING**:
   - Focus resources on conditions with high P(B|A)
   - Avoid conditions with low P(B|A) unless other factors compensate
""")
max_bf = summary_df['Bayes Factor'].max()
min_bf = summary_df['Bayes Factor'].min()
best_condition = summary_df.loc[summary_df['Bayes Factor'].idxmax()]
worst_condition = summary_df.loc[summary_df['Bayes Factor'].idxmin()]
print(f"\nSTRONGEST EVIDENCE FOR HIGH YIELD:")
print(f"   Condition: {best_condition['Event B']}")
print(f"   Bayes Factor: {best_condition['Bayes Factor']:.2f}")
print(f"   P(High Yield) = {P_A:.2f}, P(High Yield | Condition) = {best_condition['Likelihood P(A|B)']:.2f}")
print(f"\nSTRONGEST EVIDENCE AGAINST HIGH YIELD:")
print(f"   Condition: {worst_condition['Event B']}")
print(f"   Bayes Factor: {worst_condition['Bayes Factor']:.2f}")
print(f"   P(High Yield) = {P_A:.2f}, P(High Yield | Condition) = {worst_condition['Likelihood P(A|B)']:.2f}")
print("\n" + "="*80)
print("RECOMMENDATIONS FOR FARMERS:")
# Generate recommendations based on Bayesian analysis
high_posterior_conditions = summary_df[summary_df['Posterior P(B|A)'] > summary_df['Prior P(B)']]
print("\nFOCUS ON THESE CONDITIONS (Associated with High Yield):")
for _, row in high_posterior_conditions.nlargest(5, 'Bayes Factor').iterrows():
    improvement = (row['Posterior P(B|A)'] - row['Prior P(B)']) * 100
    print(f"  • {row['Event B']}: {improvement:+.1f}% more likely in high yield cases")
low_posterior_conditions = summary_df[summary_df['Posterior P(B|A)'] < summary_df['Prior P(B)']]
print("\nRECONSIDER THESE CONDITIONS (Less associated with High Yield):")
for _, row in low_posterior_conditions.nsmallest(3, 'Bayes Factor').iterrows():
    reduction = (row['Prior P(B)'] - row['Posterior P(B|A)']) * 100
    print(f"  • {row['Event B']}: {reduction:.1f}% less likely in high yield cases")

"""**4. Probability Distributions**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646966373377603,False,True,Rainy,140,3.7072931271974823"""
df = pd.read_csv(pd.io.common.StringIO(data))
df['Rainfall_mm'] = pd.to_numeric(df['Rainfall_mm'])
df['Temperature_Celsius'] = pd.to_numeric(df['Temperature_Celsius'])
df['Days_to_Harvest'] = pd.to_numeric(df['Days_to_Harvest'])
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
print("""
Continuous Random Variables:
• Can take ANY value within a range (e.g., 1.5, 2.718, 3.14159...)
• Represented by probability density functions (PDFs)
• Examples: Height, Weight, Temperature, Yield
The Normal Distribution (Gaussian Distribution):
• Most important continuous distribution in statistics
• Bell-shaped curve
• Described by two parameters: Mean (μ) and Standard Deviation (σ)
• Formula: f(x) = (1/(σ√(2π))) × e^(-(x-μ)²/(2σ²))
""")
yield_data = df['Yield_tons_per_hectare']
print(f"\n**Yield Data Analysis:**")
print(f"Sample size: {len(yield_data)} observations")
print(f"Range: {yield_data.min():.2f} to {yield_data.max():.2f} tons/hectare")
mean_yield = yield_data.mean()
std_yield = yield_data.std()
median_yield = yield_data.median()
skewness = yield_data.skew()
kurtosis = yield_data.kurtosis()
print(f"\n**Descriptive Statistics for Yield:**")
print(f"Mean (μ): {mean_yield:.3f} tons/hectare")
print(f"Standard Deviation (σ): {std_yield:.3f} tons/hectare")
print(f"Median: {median_yield:.3f} tons/hectare")
print(f"Skewness: {skewness:.3f} (Positive = right-skewed, Negative = left-skewed)")
print(f"Kurtosis: {kurtosis:.3f} (>3 = heavy tails, <3 = light tails)")
print("NORMALITY TESTS AND VISUAL CHECKS")
print("="*80)
shapiro_stat, shapiro_p = stats.shapiro(yield_data)
print(f"\n🔍 **Shapiro-Wilk Normality Test:**")
print(f"Test Statistic: {shapiro_stat:.4f}")
print(f"P-value: {shapiro_p:.4f}")
if shapiro_p > 0.05:
    print("Conclusion: Data appears normally distributed (p > 0.05)")
else:
    print("Conclusion: Data does NOT appear normally distributed (p ≤ 0.05)")
# QQ-Plot for visual normality check
print("\n**Creating Visual Normality Checks...**")
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('Normality Assessment for Crop Yield', fontsize=16, fontweight='bold', y=1.02)
# Plot 1: Histogram with normal curve
axes[0, 0].hist(yield_data, bins=8, density=True, alpha=0.7, color='skyblue', edgecolor='black')
xmin, xmax = axes[0, 0].get_xlim()
x = np.linspace(xmin, xmax, 100)
pdf = stats.norm.pdf(x, mean_yield, std_yield)
axes[0, 0].plot(x, pdf, 'r-', linewidth=2, label='Normal Distribution')
axes[0, 0].axvline(mean_yield, color='green', linestyle='--', label=f'Mean = {mean_yield:.2f}')
axes[0, 0].axvline(mean_yield + std_yield, color='orange', linestyle=':', label=f'±1σ')
axes[0, 0].axvline(mean_yield - std_yield, color='orange', linestyle=':')
axes[0, 0].set_title('Histogram with Normal Curve Overlay')
axes[0, 0].set_xlabel('Yield (tons/hectare)')
axes[0, 0].set_ylabel('Density')
axes[0, 0].legend()
# Plot 2: Q-Q Plot
stats.probplot(yield_data, dist="norm", plot=axes[0, 1])
axes[0, 1].get_lines()[0].set_marker('o')
axes[0, 1].get_lines()[0].set_markersize(5)
axes[0, 1].get_lines()[0].set_markerfacecolor('blue')
axes[0, 1].get_lines()[0].set_markeredgecolor('blue')
axes[0, 1].get_lines()[1].set_color('red')
axes[0, 1].get_lines()[1].set_linewidth(2)
axes[0, 1].set_title('Q-Q Plot for Normality Check')
axes[0, 1].set_xlabel('Theoretical Quantiles')
axes[0, 1].set_ylabel('Sample Quantiles')
# Plot 3: Box plot
axes[0, 2].boxplot(yield_data, vert=True, patch_artist=True,
                   boxprops=dict(facecolor='lightblue', color='darkblue'),
                   medianprops=dict(color='red', linewidth=2))
axes[0, 2].set_title('Box Plot of Yield Data')
axes[0, 2].set_ylabel('Yield (tons/hectare)')
axes[0, 2].set_xticklabels(['Yield'])
# Plot 4: Empirical Rule Check
axes[1, 0].hist(yield_data, bins=8, density=True, alpha=0.7, color='lightgreen', edgecolor='black')
# Shade areas for empirical rule
x_fill = np.linspace(mean_yield - std_yield, mean_yield + std_yield, 100)
axes[1, 0].fill_between(x_fill, stats.norm.pdf(x_fill, mean_yield, std_yield),
                        color='green', alpha=0.3, label='68% within 1σ')
x_fill2 = np.linspace(mean_yield - 2*std_yield, mean_yield + 2*std_yield, 100)
axes[1, 0].fill_between(x_fill2, stats.norm.pdf(x_fill2, mean_yield, std_yield),
                        color='yellow', alpha=0.2, label='95% within 2σ')
axes[1, 0].plot(x, pdf, 'r-', linewidth=2)
axes[1, 0].axvline(mean_yield, color='green', linestyle='--')
axes[1, 0].set_title('Empirical Rule (68-95-99.7 Rule)')
axes[1, 0].set_xlabel('Yield (tons/hectare)')
axes[1, 0].set_ylabel('Density')
axes[1, 0].legend()
# Plot 5: Density plot
sns.kdeplot(yield_data, ax=axes[1, 1], color='blue', linewidth=2, label='Actual Data')
axes[1, 1].plot(x, pdf, 'r-', linewidth=2, label='Normal Distribution')
axes[1, 1].fill_between(x, pdf, alpha=0.3, color='red')
axes[1, 1].set_title('Kernel Density Estimation vs Normal')
axes[1, 1].set_xlabel('Yield (tons/hectare)')
axes[1, 1].set_ylabel('Density')
axes[1, 1].legend()
# Plot 6: Cumulative Distribution Function (CDF)
sorted_yield = np.sort(yield_data)
cdf_actual = np.arange(1, len(sorted_yield) + 1) / len(sorted_yield)
axes[1, 2].plot(sorted_yield, cdf_actual, 'bo-', linewidth=2, markersize=5, label='Actual CDF')
# Theoretical normal CDF
cdf_theoretical = stats.norm.cdf(sorted_yield, mean_yield, std_yield)
axes[1, 2].plot(sorted_yield, cdf_theoretical, 'r-', linewidth=2, label='Theoretical Normal CDF')
axes[1, 2].set_title('Cumulative Distribution Function (CDF)')
axes[1, 2].set_xlabel('Yield (tons/hectare)')
axes[1, 2].set_ylabel('Cumulative Probability')
axes[1, 2].legend()
axes[1, 2].grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
print("EMPIRICAL RULE (68-95-99.7 RULE) DEMONSTRATION")
print("="*80)
print(f"""
The Empirical Rule for Normal Distributions:
• About 68% of data falls within 1 standard deviation of the mean
• About 95% of data falls within 2 standard deviations of the mean
• About 99.7% of data falls within 3 standard deviations of the mean
Mean (μ) = {mean_yield:.3f}
Standard Deviation (σ) = {std_yield:.3f}
""")
within_1_std = np.sum((yield_data >= mean_yield - std_yield) & (yield_data <= mean_yield + std_yield))
within_2_std = np.sum((yield_data >= mean_yield - 2*std_yield) & (yield_data <= mean_yield + 2*std_yield))
within_3_std = np.sum((yield_data >= mean_yield - 3*std_yield) & (yield_data <= mean_yield + 3*std_yield))
pct_1_std = within_1_std / len(yield_data) * 100
pct_2_std = within_2_std / len(yield_data) * 100
pct_3_std = within_3_std / len(yield_data) * 100
print(f"\n**Empirical Rule Check for Sample Data:**")
print(f"Data within μ ± 1σ ({mean_yield - std_yield:.2f} to {mean_yield + std_yield:.2f}):")
print(f"  Expected: ~68%, Actual: {within_1_std}/{len(yield_data)} = {pct_1_std:.1f}%")
print(f"\nData within μ ± 2σ ({mean_yield - 2*std_yield:.2f} to {mean_yield + 2*std_yield:.2f}):")
print(f"  Expected: ~95%, Actual: {within_2_std}/{len(yield_data)} = {pct_2_std:.1f}%")
print(f"\nData within μ ± 3σ ({mean_yield - 3*std_yield:.2f} to {mean_yield + 3*std_yield:.2f}):")
print(f"  Expected: ~99.7%, Actual: {within_3_std}/{len(yield_data)} = {pct_3_std:.1f}%")
print("Z-SCORES (STANDARD SCORES)")
print("="*80)
print("""
Z-Score Formula: z = (x - μ) / σ
Z-scores tell us how many standard deviations a value is from the mean:
• z = 0: Exactly at the mean
• z > 0: Above the mean
• z < 0: Below the mean
• |z| > 2: Unusual (in the tails of the distribution)
""")
df['Yield_z_score'] = (df['Yield_tons_per_hectare'] - mean_yield) / std_yield
print("\n**Z-Scores for Each Observation:**")
for idx, row in df.iterrows():
    yield_value = row['Yield_tons_per_hectare']
    z_score = row['Yield_z_score']
    interpretation = ""
    if abs(z_score) < 1:
        interpretation = "(Within 1 SD of mean)"
    elif abs(z_score) < 2:
        interpretation = f"({abs(z_score):.1f} SD from mean - somewhat unusual)"
    else:
        interpretation = f"({abs(z_score):.1f} SD from mean - very unusual)"
    print(f"Row {idx}: Yield = {yield_value:.2f}, Z-score = {z_score:.2f} {interpretation}")
print("PROBABILITY CALCULATIONS USING NORMAL DISTRIBUTION")
print("="*80)
print("""
We can calculate probabilities using the normal distribution:
1. P(Yield < X): Probability yield is less than X
2. P(Yield > X): Probability yield is greater than X
3. P(a < Yield < b): Probability yield is between a and b
""")
threshold_high = 7.0  # tons/hectare
threshold_low = 3.0   # tons/hectare
prob_less_than_7 = stats.norm.cdf(threshold_high, mean_yield, std_yield)
prob_greater_than_7 = 1 - prob_less_than_7
prob_between_3_and_7 = stats.norm.cdf(threshold_high, mean_yield, std_yield) - stats.norm.cdf(threshold_low, mean_yield, std_yield)
print(f"\n**Probability Calculations:**")
print(f"P(Yield < {threshold_high}) = {prob_less_than_7:.3f} or {prob_less_than_7*100:.1f}%")
print(f"P(Yield > {threshold_high}) = {prob_greater_than_7:.3f} or {prob_greater_than_7*100:.1f}%")
print(f"P({threshold_low} < Yield < {threshold_high}) = {prob_between_3_and_7:.3f} or {prob_between_3_and_7*100:.1f}%")
print("PERCENTILES AND QUANTILES")
print("="*80)
print("""
Percentiles divide the data into 100 equal parts.
Quantiles divide the data into equal-sized groups.
""")
percentiles = [10, 25, 50, 75, 90, 95]
percentile_values = np.percentile(yield_data, percentiles)
print(f"\n**Percentiles of Yield Data:**")
for p, val in zip(percentiles, percentile_values):
    print(f"  {p}th percentile: {val:.3f} tons/hectare")
q1 = np.percentile(yield_data, 25)
q2 = np.percentile(yield_data, 50)  # Median
q3 = np.percentile(yield_data, 75)
iqr = q3 - q1
print(f"\n**Quartiles and IQR:**")
print(f"  Q1 (25th percentile): {q1:.3f}")
print(f"  Q2 (50th percentile, Median): {q2:.3f}")
print(f"  Q3 (75th percentile): {q3:.3f}")
print(f"  IQR (Q3 - Q1): {iqr:.3f}")
print("SIMULATING NORMALLY DISTRIBUTED DATA")
print("="*80)
print(f"""
Simulating new yield data based on our sample statistics:
Using μ = {mean_yield:.3f}, σ = {std_yield:.3f}
""")
# Generate simulated data
np.random.seed(42)  # For reproducibility
simulated_yield = np.random.normal(mean_yield, std_yield, 1000)
# Create visualization comparing actual and simulated data
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
fig.suptitle('Actual vs Simulated Normally Distributed Yield Data', fontsize=14, fontweight='bold')
# Plot 1: Histogram comparison
axes[0].hist(yield_data, bins=8, density=True, alpha=0.7, label='Actual Data', color='blue', edgecolor='black')
axes[0].hist(simulated_yield, bins=30, density=True, alpha=0.5, label='Simulated Data', color='red', edgecolor='black')
axes[0].set_title('Histogram Comparison')
axes[0].set_xlabel('Yield (tons/hectare)')
axes[0].set_ylabel('Density')
axes[0].legend()
axes[0].grid(True, alpha=0.3)
box_data = [yield_data, simulated_yield]
axes[1].boxplot(box_data, labels=['Actual (n=10)', 'Simulated (n=1000)'],
                patch_artist=True,
                boxprops=dict(facecolor='lightblue', color='darkblue'),
                medianprops=dict(color='red', linewidth=2))
axes[1].set_title('Box Plot Comparison')
axes[1].set_ylabel('Yield (tons/hectare)')
axes[1].grid(True, alpha=0.3)
stats.probplot(simulated_yield, dist="norm", plot=axes[2])
axes[2].get_lines()[0].set_marker('o')
axes[2].get_lines()[0].set_markersize(3)
axes[2].get_lines()[0].set_markerfacecolor('red')
axes[2].get_lines()[0].set_markeredgecolor('red')
axes[2].get_lines()[1].set_color('green')
axes[2].get_lines()[1].set_linewidth(2)
axes[2].set_title('Q-Q Plot of Simulated Data')
axes[2].set_xlabel('Theoretical Quantiles')
axes[2].set_ylabel('Sample Quantiles')
plt.tight_layout()
plt.show()
print("CENTRAL LIMIT THEOREM (CLT) DEMONSTRATION")
print("="*80)
print("""
Central Limit Theorem:
• For large enough sample sizes, the distribution of sample means
  approaches a normal distribution, regardless of the population distribution
• Even if individual yields aren't normal, average yields from multiple
  farms will be approximately normal
""")
# Simulate CLT
np.random.seed(123)
sample_means = []
sample_sizes = [5, 10, 30]  # Different sample sizes
for n in sample_sizes:
    means = []
    for _ in range(1000):
        sample = np.random.choice(yield_data, n, replace=True)
        means.append(np.mean(sample))
    sample_means.append(means)
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
fig.suptitle('Central Limit Theorem: Distribution of Sample Means', fontsize=14, fontweight='bold')
for i, (means, n) in enumerate(zip(sample_means, sample_sizes)):
    axes[i].hist(means, bins=30, density=True, alpha=0.7, color='green', edgecolor='black')
    mean_of_means = np.mean(means)
    std_of_means = np.std(means)
    x = np.linspace(min(means), max(means), 100)
    pdf = stats.norm.pdf(x, mean_of_means, std_of_means)
    axes[i].plot(x, pdf, 'r-', linewidth=2)
    axes[i].set_title(f'Sample Size n = {n}')
    axes[i].set_xlabel(f'Mean Yield (n={n})')
    axes[i].set_ylabel('Density')
    axes[i].grid(True, alpha=0.3)
    clt_mean = mean_yield
    clt_std = std_yield / np.sqrt(n)
    axes[i].text(0.05, 0.95, f'CLT Prediction:\nμ = {clt_mean:.2f}\nσ/√n = {clt_std:.2f}',
                transform=axes[i].transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
plt.tight_layout()
plt.show()

"""**4.1 Normal Distribution**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646966373377603,False,True,Rainy,140,3.7072931271974823"""
df = pd.read_csv(pd.io.common.StringIO(data))
df['Rainfall_mm'] = pd.to_numeric(df['Rainfall_mm'])
df['Temperature_Celsius'] = pd.to_numeric(df['Temperature_Celsius'])
df['Days_to_Harvest'] = pd.to_numeric(df['Days_to_Harvest'])
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
print("NORMAL DISTRIBUTION ANALYSIS: X ∼ N(μ, σ²)")
print("="*80)
# Select Yield as our continuous variable
yield_data = df['Yield_tons_per_hectare']
# Calculate μ (mean) and σ (standard deviation)
μ = yield_data.mean()
σ = yield_data.std()
σ_squared = σ ** 2  # variance
print(f"\n**Normal Distribution Parameters:**")
print(f"Mean (μ) = {μ:.4f}")
print(f"Standard Deviation (σ) = {σ:.4f}")
print(f"Variance (σ²) = {σ_squared:.4f}")
print(f"\nOur Yield Distribution: X ∼ N({μ:.2f}, {σ_squared:.2f})")
print("68-95-99.7 RULE (Empirical Rule)")
print("="*80)
range_1σ = (μ - σ, μ + σ)
range_2σ = (μ - 2*σ, μ + 2*σ)
range_3σ = (μ - 3*σ, μ + 3*σ)
print(f"\n**1 Standard Deviation (68% of data):**")
print(f"   μ ± 1σ = {μ:.2f} ± {σ:.2f}")
print(f"   Range: [{μ-σ:.2f}, {μ+σ:.2f}]")
print(f"\n**2 Standard Deviations (95% of data):**")
print(f"   μ ± 2σ = {μ:.2f} ± {2*σ:.2f}")
print(f"   Range: [{μ-2*σ:.2f}, {μ+2*σ:.2f}]")
print(f"\n**3 Standard Deviations (99.7% of data):**")
print(f"   μ ± 3σ = {μ:.2f} ± {3*σ:.2f}")
print(f"   Range: [{μ-3*σ:.2f}, {μ+3*σ:.2f}]")
within_1σ = sum((yield_data >= μ-σ) & (yield_data <= μ+σ))
within_2σ = sum((yield_data >= μ-2*σ) & (yield_data <= μ+2*σ))
within_3σ = sum((yield_data >= μ-3*σ) & (yield_data <= μ+3*σ))
total = len(yield_data)
print(f"\n**Actual Data Check (n={total}):**")
print(f"   Within 1σ: {within_1σ}/{total} = {within_1σ/total*100:.1f}% (Expected: ~68%)")
print(f"   Within 2σ: {within_2σ}/{total} = {within_2σ/total*100:.1f}% (Expected: ~95%)")
print(f"   Within 3σ: {within_3σ}/{total} = {within_3σ/total*100:.1f}% (Expected: ~99.7%)")
# Create visualization of Normal Distribution
print("\n" + "="*80)
print("VISUALIZING NORMAL DISTRIBUTION")
print("="*80)
# Create figure with multiple subplots
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
fig.suptitle('Normal Distribution Analysis: X ∼ N(μ, σ²)', fontsize=16, fontweight='bold')
# 1. Histogram with normal curve
axes[0, 0].hist(yield_data, bins=8, density=True, alpha=0.7, color='skyblue', edgecolor='black', label='Actual Data')
# Generate normal curve
x = np.linspace(μ - 4*σ, μ + 4*σ, 1000)
pdf = (1/(σ * np.sqrt(2*np.pi))) * np.exp(-0.5 * ((x - μ)/σ)**2)
axes[0, 0].plot(x, pdf, 'r-', linewidth=2, label=f'N({μ:.2f}, {σ_squared:.2f})')
axes[0, 0].set_title('Histogram with Normal Curve')
axes[0, 0].set_xlabel('Yield (tons/hectare)')
axes[0, 0].set_ylabel('Density')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)
# 2. Empirical Rule Visualization
axes[0, 1].plot(x, pdf, 'k-', linewidth=2, label='Normal PDF')
# 1σ (68%)
x_1σ = np.linspace(μ-σ, μ+σ, 100)
pdf_1σ = (1/(σ * np.sqrt(2*np.pi))) * np.exp(-0.5 * ((x_1σ - μ)/σ)**2)
axes[0, 1].fill_between(x_1σ, pdf_1σ, color='green', alpha=0.3, label='68% within 1σ')
# 2σ (95%)
x_2σ = np.linspace(μ-2*σ, μ+2*σ, 100)
pdf_2σ = (1/(σ * np.sqrt(2*np.pi))) * np.exp(-0.5 * ((x_2σ - μ)/σ)**2)
axes[0, 1].fill_between(x_2σ, pdf_2σ, color='yellow', alpha=0.2, label='95% within 2σ')
# 3σ (99.7%)
x_3σ = np.linspace(μ-3*σ, μ+3*σ, 100)
pdf_3σ = (1/(σ * np.sqrt(2*np.pi))) * np.exp(-0.5 * ((x_3σ - μ)/σ)**2)
axes[0, 1].fill_between(x_3σ, pdf_3σ, color='red', alpha=0.1, label='99.7% within 3σ')
axes[0, 1].set_title('68-95-99.7 Empirical Rule')
axes[0, 1].set_xlabel('Yield (tons/hectare)')
axes[0, 1].set_ylabel('Density')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)
# 3. Z-scores calculation and plot
z_scores = (yield_data - μ) / σ
df['Z_Score'] = z_scores
axes[1, 0].scatter(range(len(yield_data)), z_scores, color='blue', s=100, alpha=0.7)
axes[1, 0].axhline(y=0, color='red', linestyle='--', label='Mean (z=0)')
axes[1, 0].axhline(y=1, color='green', linestyle=':', label='+1σ')
axes[1, 0].axhline(y=-1, color='green', linestyle=':', label='-1σ')
axes[1, 0].axhline(y=2, color='orange', linestyle=':', label='+2σ')
axes[1, 0].axhline(y=-2, color='orange', linestyle=':', label='-2σ')
for i, (yield_val, z) in enumerate(zip(yield_data, z_scores)):
    axes[1, 0].text(i, z + 0.1, f'{yield_val:.1f}', ha='center', fontsize=8)
axes[1, 0].set_title('Z-Scores for Each Yield Observation')
axes[1, 0].set_xlabel('Observation Number')
axes[1, 0].set_ylabel('Z-Score')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)
stats.probplot(yield_data, dist="norm", plot=axes[1, 1])
axes[1, 1].get_lines()[0].set_marker('o')
axes[1, 1].get_lines()[0].set_markersize(6)
axes[1, 1].get_lines()[0].set_markerfacecolor('blue')
axes[1, 1].get_lines()[0].set_markeredgecolor('blue')
axes[1, 1].get_lines()[1].set_color('red')
axes[1, 1].get_lines()[1].set_linewidth(2)
axes[1, 1].set_title('Q-Q Plot: Checking for Normality')
axes[1, 1].set_xlabel('Theoretical Quantiles')
axes[1, 1].set_ylabel('Sample Quantiles')
axes[1, 1].grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
print("PROBABILITY CALCULATIONS USING NORMAL DISTRIBUTION")
print("="*80)
print(f"\n**Probability that a random farm has:**")

# P(Yield < threshold)
thresholds = [3.0, 5.0, 7.0, μ]
for threshold in thresholds:
    prob = stats.norm.cdf(threshold, μ, σ)
    print(f"   Yield < {threshold}: {prob:.3f} or {prob*100:.1f}%")
print(f"\n**Probability that a random farm has:**")

# P(Yield > threshold)
for threshold in thresholds:
    prob = 1 - stats.norm.cdf(threshold, μ, σ)
    print(f"   Yield > {threshold}: {prob:.3f} or {prob*100:.1f}%")
# Calculate probability between two values
print(f"\n**Probability between ranges:**")
ranges = [(3, 5), (5, 7), (μ-σ, μ+σ)]
for low, high in ranges:
    prob = stats.norm.cdf(high, μ, σ) - stats.norm.cdf(low, μ, σ)
    print(f"   {low} < Yield < {high}: {prob:.3f} or {prob*100:.1f}%")
# Find percentiles
print(f"\n**Percentiles of Yield Distribution:**")
percentiles = [5, 25, 50, 75, 95]
for p in percentiles:
    value = stats.norm.ppf(p/100, μ, σ)
    print(f"   {p}th percentile: {value:.3f} tons/hectare")
# Check for outliers using 3σ rule
print("\n" + "="*80)
print("OUTLIER DETECTION USING 3σ RULE")
print("="*80)
lower_bound = μ - 3*σ
upper_bound = μ + 3*σ
outliers = yield_data[(yield_data < lower_bound) | (yield_data > upper_bound)]
print(f"\nOutlier bounds (3σ rule):")
print(f"   Lower bound (μ - 3σ): {lower_bound:.3f}")
print(f"   Upper bound (μ + 3σ): {upper_bound:.3f}")
if len(outliers) > 0:
    print(f"\n**Outliers detected:**")
    for outlier in outliers:
        z = (outlier - μ) / σ
        print(f"   Yield = {outlier:.3f} (Z-score = {z:.2f})")
else:
    print(f"\n**No outliers detected** using 3σ rule")

"""**C. Task 1: Define Events**"""

import pandas as pd
import numpy as np
data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646966373377603,False,True,Rainy,140,3.7072931271974823"""
df = pd.read_csv(pd.io.common.StringIO(data))
df['Rainfall_mm'] = pd.to_numeric(df['Rainfall_mm'])
df['Temperature_Celsius'] = pd.to_numeric(df['Temperature_Celsius'])
df['Days_to_Harvest'] = pd.to_numeric(df['Days_to_Harvest'])
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
print("\n**SELECTED VARIABLES:**")
print("1. Yield_tons_per_hectare (Continuous Variable)")
print("2. Region (Categorical Variable: North, South, West)")
print("DEFINING EVENTS")
print("="*80)
mean_yield = df['Yield_tons_per_hectare'].mean()
std_yield = df['Yield_tons_per_hectare'].std()
median_yield = df['Yield_tons_per_hectare'].median()
print(f"\n**Yield Statistics:**")
print(f"  Mean (μ): {mean_yield:.3f} tons/hectare")
print(f"  Standard Deviation (σ): {std_yield:.3f} tons/hectare")
print(f"  Median: {median_yield:.3f} tons/hectare")
print(f"  Minimum: {df['Yield_tons_per_hectare'].min():.3f} tons/hectare")
print(f"  Maximum: {df['Yield_tons_per_hectare'].max():.3f} tons/hectare")
# Event 1: High Yield
high_yield_threshold = mean_yield  # Above average yield
df['High_Yield'] = df['Yield_tons_per_hectare'] > high_yield_threshold
print("\n**EVENT 1: High Yield (A)**")
print("   Let A be the event that yield is above average")
print(f"   Mathematical Notation: A = {{Y > {high_yield_threshold:.3f}}}")
print(f"   Where Y = Yield_tons_per_hectare")
print(f"   Threshold: Above average yield = {high_yield_threshold:.3f} tons/hectare")
print(f"   Number of cases: {df['High_Yield'].sum()} out of {len(df)}")
# Event 2: Southern Region
print("\n**EVENT 2: Southern Region (B)**")
print("   Let B be the event that farm is in Southern region")
print("   Mathematical Notation: B = {Region = 'South'}")
print(f"   Number of cases: {(df['Region'] == 'South').sum()} out of {len(df)}")
# Event 3: Moderate Temperature Range
print("\n**EVENT 3: Moderate Temperature (C)**")
print("   Let C be the event that temperature is moderate (20-30°C)")
print("   Mathematical Notation: C = {20 ≤ Temperature ≤ 30}")
print(f"   Temperature range: 20°C to 30°C")
print(f"   Number of cases: {((df['Temperature_Celsius'] >= 20) & (df['Temperature_Celsius'] <= 30)).sum()} out of {len(df)}")
# Additional Event 4: Using Fertilizer
print("\n**EVENT 4: Fertilizer Used (D)**")
print("   Let D be the event that fertilizer is used")
print("   Mathematical Notation: D = {Fertilizer_Used = True}")
print(f"   Number of cases: {df['Fertilizer_Used'].sum()} out of {len(df)}")
print("MATHEMATICAL REPRESENTATION OF EVENTS")
print("="*80)
print("""
Let's define our sample space and events more formally:
SAMPLE SPACE (Ω):
  All possible outcomes of our agricultural observations
  Ω = {All 10 observations in our dataset}
RANDOM VARIABLES:
  1. Y = Yield_tons_per_hectare (continuous)
  2. R = Region (categorical: {North, South, West})
EVENTS:
  1. Event A: High Yield
     A = {ω ∈ Ω : Y(ω) > 5.08}
     Where Y(ω) is the yield for observation ω
     P(A) = Number of observations with Y > 5.08 / 10
  2. Event B: Southern Region
     B = {ω ∈ Ω : R(ω) = 'South'}
     Where R(ω) is the region for observation ω
     P(B) = Number of observations with Region = 'South' / 10
  3. Event C: Moderate Temperature
     C = {ω ∈ Ω : 20 ≤ T(ω) ≤ 30}
     Where T(ω) is the temperature for observation ω
     P(C) = Number of observations with 20 ≤ T ≤ 30 / 10
  4. Event D: Fertilizer Used
     D = {ω ∈ Ω : F(ω) = True}
     Where F(ω) is the fertilizer usage for observation ω
     P(D) = Number of observations with Fertilizer_Used = True / 10 """)
print("PROBABILITY CALCULATIONS")
print("="*80)
# Calculate individual probabilities
P_A = df['High_Yield'].mean()
P_B = (df['Region'] == 'South').mean()
P_C = ((df['Temperature_Celsius'] >= 20) & (df['Temperature_Celsius'] <= 30)).mean()
P_D = df['Fertilizer_Used'].mean()
print(f"\n**Individual Probabilities:**")
print(f"  P(A) = P(High Yield) = {P_A:.3f} ({P_A*100:.1f}%)")
print(f"  P(B) = P(Southern Region) = {P_B:.3f} ({P_B*100:.1f}%)")
print(f"  P(C) = P(Moderate Temperature) = {P_C:.3f} ({P_C*100:.1f}%)")
print(f"  P(D) = P(Fertilizer Used) = {P_D:.3f} ({P_D*100:.1f}%)")
print("\n**Compound Probabilities (Intersections):**")
# P(A ∩ B): High Yield AND Southern Region
A_and_B = ((df['High_Yield'] == True) & (df['Region'] == 'South')).sum()
P_A_and_B = A_and_B / len(df)
print(f"  P(A ∩ B) = P(High Yield AND Southern Region) = {A_and_B}/10 = {P_A_and_B:.3f}")
# P(A ∩ D): High Yield AND Fertilizer Used
A_and_D = ((df['High_Yield'] == True) & (df['Fertilizer_Used'] == True)).sum()
P_A_and_D = A_and_D / len(df)
print(f"  P(A ∩ D) = P(High Yield AND Fertilizer Used) = {A_and_D}/10 = {P_A_and_D:.3f}")
# P(B ∩ C): Southern Region AND Moderate Temperature
B_and_C = ((df['Region'] == 'South') & ((df['Temperature_Celsius'] >= 20) & (df['Temperature_Celsius'] <= 30))).sum()
P_B_and_C = B_and_C / len(df)
print(f"  P(B ∩ C) = P(Southern Region AND Moderate Temperature) = {B_and_C}/10 = {P_B_and_C:.3f}")
# Calculate conditional probabilities
print("\n**Conditional Probabilities:**")
# P(A|B): Probability of High Yield given Southern Region
P_A_given_B = A_and_B / (df['Region'] == 'South').sum() if (df['Region'] == 'South').sum() > 0 else 0
print(f"  P(A|B) = P(High Yield | Southern Region) = {A_and_B}/{(df['Region'] == 'South').sum()} = {P_A_given_B:.3f}")
# P(B|A): Probability of Southern Region given High Yield
P_B_given_A = A_and_B / df['High_Yield'].sum() if df['High_Yield'].sum() > 0 else 0
print(f"  P(B|A) = P(Southern Region | High Yield) = {A_and_B}/{df['High_Yield'].sum()} = {P_B_given_A:.3f}")
# P(A|D): Probability of High Yield given Fertilizer Used
P_A_given_D = A_and_D / df['Fertilizer_Used'].sum() if df['Fertilizer_Used'].sum() > 0 else 0
print(f"  P(A|D) = P(High Yield | Fertilizer Used) = {A_and_D}/{df['Fertilizer_Used'].sum()} = {P_A_given_D:.3f}")
print("\n**Union Probabilities:**")
# P(A ∪ B): High Yield OR Southern Region
A_or_B = ((df['High_Yield'] == True) | (df['Region'] == 'South')).sum()
P_A_or_B = A_or_B / len(df)
print(f"  P(A ∪ B) = P(High Yield OR Southern Region) = {A_or_B}/10 = {P_A_or_B:.3f}")
# Verify using addition rule: P(A ∪ B) = P(A) + P(B) - P(A ∩ B)
P_A_or_B_calculated = P_A + P_B - P_A_and_B
print(f"    Verification: P(A) + P(B) - P(A ∩ B) = {P_A:.3f} + {P_B:.3f} - {P_A_and_B:.3f} = {P_A_or_B_calculated:.3f}")
print("DATASET WITH EVENT INDICATORS")
print("="*80)
# Create a display DataFrame
display_df = df.copy()
display_df['Event A (High Yield)'] = display_df['High_Yield']
display_df['Event B (South Region)'] = display_df['Region'] == 'South'
display_df['Event C (Mod Temp 20-30°C)'] = (display_df['Temperature_Celsius'] >= 20) & (display_df['Temperature_Celsius'] <= 30)
display_df['Event D (Fertilizer)'] = display_df['Fertilizer_Used']
print(display_df[['Region', 'Yield_tons_per_hectare', 'Temperature_Celsius', 'Fertilizer_Used',
                  'Event A (High Yield)', 'Event B (South Region)',
                  'Event C (Mod Temp 20-30°C)', 'Event D (Fertilizer)']].to_string(index=False))
print("VENN DIAGRAM REPRESENTATION")
print("="*80)
count_A = df['High_Yield'].sum()
count_B = (df['Region'] == 'South').sum()
count_C = ((df['Temperature_Celsius'] >= 20) & (df['Temperature_Celsius'] <= 30)).sum()
count_D = df['Fertilizer_Used'].sum()
count_A_and_B = A_and_B
count_A_and_D = A_and_D
count_B_and_C = B_and_C
print(f"\n**Event Counts:**")
print(f"  Event A (High Yield): {count_A} observations")
print(f"  Event B (South Region): {count_B} observations")
print(f"  Event C (Mod Temp 20-30°C): {count_C} observations")
print(f"  Event D (Fertilizer Used): {count_D} observations")
print(f"\n**Intersection Counts:**")
print(f"  A ∩ B: {count_A_and_B} observations")
print(f"  A ∩ D: {count_A_and_D} observations")
print(f"  B ∩ C: {count_B_and_C} observations")
print("INDEPENDENCE CHECKING")
print("="*80)
print("""Two events X and Y are independent if: P(X ∩ Y) = P(X) × P(Y)""")
expected_A_and_B = P_A * P_B
print(f"\nChecking independence of A and B:")
print(f"  P(A ∩ B) = {P_A_and_B:.3f}")
print(f"  P(A) × P(B) = {P_A:.3f} × {P_B:.3f} = {expected_A_and_B:.3f}")
if abs(P_A_and_B - expected_A_and_B) < 0.01:
    print(f"  Conclusion: A and B are approximately independent")
else:
    print(f"  Conclusion: A and B are DEPENDENT (difference = {abs(P_A_and_B - expected_A_and_B):.3f})")
expected_A_and_D = P_A * P_D
print(f"\nChecking independence of A and D:")
print(f"  P(A ∩ D) = {P_A_and_D:.3f}")
print(f"  P(A) × P(D) = {P_A:.3f} × {P_D:.3f} = {expected_A_and_D:.3f}")
if abs(P_A_and_D - expected_A_and_D) < 0.01:
    print(f"  Conclusion: A and D are approximately independent")
else:
    print(f"  Conclusion: A and D are DEPENDENT (difference = {abs(P_A_and_D - expected_A_and_D):.3f})")
print("REAL-WORLD INTERPRETATION")
print("="*80)
print(f"""
Based on our analysis:
1. **High Yield Farms (Event A):**
   - {count_A} out of 10 farms have above-average yield
   - Probability: {P_A:.2f} or {P_A*100:.0f}%
   - This is our target outcome for farmers
2. **Southern Region Farms (Event B):**
   - {count_B} out of 10 farms are in Southern region
   - Probability: {P_B:.2f} or {P_B*100:.0f}%
   - Regional factors might affect yield
3. **Relationship between Region and Yield:**
   - P(High Yield | Southern Region) = {P_A_given_B:.2f}
   - P(Southern Region | High Yield) = {P_B_given_A:.2f}
   - This suggests that Southern farms are {P_A_given_B/P_A:.1f}x more likely to have high yield than average
4. **Fertilizer Impact:**
   - P(High Yield | Fertilizer Used) = {P_A_given_D:.2f}
   - Farms using fertilizer are {P_A_given_D/P_A:.1f}x more likely to have high yield
PRACTICAL IMPLICATIONS:
• Southern regions show promise for high yield
• Fertilizer use is associated with better yields
• Temperature range 20-30°C occurs in {count_C} farms """)
# Summary table of all probabilities
print("\n" + "="*80)
print("SUMMARY OF ALL PROBABILITIES")
print("="*80)

summary_data = {
    'Event': ['A: High Yield', 'B: South Region', 'C: Mod Temp (20-30°C)', 'D: Fertilizer Used',
              'A ∩ B', 'A ∩ D', 'B ∩ C', 'A ∪ B'],
    'Description': ['Yield > average', 'Region = South', '20 ≤ Temp ≤ 30', 'Fertilizer = True',
                   'High Yield AND South', 'High Yield AND Fertilizer', 'South AND Mod Temp', 'High Yield OR South'],
    'Count': [count_A, count_B, count_C, count_D, count_A_and_B, count_A_and_D, count_B_and_C, A_or_B],
    'Probability': [P_A, P_B, P_C, P_D, P_A_and_B, P_A_and_D, P_B_and_C, P_A_or_B],
    'Percentage': [f"{P_A*100:.1f}%", f"{P_B*100:.1f}%", f"{P_C*100:.1f}%", f"{P_D*100:.1f}%",
                   f"{P_A_and_B*100:.1f}%", f"{P_A_and_D*100:.1f}%", f"{P_B_and_C*100:.1f}%", f"{P_A_or_B*100:.1f}%"]
}

summary_df = pd.DataFrame(summary_data)
print(summary_df.to_string(index=False))
print("MATHEMATICAL NOTATION SUMMARY")
print("="*80)

print("""
FINAL MATHEMATICAL NOTATION:

Sample Space: Ω = {ω₁, ω₂, ..., ω₁₀} where each ωᵢ is an observation

Random Variables:
  Y: Ω → ℝ, Y(ω) = Yield_tons_per_hectare of observation ω
  R: Ω → {North, South, West}, R(ω) = Region of observation ω
  T: Ω → ℝ, T(ω) = Temperature_Celsius of observation ω
  F: Ω → {True, False}, F(ω) = Fertilizer_Used status of observation ω

Events:
  A = {ω ∈ Ω : Y(ω) > 5.08}
  B = {ω ∈ Ω : R(ω) = 'South'}
  C = {ω ∈ Ω : 20 ≤ T(ω) ≤ 30}
  D = {ω ∈ Ω : F(ω) = True}

Probabilities (Empirical):
  P(A) = 6/10 = 0.6
  P(B) = 4/10 = 0.4
  P(C) = 5/10 = 0.5
  P(D) = 5/10 = 0.5

Conditional Probabilities:
  P(A|B) = P(A ∩ B)/P(B) = 4/4 = 1.0
  P(B|A) = P(A ∩ B)/P(A) = 4/6 = 0.667
  P(A|D) = P(A ∩ D)/P(D) = 3/5 = 0.6
""")

"""**D. Task 2: Conditional Probability**"""

import pandas as pd
import numpy as np
import itertools
data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646966373377603,False,True,Rainy,140,3.7072931271974823"""
df = pd.read_csv(pd.io.common.StringIO(data))
df['Rainfall_mm'] = pd.to_numeric(df['Rainfall_mm'])
df['Temperature_Celsius'] = pd.to_numeric(df['Temperature_Celsius'])
df['Days_to_Harvest'] = pd.to_numeric(df['Days_to_Harvest'])
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
average_yield = df['Yield_tons_per_hectare'].mean()
df['Event_A'] = df['Yield_tons_per_hectare'] > average_yield  # A = High Yield
df['Event_B'] = df['Region'] == 'South'  # B = South Region
df['Event_C'] = (df['Temperature_Celsius'] >= 20) & (df['Temperature_Celsius'] <= 30)
df['Event_D'] = df['Fertilizer_Used']  # D = Fertilizer Used
P_A = df['Event_A'].mean()
P_B = df['Event_B'].mean()
P_C = df['Event_C'].mean()
P_D = df['Event_D'].mean()
print(f"\n**INDIVIDUAL EVENT PROBABILITIES:**")
print(f"  P(A) = P(High Yield) = {P_A:.3f} ({P_A*100:.1f}%)")
print(f"  P(B) = P(South Region) = {P_B:.3f} ({P_B*100:.1f}%)")
print(f"  P(C) = P(Moderate Temperature) = {P_C:.3f} ({P_C*100:.1f}%)")
print(f"  P(D) = P(Fertilizer Used) = {P_D:.3f} ({P_D*100:.1f}%)")
def conditional_probability(event_A_col, event_B_col):
    both_cases = ((df[event_A_col] == True) & (df[event_B_col] == True)).sum()
    b_cases = (df[event_B_col] == True).sum()
    if b_cases == 0:
        return 0
    return both_cases / b_cases
event_pairs = [
    ('A', 'B', 'Event_A', 'Event_B', 'High Yield', 'South Region'),
    ('A', 'C', 'Event_A', 'Event_C', 'High Yield', 'Moderate Temperature'),
    ('A', 'D', 'Event_A', 'Event_D', 'High Yield', 'Fertilizer Used'),
    ('B', 'A', 'Event_B', 'Event_A', 'South Region', 'High Yield'),
    ('B', 'C', 'Event_B', 'Event_C', 'South Region', 'Moderate Temperature'),
    ('B', 'D', 'Event_B', 'Event_D', 'South Region', 'Fertilizer Used'),
    ('C', 'A', 'Event_C', 'Event_A', 'Moderate Temperature', 'High Yield'),
    ('C', 'B', 'Event_C', 'Event_B', 'Moderate Temperature', 'South Region'),
    ('C', 'D', 'Event_C', 'Event_D', 'Moderate Temperature', 'Fertilizer Used'),
    ('D', 'A', 'Event_D', 'Event_A', 'Fertilizer Used', 'High Yield'),
    ('D', 'B', 'Event_D', 'Event_B', 'Fertilizer Used', 'South Region'),
    ('D', 'C', 'Event_D', 'Event_C', 'Fertilizer Used', 'Moderate Temperature'),
]
print("ANALYSIS OF ALL PAIRS: P(A|B) = P(A ∩ B) / P(B)")
print("="*80)
summary_data = []
for event1, event2, col1, col2, desc1, desc2 in event_pairs:
    # Get probabilities
    if event1 == 'A':
        P_event1 = P_A
    elif event1 == 'B':
        P_event1 = P_B
    elif event1 == 'C':
        P_event1 = P_C
    else:  # D
        P_event1 = P_D
    if event2 == 'A':
        P_event2 = P_A
    elif event2 == 'B':
        P_event2 = P_B
    elif event2 == 'C':
        P_event2 = P_C
    else:  # D
        P_event2 = P_D
    # Calculate conditional probability
    P_A_given_B = conditional_probability(col1, col2)
    # Count cases for the formula display
    both_count = ((df[col1] == True) & (df[col2] == True)).sum()
    b_count = (df[col2] == True).sum()
    total = len(df)
    print(f"\n**Pair: P({event1} | {event2}) = P({desc1} | {desc2})**")
    print("-" * 60)
    # Mathematical formula display
    print(f"  Formula: P({event1}|{event2}) = P({event1} ∩ {event2}) / P({event2})")
    print(f"  Calculation: {both_count}/{b_count} = {P_A_given_B:.3f}")
    print(f"  P({event1}) = {P_event1:.3f}, P({event2}) = {P_event2:.3f}")
    print(f"  P({event1} ∩ {event2}) = {both_count}/{total} = {both_count/total:.3f}")
    # Interpretation
    print(f"\n  *Interpretation:**")
    if event1 == 'A' and event2 == 'B':
        print("     Given that a farm is in the SOUTH REGION,")
        print(f"     the probability of having HIGH YIELD is {P_A_given_B:.1%}")
        if P_A_given_B > P_event1:
            print("     → South region farms are MORE LIKELY to have high yield")
        elif P_A_given_B < P_event1:
            print("     → South region farms are LESS LIKELY to have high yield")
        else:
            print("     → South region doesn't affect high yield probability")
    elif event1 == 'A' and event2 == 'C':
        print("     Given MODERATE TEMPERATURE (20-30°C),")
        print(f"     the probability of HIGH YIELD is {P_A_given_B:.1%}")
    elif event1 == 'A' and event2 == 'D':
        print("     Given that FERTILIZER IS USED,")
        print(f"     the probability of HIGH YIELD is {P_A_given_B:.1%}")
    elif event1 == 'B' and event2 == 'A':
        print("     Given that a farm has HIGH YIELD,")
        print(f"     the probability it's in SOUTH REGION is {P_A_given_B:.1%}")
    elif event1 == 'B' and event2 == 'C':
        print("     Given MODERATE TEMPERATURE,")
        print(f"     the probability of SOUTH REGION is {P_A_given_B:.1%}")
    elif event1 == 'B' and event2 == 'D':
        print("     Given that FERTILIZER IS USED,")
        print(f"     the probability of SOUTH REGION is {P_A_given_B:.1%}")
    elif event1 == 'C' and event2 == 'A':
        print("     Given HIGH YIELD,")
        print(f"     the probability of MODERATE TEMPERATURE is {P_A_given_B:.1%}")
    elif event1 == 'C' and event2 == 'B':
        print("     Given SOUTH REGION,")
        print(f"     the probability of MODERATE TEMPERATURE is {P_A_given_B:.1%}")
    elif event1 == 'C' and event2 == 'D':
        print("     Given that FERTILIZER IS USED,")
        print(f"     the probability of MODERATE TEMPERATURE is {P_A_given_B:.1%}")
    elif event1 == 'D' and event2 == 'A':
        print("     Given HIGH YIELD,")
        print(f"     the probability that FERTILIZER WAS USED is {P_A_given_B:.1%}")
    elif event1 == 'D' and event2 == 'B':
        print("     Given SOUTH REGION,")
        print(f"     the probability that FERTILIZER WAS USED is {P_A_given_B:.1%}")
    elif event1 == 'D' and event2 == 'C':
        print("     Given MODERATE TEMPERATURE,")
        print(f"     the probability that FERTILIZER WAS USED is {P_A_given_B:.1%}")
    summary_data.append({
        'P(A|B)': f'P({event1}|{event2})',
        'Description': f'P({desc1} | {desc2})',
        'Value': P_A_given_B,
        'P(A)': P_event1,
        'P(B)': P_event2,
        'Comparison': 'Higher' if P_A_given_B > P_event1 else 'Lower' if P_A_given_B < P_event1 else 'Same'
    })
print("COMPARISON MATRIX: P(A|B) vs P(A)")
print("="*80)
print("""
This shows how knowing B changes the probability of A:
                  P(A|B)   vs   P(A)     →   Effect of knowing B
----------------------------------------------------------------
P(High Yield | South)     = 1.000  vs  0.600  →  INCREASES probability
P(High Yield | Mod Temp)  = 0.800  vs  0.600  →  INCREASES probability
P(High Yield | Fertilizer)= 0.600  vs  0.600  →  NO CHANGE
P(South | High Yield)     = 0.667  vs  0.400  →  INCREASES probability
P(Mod Temp | High Yield)  = 0.667  vs  0.500  →  INCREASES probability
P(Fertilizer | High Yield)= 0.500  vs  0.500  →  NO CHANGE""")
print("BAYES' THEOREM APPLICATION")
print("="*80)
print("""
Let's verify P(B|A) using Bayes' Theorem:
P(B|A) = [P(A|B) × P(B)] / P(A)""")
# For P(South | High Yield) = P(B|A)
P_A_given_B = conditional_probability('Event_A', 'Event_B')  # P(A|B) = 1.000
P_B = P_B  # 0.400
P_A = P_A  # 0.600
P_B_given_A_calculated = (P_A_given_B * P_B) / P_A
print(f"\nP(B|A) = P(South | High Yield) = [P(A|B) × P(B)] / P(A)")
print(f"       = [{P_A_given_B:.3f} × {P_B:.3f}] / {P_A:.3f}")
print(f"       = {P_B_given_A_calculated:.3f}")
# Compare with direct calculation
P_B_given_A_direct = conditional_probability('Event_B', 'Event_A')
print(f"\nDirect calculation: P(B|A) = {P_B_given_A_direct:.3f}")
print(f"Bayes' Theorem gives the same result: {P_B_given_A_calculated:.3f}")

"""**E. Task 3: Independence Check**"""

import pandas as pd
import numpy as np
print("\n**Events Definition:**")
print("Event A: High Yield (Yield > Average)")
print("Event B: South Region (Region = 'South')")
data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646198956255336,False,True,Rainy,140,3.7072931271974823"""
df = pd.read_csv(pd.io.common.StringIO(data))
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
# Event A: High Yield
average_yield = df['Yield_tons_per_hectare'].mean()
df['Event_A'] = df['Yield_tons_per_hectare'] > average_yield
# Event B: South Region
df['Event_B'] = df['Region'] == 'South'
print(f"\n**Data Summary (10 records):**")
print(f"Average Yield: {average_yield:.3f} tons/hectare")
print(df[['Region', 'Yield_tons_per_hectare', 'Event_A', 'Event_B']].to_string(index=False))
print("1. COMPUTING PROBABILITIES")
print("="*80)
# P(A) - Probability of High Yield
P_A = df['Event_A'].mean()
print(f"\n**P(A) = P(High Yield):**")
print(f"   Number of High Yield cases: {df['Event_A'].sum()}")
print(f"   Total cases: {len(df)}")
print(f"   P(A) = {df['Event_A'].sum()}/{len(df)} = {P_A:.3f}")
# P(B) - Probability of South Region
P_B = df['Event_B'].mean()
print(f"\n**P(B) = P(South Region):**")
print(f"   Number of South Region cases: {df['Event_B'].sum()}")
print(f"   Total cases: {len(df)}")
print(f"   P(B) = {df['Event_B'].sum()}/{len(df)} = {P_B:.3f}")
# P(A ∩ B) - Probability of both A and B
A_and_B = ((df['Event_A'] == True) & (df['Event_B'] == True)).sum()
P_A_and_B = A_and_B / len(df)
print(f"\n**P(A ∩ B) = P(High Yield AND South Region):**")
print(f"   Number of cases with both High Yield AND South Region: {A_and_B}")
print(f"   Total cases: {len(df)}")
print(f"   P(A ∩ B) = {A_and_B}/{len(df)} = {P_A_and_B:.3f}")
print("2. COMPARING P(A ∩ B) WITH P(A)P(B)")
print("="*80)
# Calculate P(A)P(B)
P_A_times_P_B = P_A * P_B
print(f"\n**Calculation:**")
print(f"   P(A) × P(B) = {P_A:.3f} × {P_B:.3f}")
print(f"               = {P_A_times_P_B:.3f}")
print(f"\n**Comparison:**")
print(f"   P(A ∩ B) = {P_A_and_B:.3f}")
print(f"   P(A) × P(B) = {P_A_times_P_B:.3f}")
print(f"   Difference = {abs(P_A_and_B - P_A_times_P_B):.3f}")
print("3. CHECKING INDEPENDENCE")
print("="*80)
print("\n**Independence Rule:**")
print("Two events A and B are independent if: P(A ∩ B) = P(A) × P(B)")
if abs(P_A_and_B - P_A_times_P_B) < 0.01:
    print(f"\n**RESULT: INDEPENDENT**")
    print(f"   Because P(A ∩ B) ≈ P(A) × P(B)")
    print(f"   {P_A_and_B:.3f} ≈ {P_A_times_P_B:.3f}")
else:
    print(f"\n**RESULT: DEPENDENT (NOT INDEPENDENT)**")
    print(f"   Because P(A ∩ B) ≠ P(A) × P(B)")
    print(f"   {P_A_and_B:.3f} ≠ {P_A_times_P_B:.3f}")
print("MATHEMATICAL VERIFICATION")
print("="*80)
print(f"\n**Step-by-step calculation:**")
print(f"1. P(A) = {df['Event_A'].sum()}/{len(df)} = {P_A:.3f}")
print(f"2. P(B) = {df['Event_B'].sum()}/{len(df)} = {P_B:.3f}")
print(f"3. P(A) × P(B) = {P_A:.3f} × {P_B:.3f} = {P_A_times_P_B:.3f}")
print(f"4. P(A ∩ B) = {A_and_B}/{len(df)} = {P_A_and_B:.3f}")
print(f"\n**Conclusion: {'INDEPENDENT' if abs(P_A_and_B - P_A_times_P_B) < 0.01 else 'DEPENDENT'}**")
print(f"   Because {P_A_and_B:.3f} {'=' if abs(P_A_and_B - P_A_times_P_B) < 0.01 else '≠'} {P_A_times_P_B:.3f}")
print("ADDITIONAL ANALYSIS WITH EXACT VALUES")
print("="*80)
print(f"\n**Exact values from data:**")
print(f"Total farms: {len(df)}")
print(f"High Yield farms: {df['Event_A'].sum()}")
print(f"South Region farms: {df['Event_B'].sum()}")
print(f"Both High Yield AND South Region: {A_and_B}")
print(f"\n**Exact probabilities:**")
print(f"P(A) = {df['Event_A'].sum()}/{len(df)} = {df['Event_A'].sum()/len(df):.4f}")
print(f"P(B) = {df['Event_B'].sum()}/{len(df)} = {df['Event_B'].sum()/len(df):.4f}")
print(f"P(A) × P(B) = {P_A_times_P_B:.4f}")
print(f"P(A ∩ B) = {A_and_B}/{len(df)} = {P_A_and_B:.4f}")

"""**F. Task 4: Bayes’ Rule**"""

import pandas as pd
import numpy as np
print("BAYES' THEOREM ANALYSIS: P(B|A) = [P(A|B) × P(B)] / P(A)")
data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646198956255336,False,True,Rainy,140,3.7072931271974823"""
df = pd.read_csv(pd.io.common.StringIO(data))
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
print("\n**Dataset (10 records):**")
print(df[['Region', 'Yield_tons_per_hectare']].to_string(index=False))
print("1. DEFINE EVENTS")
print("="*80)
print("""
Event A: High Yield (Yield > Average)
Event B: South Region (Region = 'South')""")
# Calculate average yield
average_yield = df['Yield_tons_per_hectare'].mean()
# Create event columns
df['Event_A'] = df['Yield_tons_per_hectare'] > average_yield  # High Yield
df['Event_B'] = df['Region'] == 'South'  # South Region
print(f"Average Yield: {average_yield:.3f} tons/hectare")
print("\n**Dataset with Events:**")
print(df[['Region', 'Yield_tons_per_hectare', 'Event_A', 'Event_B']].to_string(index=False))
print("2. COMPUTE P(B) - PROBABILITY OF SOUTH REGION")
print("="*80)
# P(B) = Probability of South Region
south_count = df['Event_B'].sum()
total_count = len(df)
P_B = south_count / total_count
print(f"Number of South Region farms: {south_count}")
print(f"Total farms: {total_count}")
print(f"\nP(B) = P(South Region) = {south_count}/{total_count}")
print(f"P(B) = {P_B:.3f} ({P_B*100:.1f}%)")
print("3. COMPUTE P(A|B) - CONDITIONAL PROBABILITY")
print("="*80)
print("""
P(A|B) = Probability of High Yield GIVEN South Region
       = P(High Yield AND South Region) / P(South Region)
       = Count(High Yield AND South) / Count(South)""")
# Count of South Region farms
south_farms = df[df['Event_B'] == True]
south_count = len(south_farms)
# Count of High Yield AND South Region
high_yield_south = south_farms[south_farms['Event_A'] == True]
high_yield_south_count = len(high_yield_south)
# Calculate P(A|B)
P_A_given_B = high_yield_south_count / south_count if south_count > 0 else 0
print(f"South Region farms: {south_count}")
print(f"High Yield AND South Region farms: {high_yield_south_count}")
print(f"\nP(A|B) = {high_yield_south_count}/{south_count}")
print(f"P(A|B) = {P_A_given_B:.3f} ({P_A_given_B*100:.1f}%)")
print("\n**Interpretation:**")
print(f"Given that a farm is in South Region, the probability of High Yield is {P_A_given_B:.1%}")
print("4. BAYES' THEOREM: COMPUTE P(B|A)")
print("="*80)
print("""
Bayes' Theorem Formula:
P(B|A) = [P(A|B) × P(B)] / P(A)
Where:
• P(B|A) = Probability of South Region GIVEN High Yield
• P(A|B) = Probability of High Yield GIVEN South Region (calculated above)
• P(B) = Probability of South Region (calculated above)
• P(A) = Probability of High Yield (need to calculate)""")
# Calculate P(A) - Probability of High Yield
high_yield_count = df['Event_A'].sum()
P_A = high_yield_count / total_count
print(f"\n**Calculate P(A):**")
print(f"High Yield farms: {high_yield_count}")
print(f"Total farms: {total_count}")
print(f"P(A) = P(High Yield) = {high_yield_count}/{total_count} = {P_A:.3f}")
print(f"\n**Apply Bayes' Theorem:**")
print(f"P(A|B) = {P_A_given_B:.3f}")
print(f"P(B) = {P_B:.3f}")
print(f"P(A) = {P_A:.3f}")
P_B_given_A_calculated = (P_A_given_B * P_B) / P_A
print(f"\n**Calculation:**")
print(f"P(B|A) = [{P_A_given_B:.3f} * {P_B:.3f}] / {P_A:.3f}")
print(f"       = {P_A_given_B * P_B:.3f} / {P_A:.3f}")
print(f"       = {P_B_given_A_calculated:.3f} ({P_B_given_A_calculated*100:.1f}%)")
print("5. EMPIRICAL VALUE: DIRECT CALCULATION OF P(B|A)")
print("="*80)
print("""
Direct calculation from data:
P(B|A) = P(South Region | High Yield)
       = Count(South Region AND High Yield) / Count(High Yield)""")
high_yield_farms = df[df['Event_A'] == True]
high_yield_count = len(high_yield_farms)
south_and_high_yield = high_yield_farms[high_yield_farms['Event_B'] == True]
south_and_high_yield_count = len(south_and_high_yield)
P_B_given_A_empirical = south_and_high_yield_count / high_yield_count if high_yield_count > 0 else 0
print(f"High Yield farms: {high_yield_count}")
print(f"South Region AND High Yield farms: {south_and_high_yield_count}")
print(f"\nP(B|A) (Empirical) = {south_and_high_yield_count}/{high_yield_count}")
print(f"                    = {P_B_given_A_empirical:.3f} ({P_B_given_A_empirical*100:.1f}%)")
print("6. COMPARISON: BAYES' THEOREM VS EMPIRICAL VALUE")
print("="*80)
print("\n**Results Comparison:**")
print(f"Bayes' Theorem calculation: P(B|A) = {P_B_given_A_calculated:.3f}")
print(f"Empirical (direct) value:   P(B|A) = {P_B_given_A_empirical:.3f}")
difference = abs(P_B_given_A_calculated - P_B_given_A_empirical)
if difference < 0.001:
    print(f"\n**PERFECT MATCH!**")
    print(f"   Bayes' Theorem gives the exact same result as empirical calculation.")
    print(f"   Difference: {difference:.6f}")
else:
    print(f"\n**Difference found:** {difference:.6f}")
    print(f"   This could be due to rounding or calculation precision.")
print("7. VERIFICATION WITH ACTUAL DATA")
print("="*80)
print("\n**Verification Table:**")
verification_df = df[['Region', 'Yield_tons_per_hectare', 'Event_A', 'Event_B']].copy()
verification_df['High Yield?'] = verification_df['Event_A'].apply(lambda x: 'Yes' if x else 'No')
verification_df['South Region?'] = verification_df['Event_B'].apply(lambda x: 'Yes' if x else 'No')
print(verification_df.to_string(index=False))
print(f"\n**Count Summary:**")
print(f"Total farms: {total_count}")
print(f"High Yield farms (A): {high_yield_count}")
print(f"South Region farms (B): {south_count}")
print(f"Both A and B: {south_and_high_yield_count}")
print("\n**Probability Summary:**")
print(f"P(A) = P(High Yield) = {high_yield_count}/{total_count} = {P_A:.3f}")
print(f"P(B) = P(South Region) = {south_count}/{total_count} = {P_B:.3f}")
print(f"P(A ∩ B) = {south_and_high_yield_count}/{total_count} = {south_and_high_yield_count/total_count:.3f}")
print(f"P(A|B) = {high_yield_south_count}/{south_count} = {P_A_given_B:.3f}")
print(f"P(B|A) = {south_and_high_yield_count}/{high_yield_count} = {P_B_given_A_empirical:.3f}")
print("8. STEP-BY-STEP BAYES' THEOREM VERIFICATION")
print("="*80)
print("""
Let's verify each step of Bayes' Theorem:
Step 1: Calculate all components""")
print(f"1. P(A) = P(High Yield) = {high_yield_count}/{total_count} = {P_A:.4f}")
print(f"2. P(B) = P(South Region) = {south_count}/{total_count} = {P_B:.4f}")
print(f"3. P(A|B) = P(High Yield | South) = {high_yield_south_count}/{south_count} = {P_A_given_B:.4f}")
print(f"\nStep 2: Apply Bayes' Theorem")
print(f"P(B|A) = [P(A|B) * P(B)] / P(A)")
print(f"       = [{P_A_given_B:.4f} * {P_B:.4f}] / {P_A:.4f}")
print(f"       = {P_A_given_B * P_B:.4f} / {P_A:.4f}")
print(f"       = {P_B_given_A_calculated:.4f}")
print(f"\nStep 3: Compare with empirical value")
print(f"Empirical P(B|A) = {south_and_high_yield_count}/{high_yield_count} = {P_B_given_A_empirical:.4f}")
print(f"\n**Conclusion: Bayes' Theorem is verified!**")
print(f"   Calculated: {P_B_given_A_calculated:.4f}")
print(f"   Empirical:  {P_B_given_A_empirical:.4f}")

"""**G. Task 5: Probability Distribution (Normal Only)**

**G1. Explore a Numerical Variable**
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
labels = ["100.00 - 118.00", "118.00 - 136.00", "136.00 - 154.00", "154.00 - 172.00",
          "172.00 - 190.00", "190.00 - 208.00", "208.00 - 226.00", "226.00 - 244.00",
          "244.00 - 262.00", "262.00 - 280.00", "280.00 - 298.00", "298.00 - 316.00",
          "316.00 - 334.00", "334.00 - 352.00", "352.00 - 370.00", "370.00 - 388.00",
          "388.00 - 406.00", "406.00 - 424.00", "424.00 - 442.00", "442.00 - 460.00",
          "460.00 - 478.00", "478.00 - 496.00", "496.00 - 514.00", "514.00 - 532.00",
          "532.00 - 550.00", "550.00 - 568.00", "568.00 - 586.00", "586.00 - 604.00",
          "604.00 - 622.00", "622.00 - 640.00", "640.00 - 658.00", "658.00 - 676.00",
          "676.00 - 694.00", "694.00 - 712.00", "712.00 - 730.00", "730.00 - 748.00",
          "748.00 - 766.00", "766.00 - 784.00", "784.00 - 802.00", "802.00 - 820.00",
          "820.00 - 838.00", "838.00 - 856.00", "856.00 - 874.00", "874.00 - 892.00",
          "892.00 - 910.00", "910.00 - 928.00", "928.00 - 946.00", "946.00 - 964.00",
          "964.00 - 982.00", "982.00 - 1000.00"]
counts = [20220, 20109, 20008, 19900, 19867, 19830, 20237, 20063, 20050, 20174,
          19953, 19667, 20148, 19693, 19959, 19954, 20004, 20195, 19920, 19987,
          19947, 20006, 19927, 19917, 20115, 19844, 19971, 20117, 20051, 19889,
          20087, 19788, 20177, 20139, 20270, 20124, 20009, 19972, 19860, 19833,
          20035, 19917, 20226, 20085, 19894, 19784, 19891, 20012, 20329, 19846]
midpoints = []
for label in labels:
    start, end = label.split(" - ")
    start = float(start)
    end = float(end.replace("00", "")) if "00" in end else float(end)
    midpoints.append((start + end) / 2)
data = []
for midpoint, count in zip(midpoints, counts):
    data.extend([midpoint] * count)
data = np.array(data)
μ = np.mean(data)
σ = np.std(data)
n = len(data)
print(f"Statistics for Numerical Variable (100-1000 range):")
print(f"Mean (μ): {μ:.2f}")
print(f"Standard Deviation (σ): {σ:.2f}")
print(f"Total Data Points (n): {n:,}")
print(f"Minimum Value: {np.min(data):.2f}")
print(f"Maximum Value: {np.max(data):.2f}")
plt.figure(figsize=(12, 8))
plt.hist(data, bins=50, density=True, alpha=0.6, color='blue', edgecolor='black',
         label=f'Actual Data (n={n:,})')
x = np.linspace(np.min(data) - 3*σ, np.max(data) + 3*σ, 1000)
y = norm.pdf(x, μ, σ)
plt.plot(x, y, 'r-', linewidth=2, label=f'Normal Distribution\nμ={μ:.2f}, σ={σ:.2f}')
plt.axvline(μ, color='red', linestyle='--', alpha=0.5, label=f'Mean = {μ:.2f}')
plt.axvspan(μ - σ, μ + σ, alpha=0.2, color='gray', label=f'±1σ ({μ-σ:.1f} to {μ+σ:.1f})')
plt.xlabel('Value', fontsize=12)
plt.ylabel('Density', fontsize=12)
plt.title(f'Histogram with Normal Distribution Overlay\nμ = {μ:.2f}, σ = {σ:.2f}, n = {n:,}', fontsize=14)
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)
textstr = '\n'.join((
    f'μ = {μ:.2f}',
    f'σ = {σ:.2f}',
    f'n = {n:,}',
    f'Range: {np.min(data):.1f} to {np.max(data):.1f}',
    f'68% within: {μ-σ:.1f} to {μ+σ:.1f}',
    f'95% within: {μ-2*σ:.1f} to {μ+2*σ:.1f}'))
props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)
plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes, fontsize=10,
         verticalalignment='top', bbox=props)
plt.tight_layout()
plt.show()
print("ADDITIONAL STATISTICS:")
print("="*50)
print(f"Variance (σ²): {σ**2:.2f}")
print(f"Coefficient of Variation: {(σ/μ)*100:.2f}%")
print(f"Skewness: {np.mean((data - μ)**3) / (σ**3):.4f}")
print(f"Kurtosis: {np.mean((data - μ)**4) / (σ**4) - 3:.4f}")
print(f"25th Percentile (Q1): {np.percentile(data, 25):.2f}")
print(f"50th Percentile (Median): {np.median(data):.2f}")
print(f"75th Percentile (Q3): {np.percentile(data, 75):.2f}")
print(f"IQR: {np.percentile(data, 75) - np.percentile(data, 25):.2f}")
expected_uniform_mean = (100 + 1000) / 2
expected_uniform_std = (1000 - 100) / np.sqrt(12)
print(f"\nExpected for Uniform Distribution [100, 1000]:")
print(f"  Expected Mean: {expected_uniform_mean:.2f}")
print(f"  Expected Std Dev: {expected_uniform_std:.2f}")
print(f"\nDifference from Uniform:")
print(f"  Mean difference: {μ - expected_uniform_mean:.2f}")
print(f"  Std Dev difference: {σ - expected_uniform_std:.2f}")

"""**G2. Normal Probability Questions**"""

import numpy as np
from scipy.stats import norm
labels = ["100.00 - 118.00", "118.00 - 136.00", "136.00 - 154.00", "154.00 - 172.00",
          "172.00 - 190.00", "190.00 - 208.00", "208.00 - 226.00", "226.00 - 244.00",
          "244.00 - 262.00", "262.00 - 280.00", "280.00 - 298.00", "298.00 - 316.00",
          "316.00 - 334.00", "334.00 - 352.00", "352.00 - 370.00", "370.00 - 388.00",
          "388.00 - 406.00", "406.00 - 424.00", "424.00 - 442.00", "442.00 - 460.00",
          "460.00 - 478.00", "478.00 - 496.00", "496.00 - 514.00", "514.00 - 532.00",
          "532.00 - 550.00", "550.00 - 568.00", "568.00 - 586.00", "586.00 - 604.00",
          "604.00 - 622.00", "622.00 - 640.00", "640.00 - 658.00", "658.00 - 676.00",
          "676.00 - 694.00", "694.00 - 712.00", "712.00 - 730.00", "730.00 - 748.00",
          "748.00 - 766.00", "766.00 - 784.00", "784.00 - 802.00", "802.00 - 820.00",
          "820.00 - 838.00", "838.00 - 856.00", "856.00 - 874.00", "874.00 - 892.00",
          "892.00 - 910.00", "910.00 - 928.00", "928.00 - 946.00", "946.00 - 964.00",
          "964.00 - 982.00", "982.00 - 1000.00"]
counts = [20220, 20109, 20008, 19900, 19867, 19830, 20237, 20063, 20050, 20174,
          19953, 19667, 20148, 19693, 19959, 19954, 20004, 20195, 19920, 19987,
          19947, 20006, 19927, 19917, 20115, 19844, 19971, 20117, 20051, 19889,
          20087, 19788, 20177, 20139, 20270, 20124, 20009, 19972, 19860, 19833,
          20035, 19917, 20226, 20085, 19894, 19784, 19891, 20012, 20329, 19846]
# Calculate midpoints
midpoints = []
for label in labels:
    start, end = label.split(" - ")
    start = float(start)
    end = float(end)
    midpoints.append((start + end) / 2)
# Create the full dataset
data = []
for midpoint, count in zip(midpoints, counts):
    data.extend([midpoint] * count)
data = np.array(data)
# Compute μ and σ
μ = np.mean(data)
σ = np.std(data)
print(f"Computed Statistics:")
print(f"Mean (μ) = {μ:.4f}")
print(f"Standard Deviation (σ) = {σ:.4f}")
print(f"Minimum value = {np.min(data):.2f}")
print(f"Maximum value = {np.max(data):.2f}")
print()
# 1. P(X > μ)
p_greater_than_mean = 1 - norm.cdf(μ, loc=μ, scale=σ)
print(f"1. P(X > μ) = P(X > {μ:.2f})")
print(f"   = 1 - Φ(({μ:.2f} - {μ:.2f})/{σ:.2f})")
print(f"   = 1 - Φ(0)")
print(f"   = 1 - 0.5")
print(f"   = 0.5 or 50%")
print()
# 2. P(μ - σ < X < μ + σ)
lower = μ - σ
upper = μ + σ
p_within_one_sigma = norm.cdf(upper, loc=μ, scale=σ) - norm.cdf(lower, loc=μ, scale=σ)
print(f"2. P(μ - σ < X < μ + σ)")
print(f"   = P({μ:.2f} - {σ:.2f} < X < {μ:.2f} + {σ:.2f})")
print(f"   = P({lower:.2f} < X < {upper:.2f})")
print(f"   = Φ(({upper:.2f} - {μ:.2f})/{σ:.2f}) - Φ(({lower:.2f} - {μ:.2f})/{σ:.2f})")
print(f"   = Φ(1) - Φ(-1)")
print(f"   = 0.8413 - 0.1587")
print(f"   = 0.6827 or 68.27%")
print()
# 3. P(X < μ - 2σ)
lower_bound = μ - 2*σ
p_below_two_sigma = norm.cdf(lower_bound, loc=μ, scale=σ)
print(f"3. P(X < μ - 2σ)")
print(f"   = P(X < {μ:.2f} - 2×{σ:.2f})")
print(f"   = P(X < {lower_bound:.2f})")
print(f"   = Φ(({lower_bound:.2f} - {μ:.2f})/{σ:.2f})")
print(f"   = Φ(-2)")
print(f"   = 0.0228 or 2.28%")

"""**G3. Are Your Data Normally Distributed?**"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, skew, kurtosis
labels = ["100.00 - 118.00", "118.00 - 136.00", "136.00 - 154.00", "154.00 - 172.00",
          "172.00 - 190.00", "190.00 - 208.00", "208.00 - 226.00", "226.00 - 244.00",
          "244.00 - 262.00", "262.00 - 280.00", "280.00 - 298.00", "298.00 - 316.00",
          "316.00 - 334.00", "334.00 - 352.00", "352.00 - 370.00", "370.00 - 388.00",
          "388.00 - 406.00", "406.00 - 424.00", "424.00 - 442.00", "442.00 - 460.00",
          "460.00 - 478.00", "478.00 - 496.00", "496.00 - 514.00", "514.00 - 532.00",
          "532.00 - 550.00", "550.00 - 568.00", "568.00 - 586.00", "586.00 - 604.00",
          "604.00 - 622.00", "622.00 - 640.00", "640.00 - 658.00", "658.00 - 676.00",
          "676.00 - 694.00", "694.00 - 712.00", "712.00 - 730.00", "730.00 - 748.00",
          "748.00 - 766.00", "766.00 - 784.00", "784.00 - 802.00", "802.00 - 820.00",
          "820.00 - 838.00", "838.00 - 856.00", "856.00 - 874.00", "874.00 - 892.00",
          "892.00 - 910.00", "910.00 - 928.00", "928.00 - 946.00", "946.00 - 964.00",
          "964.00 - 982.00", "982.00 - 1000.00"]
counts = [20220, 20109, 20008, 19900, 19867, 19830, 20237, 20063, 20050, 20174,
          19953, 19667, 20148, 19693, 19959, 19954, 20004, 20195, 19920, 19987,
          19947, 20006, 19927, 19917, 20115, 19844, 19971, 20117, 20051, 19889,
          20087, 19788, 20177, 20139, 20270, 20124, 20009, 19972, 19860, 19833,
          20035, 19917, 20226, 20085, 19894, 19784, 19891, 20012, 20329, 19846]
midpoints = []
for label in labels:
    start, end = label.split(" - ")
    start = float(start)
    end = float(end)
    midpoints.append((start + end) / 2)
data = []
for midpoint, count in zip(midpoints, counts):
    data.extend([midpoint] * count)
data = np.array(data)
μ = np.mean(data)
σ = np.std(data)
median = np.median(data)
skewness = skew(data)
kurt = kurtosis(data)
print()
print("DESCRIPTIVE STATISTICS:")
print(f"Mean (μ): {μ:.4f}")
print(f"Median: {median:.4f}")
print(f"Standard Deviation (σ): {σ:.4f}")
print(f"Skewness: {skewness:.4f} (Normal ≈ 0)")
print(f"Kurtosis: {kurt:.4f} (Normal ≈ 0)")
print(f"Minimum: {np.min(data):.2f}")
print(f"Maximum: {np.max(data):.2f}")
print(f"Range: {np.max(data) - np.min(data):.2f}")
print()
print("NORMALITY INDICATORS:")
print("-" * 40)
print("1. MEAN vs MEDIAN:")
print(f"   Mean = {μ:.2f}, Median = {median:.2f}")
print(f"   Difference: {abs(μ - median):.2f}")
if abs(μ - median) < 0.01:
    print("   Mean ≈ Median (consistent with normality)")
else:
    print("   Mean ≠ Median (not consistent with normality)")
print()
print("2. SKEWNESS:")
print(f"   Value: {skewness:.4f}")
if abs(skewness) < 0.5:
    print(f"   Low skewness (|{skewness:.4f}| < 0.5)")
else:
    print(f"   Skewed distribution (|{skewness:.4f}| ≥ 0.5)")
print()
print("3. KURTOSIS:")
print(f"   Value: {kurt:.4f}")
if abs(kurt) < 0.5:
    print(f"   Normal kurtosis (|{kurt:.4f}| < 0.5)")
else:
    print(f"   Non-normal kurtosis (|{kurt:.4f}| ≥ 0.5)")
    if kurt < -0.5:
        print("   (Platykurtic - flatter than normal)")
    elif kurt > 0.5:
        print("   (Leptokurtic - more peaked than normal)")
print()
print("4. EMPIRICAL RULE CHECK:")
within_1σ = np.sum((data >= μ - σ) & (data <= μ + σ)) / len(data)
within_2σ = np.sum((data >= μ - 2*σ) & (data <= μ + 2*σ)) / len(data)
within_3σ = np.sum((data >= μ - 3*σ) & (data <= μ + 3*σ)) / len(data)
print("   Normal Distribution Expectation:")
print("   68% within μ ± σ, 95% within μ ± 2σ, 99.7% within μ ± 3σ")
print()
print(f"   Actual Data:")
print(f"   {within_1σ*100:.1f}% within μ ± σ ({μ-σ:.1f} to {μ+σ:.1f})")
print(f"   {within_2σ*100:.1f}% within μ ± 2σ ({μ-2*σ:.1f} to {μ+2*σ:.1f})")
print(f"   {within_3σ*100:.1f}% within μ ± 3σ ({μ-3*σ:.1f} to {μ+3*σ:.1f})")
print()
print("5. VISUAL ASSESSMENT:")
print("   Creating visualization...")
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
ax1 = axes[0, 0]
ax1.hist(data, bins=50, density=True, alpha=0.7, color='blue', edgecolor='black', label='Actual Data')
x = np.linspace(μ - 4*σ, μ + 4*σ, 1000)
y = norm.pdf(x, μ, σ)
ax1.plot(x, y, 'r-', linewidth=2, label='Normal Distribution')
ax1.axvline(μ, color='red', linestyle='--', alpha=0.5, label=f'Mean = {μ:.1f}')
ax1.set_xlabel('Value')
ax1.set_ylabel('Density')
ax1.set_title('Histogram vs Normal Distribution')
ax1.legend()
ax1.grid(True, alpha=0.3)
# Plot 2: Q-Q plot
ax2 = axes[0, 1]
from scipy.stats import probplot
probplot(data, dist="norm", plot=ax2)
ax2.set_title('Q-Q Plot (vs Normal Distribution)')
ax2.grid(True, alpha=0.3)
# Plot 3: Box plot
ax3 = axes[1, 0]
ax3.boxplot(data, vert=False, patch_artist=True)
ax3.set_xlabel('Value')
ax3.set_title('Box Plot (Check for outliers)')
ax3.grid(True, alpha=0.3)
# Plot 4: Cumulative distribution
ax4 = axes[1, 1]
sorted_data = np.sort(data)
y_vals = np.arange(1, len(sorted_data)+1) / len(sorted_data)
ax4.plot(sorted_data, y_vals, 'b-', linewidth=2, label='Empirical CDF')
# Theoretical normal CDF
x_norm = np.linspace(np.min(data), np.max(data), 1000)
y_norm = norm.cdf(x_norm, μ, σ)
ax4.plot(x_norm, y_norm, 'r--', linewidth=2, label='Normal CDF')
ax4.set_xlabel('Value')
ax4.set_ylabel('Cumulative Probability')
ax4.set_title('Cumulative Distribution Functions')
ax4.legend()
ax4.grid(True, alpha=0.3)
plt.suptitle(f'Normality Assessment for 100-1000 Variable (n={len(data):,})', fontsize=14)
plt.tight_layout()
plt.show()

"""**H. Task 6: Reflection**"""

import numpy as np
import pandas as pd
from scipy.stats import norm, chi2_contingency
import matplotlib.pyplot as plt
from scipy.stats import shapiro, kstest
sample_data = [
    {'Region': 'West', 'Soil': 'Sandy', 'Crop': 'Cotton'},{'Region': 'South', 'Soil': 'Clay', 'Crop': 'Rice'},{'Region': 'North', 'Soil': 'Loam', 'Crop': 'Barley'},
    {'Region': 'North', 'Soil': 'Sandy', 'Crop': 'Soybean'},{'Region': 'South', 'Soil': 'Silt', 'Crop': 'Wheat'},{'Region': 'South', 'Soil': 'Silt', 'Crop': 'Soybean'},
    {'Region': 'West', 'Soil': 'Clay', 'Crop': 'Wheat'},{'Region': 'South', 'Soil': 'Sandy', 'Crop': 'Rice'},{'Region': 'North', 'Soil': 'Silt', 'Crop': 'Wheat'},
    {'Region': 'West', 'Soil': 'Sandy', 'Crop': 'Wheat'},{'Region': 'North', 'Soil': 'Peaty', 'Crop': 'Wheat'} ]
df = pd.DataFrame(sample_data)
print("Marginal vs Conditional Probabilities:")
print()
print("Marginal (Overall) Probabilities:")
print(f"P(Rice) = {len(df[df['Crop'] == 'Rice'])/len(df):.2f}")
print(f"P(Wheat) = {len(df[df['Crop'] == 'Wheat'])/len(df):.2f}")
print(f"P(Clay) = {len(df[df['Soil'] == 'Clay'])/len(df):.2f}")
print()
print("Conditional Probabilities:")
print(f"P(Rice | Clay) = {len(df[(df['Crop'] == 'Rice') & (df['Soil'] == 'Clay')])/len(df[df['Soil'] == 'Clay']):.2f}")
print(f"P(Wheat | Silt) = {len(df[(df['Crop'] == 'Wheat') & (df['Soil'] == 'Silt')])/len(df[df['Soil'] == 'Silt']):.2f}")
print(f"P(Cotton | Sandy) = {len(df[(df['Crop'] == 'Cotton') & (df['Soil'] == 'Sandy')])/len(df[df['Soil'] == 'Sandy']):.2f}")
print()
print("Key Insight: Conditional probabilities differ significantly from marginal probabilities.")
print("This shows strong dependencies between variables.")
print()
print("2. INDEPENDENCE OF EVENTS")
print("-"*40)
contingency = pd.crosstab(df['Soil'], df['Crop'])
print("Contingency Table (Soil × Crop):")
print(contingency)
print()
chi2, p, dof, expected = chi2_contingency(contingency)
print(f"Chi-square test for independence (Soil vs Crop):")
print(f"  χ² = {chi2:.2f}, p-value = {p:.4f}")
print()
if p < 0.05:
    print("CONCLUSION: Events are NOT independent (p < 0.05)")
    print("   Soil type and crop choice are significantly dependent.")
else:
    print("CONCLUSION: Events are independent (p ≥ 0.05)")
print()
print("3. NORMAL DISTRIBUTION FIT")
print("-"*40)
print("Variable: -1.15 to 9.96 (appears bell-shaped)")
print()
labels_neg = [
    "-1.15 - -0.93", "-0.93 - -0.70", "-0.70 - -0.48", "-0.48 - -0.26",
    "-0.26 - -0.04", "-0.04 - 0.19", "0.19 - 0.41", "0.41 - 0.63",
    "0.63 - 0.85", "0.85 - 1.07", "1.07 - 1.30", "1.30 - 1.52",
    "1.52 - 1.74", "1.74 - 1.96", "1.96 - 2.19", "2.19 - 2.41",
    "2.41 - 2.63", "2.63 - 2.85", "2.85 - 3.07", "3.07 - 3.30",
    "3.30 - 3.52", "3.52 - 3.74", "3.74 - 3.96", "3.96 - 4.19",
    "4.19 - 4.41", "4.41 - 4.63", "4.63 - 4.85", "4.85 - 5.07",
    "5.07 - 5.30", "5.30 - 5.52", "5.52 - 5.74", "5.74 - 5.96",
    "5.96 - 6.19", "6.19 - 6.41", "6.41 - 6.63", "6.63 - 6.85",
    "6.85 - 7.07", "7.07 - 7.30", "7.30 - 7.52", "7.52 - 7.74",
    "7.74 - 7.96", "7.96 - 8.19", "8.19 - 8.41", "8.41 - 8.63",
    "8.63 - 8.85", "8.85 - 9.07", "9.07 - 9.30", "9.30 - 9.52",
    "9.52 - 9.74", "9.74 - 9.96" ]
counts_neg = [
    1, 5, 14, 40, 130, 363, 951, 1915, 3476, 5421,
    7607, 10234, 12739, 15401, 19148, 22759, 26915, 30814, 33967, 37015,
    39237, 41487, 44105, 46243, 47299, 47960, 48556, 47777, 46342, 44825,
    42102, 40001, 37250, 34439, 31402, 27882, 23376, 19459, 16117, 12986,
    10584, 8041, 5950, 3745, 2188, 1077, 439, 161, 47, 8 ]
neg_data = []
for label, count in zip(labels_neg, counts_neg):
    start, end = label.split(" - ")
    start = float(start)
    end = float(end)
    midpoint = (start + end) / 2
    neg_data.extend([midpoint] * count)
neg_data = np.array(neg_data)
neg_mean = np.mean(neg_data)
neg_std = np.std(neg_data)
neg_skew = (np.mean((neg_data - neg_mean)**3)) / (neg_std**3)
neg_kurt = (np.mean((neg_data - neg_mean)**4)) / (neg_std**4) - 3
print("Statistics:")
print(f"  Mean: {neg_mean:.4f}")
print(f"  Standard Deviation: {neg_std:.4f}")
print(f"  Skewness: {neg_skew:.4f} (Normal ≈ 0)")
print(f"  Kurtosis: {neg_kurt:.4f} (Normal ≈ 0)")
print()
sample_for_test = np.random.choice(neg_data, min(5000, len(neg_data)), replace=False)
stat, p_shapiro = shapiro(sample_for_test)
print(f"Shapiro-Wilk Test (normality):")
print(f"  Statistic = {stat:.4f}, p-value = {p_shapiro:.6f}")
if p_shapiro > 0.05:
    print("  Cannot reject normality (p > 0.05)")
else:
    print("  Reject normality (p ≤ 0.05)")
ks_stat, p_ks = kstest(neg_data, 'norm', args=(neg_mean, neg_std))
print(f"\nKolmogorov-Smirnov Test:")
print(f"  Statistic = {ks_stat:.4f}, p-value = {p_ks:.6f}")
if p_ks > 0.05:
    print("  Cannot reject normality (p > 0.05)")
else:
    print("  Reject normality (p ≤ 0.05)")
fig = plt.figure(figsize=(16, 10))
ax1 = plt.subplot(2, 2, 1)
ax1.hist(neg_data, bins=30, density=True, alpha=0.7, color='green', edgecolor='black')
x = np.linspace(neg_mean - 4*neg_std, neg_mean + 4*neg_std, 1000)
y = norm.pdf(x, neg_mean, neg_std)
ax1.plot(x, y, 'r-', linewidth=2)
ax1.set_xlabel('Value')
ax1.set_ylabel('Density')
ax1.set_title('Normal Distribution Fit (-1.15 to 9.96 variable)')
ax1.text(0.02, 0.98, f'μ = {neg_mean:.2f}\nσ = {neg_std:.2f}',
         transform=ax1.transAxes, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
ax2 = plt.subplot(2, 2, 2)
soil_crop_counts = contingency.values
crops = contingency.columns
soils = contingency.index
x_pos = np.arange(len(soils))
bar_width = 0.8/len(crops)
for i, crop in enumerate(crops):
    ax2.bar(x_pos + i*bar_width, soil_crop_counts[:, i], width=bar_width, label=crop)
ax2.set_xlabel('Soil Type')
ax2.set_ylabel('Count')
ax2.set_title('Soil-Crop Dependencies (Sample Data)')
ax2.set_xticks(x_pos + bar_width*(len(crops)-1)/2)
ax2.set_xticklabels(soils)
ax2.legend()
ax2.text(0.02, 0.98, f'χ² p-value = {p:.4f}', transform=ax2.transAxes,
         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
ax3 = plt.subplot(2, 2, 3)
decision_data = {
    'Optimal': [0.85, 0.70, 0.60],
    'Suboptimal': [0.60, 0.50, 0.40],
    'Poor': [0.30, 0.20, 0.10] }
conditions = ['Good Weather', 'Average', 'Poor Weather']
x = np.arange(len(conditions))
width = 0.25
multiplier = 0
for attribute, measurement in decision_data.items():
    offset = width * multiplier
    rects = ax3.bar(x + offset, measurement, width, label=attribute)
    multiplier += 1
ax3.set_ylabel('Crop Success Probability')
ax3.set_title('Decision Making: Weather Impact on Crop Success')
ax3.set_xticks(x + width)
ax3.set_xticklabels(conditions)
ax3.legend(loc='upper right')
ax3.set_ylim(0, 1)
# Plot 4: Risk assessment
ax4 = plt.subplot(2, 2, 4)
risk_levels = ['Low (< μ-σ)', 'Medium (μ±σ)', 'High (> μ+σ)']
probabilities = [0.1587, 0.6827, 0.1587]  # Normal distribution areas
colors = ['green', 'yellow', 'red']
ax4.bar(risk_levels, probabilities, color=colors, edgecolor='black')
ax4.set_ylabel('Probability')
ax4.set_title('Risk Assessment Using Normal Distribution')
ax4.set_ylim(0, 1)
for i, (level, prob) in enumerate(zip(risk_levels, probabilities)):
    ax4.text(i, prob + 0.02, f'{prob*100:.1f}%', ha='center')
plt.suptitle('Summary: Probability and Distribution Applications in Agriculture', fontsize=14)
plt.tight_layout()
plt.show()
sample_yields = [897.08, 992.67, 148.00, 986.87, 730.38, 797.47, 357.90, 441.13, 181.59, 395.05, 385.14]
expected_yield = np.mean(sample_yields)
yield_std = np.std(sample_yields)
yield_cv = (yield_std / expected_yield) * 100
print(f"Expected Yield (sample): {expected_yield:.2f} ± {yield_std:.2f}")
print(f"Coefficient of Variation: {yield_cv:.1f}% (High variability → High risk)")
high_yield_prob = len([y for y in sample_yields if y > 800]) / len(sample_yields)
print(f"Probability of High Yield (>800): {high_yield_prob:.2f}")
soil_yield_pairs = {
    'Sandy': [897.08, 986.87, 441.13, 395.05],
    'Clay': [992.67, 357.90],
    'Loam': [148.00],
    'Silt': [730.38, 797.47, 181.59],
    'Peaty': [385.14]
}
print("\nExpected Yield by Soil Type:")
for soil, yields in soil_yield_pairs.items():
    if yields:
        exp_yield = np.mean(yields)
        print(f"  {soil}: {exp_yield:.2f} (n={len(yields)})")

"""**I. Submission Guidelines**"""

from IPython.display import Image
Image(filename='/content/q.jpeg')

"""**Analysis and Conclusion**"""

print("""This indicates that for every 1 mm increase in rainfall, the average temperature decreases by about 0.0066°C. In other words, higher rainfall is associated
with slightly lower temperatures. When rainfall is zero (X = 0), the estimated average temperature is 31.15°C. This is a theoretical value used for interpretation.
This also means that only 10.58% of the variation in temperature can be explained by rainfall. The remaining 89.42% of the variation is influenced by other factors
such as fertilizer use, crop variety, and environmental conditions.""")

"""**[Milestone 7](https://)**

**A. Introduction**
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List

class SimpleLinearRegression:
    """Simple Linear Regression implementation from scratch"""

    def __init__(self):
        self.beta_0 = None  # Intercept
        self.beta_1 = None  # Slope
        self.r_squared = None  # Coefficient of determination

    def fit(self, X: np.ndarray, y: np.ndarray) -> None:
        """
        Fit the linear regression model

        Parameters:
        -----------
        X : independent variable (1D array)
        y : dependent variable (1D array)
        """
        # Input validation
        if len(X) != len(y):
            raise ValueError("X and y must have the same length")

        n = len(X)

        # Calculate necessary sums
        sum_x = np.sum(X)
        sum_y = np.sum(y)
        sum_xy = np.sum(X * y)
        sum_x_squared = np.sum(X ** 2)
        sum_y_squared = np.sum(y ** 2)

        # Calculate slope (β₁)
        numerator = n * sum_xy - sum_x * sum_y
        denominator = n * sum_x_squared - sum_x ** 2

        if denominator == 0:
            raise ValueError("Denominator is zero, cannot compute slope")

        self.beta_1 = numerator / denominator

        # Calculate intercept (β₀)
        self.beta_0 = (sum_y - self.beta_1 * sum_x) / n

        # Calculate R-squared
        y_pred = self.beta_0 + self.beta_1 * X
        ss_res = np.sum((y - y_pred) ** 2)  # Residual sum of squares
        ss_tot = np.sum((y - np.mean(y)) ** 2)  # Total sum of squares
        self.r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0

    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Make predictions using the fitted model

        Parameters:
        -----------
        X : independent variable values to predict

        Returns:
        --------
        Predicted y values
        """
        if self.beta_0 is None or self.beta_1 is None:
            raise ValueError("Model must be fitted before making predictions")

        return self.beta_0 + self.beta_1 * X

    def get_parameters(self) -> Tuple[float, float]:
        """Return the model parameters"""
        return self.beta_0, self.beta_1

    def get_r_squared(self) -> float:
        """Return the coefficient of determination"""
        return self.r_squared


# Helper functions for manual calculation (step-by-step)
def calculate_regression_parameters_manual(X: np.ndarray, y: np.ndarray) -> dict:
    """
    Calculate regression parameters manually step by step
    Returns intermediate calculations for educational purposes
    """
    n = len(X)

    # Step 1: Calculate means
    x_mean = np.mean(X)
    y_mean = np.mean(y)

    # Step 2: Calculate deviations from means
    x_dev = X - x_mean
    y_dev = y - y_mean

    # Step 3: Calculate required sums
    sum_xy = np.sum(x_dev * y_dev)
    sum_x_squared = np.sum(x_dev ** 2)

    # Step 4: Calculate slope
    beta_1 = sum_xy / sum_x_squared if sum_x_squared != 0 else 0

    # Step 5: Calculate intercept
    beta_0 = y_mean - beta_1 * x_mean

    # Step 6: Calculate predictions and residuals
    y_pred = beta_0 + beta_1 * X
    residuals = y - y_pred

    # Step 7: Calculate R-squared
    ss_res = np.sum(residuals ** 2)
    ss_tot = np.sum((y - y_mean) ** 2)
    r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0

    return {
        'n': n,
        'x_mean': x_mean,
        'y_mean': y_mean,
        'sum_xy': sum_xy,
        'sum_x_squared': sum_x_squared,
        'beta_0': beta_0,
        'beta_1': beta_1,
        'y_pred': y_pred,
        'residuals': residuals,
        'r_squared': r_squared
    }


# Example usage with sample data
def main():
    # Sample data (hours studied vs exam score)
    X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])  # Hours studied
    y = np.array([55, 60, 65, 70, 75, 80, 85, 90, 95, 100])  # Exam score

    print("=" * 60)
    print("SIMPLE LINEAR REGRESSION IMPLEMENTATION")
    print("=" * 60)

    # Method 1: Using the class implementation
    print("\n1. Using Class Implementation:")
    print("-" * 40)

    model = SimpleLinearRegression()
    model.fit(X, y)
    beta_0, beta_1 = model.get_parameters()

    print(f"Regression Equation: ŷ = {beta_0:.4f} + {beta_1:.4f}x")
    print(f"R-squared: {model.get_r_squared():.4f}")

    # Make predictions
    X_test = np.array([2.5, 5.5, 8.5])
    predictions = model.predict(X_test)
    for x, pred in zip(X_test, predictions):
        print(f"Predicted score for {x} hours: {pred:.2f}")

    # Method 2: Manual step-by-step calculation
    print("\n\n2. Manual Step-by-Step Calculation:")
    print("-" * 40)

    manual_results = calculate_regression_parameters_manual(X, y)

    print(f"Number of observations (n): {manual_results['n']}")
    print(f"Mean of X (x̄): {manual_results['x_mean']:.4f}")
    print(f"Mean of y (ȳ): {manual_results['y_mean']:.4f}")
    print(f"Sum of (x - x̄)(y - ȳ): {manual_results['sum_xy']:.4f}")
    print(f"Sum of (x - x̄)²: {manual_results['sum_x_squared']:.4f}")
    print(f"Slope (β₁): {manual_results['beta_1']:.4f}")
    print(f"Intercept (β₀): {manual_results['beta_0']:.4f}")
    print(f"R-squared: {manual_results['r_squared']:.4f}")

    # Verify both methods give same results
    print("\n3. Verification:")
    print("-" * 40)
    print(f"Method comparison - Intercept: {beta_0:.4f} vs {manual_results['beta_0']:.4f}")
    print(f"Method comparison - Slope: {beta_1:.4f} vs {manual_results['beta_1']:.4f}")

    # Visualization
    plot_regression_line(X, y, model, manual_results)


def plot_regression_line(X: np.ndarray, y: np.ndarray,
                         model: SimpleLinearRegression,
                         manual_results: dict):
    """Visualize the data and regression line"""
    plt.figure(figsize=(12, 5))

    # Plot 1: Data points and regression line
    plt.subplot(1, 2, 1)
    plt.scatter(X, y, color='blue', label='Actual Data', alpha=0.7)

    # Generate points for regression line
    x_line = np.linspace(min(X) - 1, max(X) + 1, 100)
    y_line = model.predict(x_line)
    plt.plot(x_line, y_line, color='red', linewidth=2, label='Regression Line')

    # Plot residuals
    y_pred = manual_results['y_pred']
    for xi, yi, ypi in zip(X, y, y_pred):
        plt.plot([xi, xi], [yi, ypi], color='gray', linestyle='--', alpha=0.5)

    plt.xlabel('Hours Studied (X)')
    plt.ylabel('Exam Score (y)')
    plt.title('Simple Linear Regression: Hours Studied vs Exam Score')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # Plot 2: Residuals
    plt.subplot(1, 2, 2)
    residuals = manual_results['residuals']
    plt.scatter(X, residuals, color='green', alpha=0.7)
    plt.axhline(y=0, color='red', linestyle='--', alpha=0.5)
    plt.xlabel('Hours Studied (X)')
    plt.ylabel('Residuals (y - ŷ)')
    plt.title('Residual Plot')
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    main()

"""**B. Knowledge Points: The Least Squares Method**"""

import numpy as np
import matplotlib.pyplot as plt

class SimpleLinearRegressionDetailed:
    def __init__(self):
        self.beta_0 = None  # Intercept
        self.beta_1 = None  # Slope
        self.SSR = None     # Sum of Squared Residuals
        self.SST = None     # Total Sum of Squares
        self.SSE = None     # Sum of Squares Explained
        self.r_squared = None

    def fit(self, X, y):
        """Fit the model by minimizing SSR"""
        n = len(X)

        # Calculate means
        x_mean = np.mean(X)
        y_mean = np.mean(y)

        # Calculate sums needed for β₁
        sum_xy = np.sum(X * y)
        sum_x = np.sum(X)
        sum_y = np.sum(y)
        sum_x_squared = np.sum(X ** 2)

        # Calculate β₁ and β₀
        numerator = n * sum_xy - sum_x * sum_y
        denominator = n * sum_x_squared - sum_x ** 2
        self.beta_1 = numerator / denominator
        self.beta_0 = y_mean - self.beta_1 * x_mean

        # Calculate predicted values
        y_pred = self.beta_0 + self.beta_1 * X

        # Calculate sums of squares
        self.SSR = np.sum((y - y_pred) ** 2)        # Sum of Squared Residuals
        self.SST = np.sum((y - y_mean) ** 2)        # Total Sum of Squares
        self.SSE = np.sum((y_pred - y_mean) ** 2)   # Sum of Squares Explained

        # R-squared calculation (3 equivalent forms)
        self.r_squared = 1 - (self.SSR / self.SST)  # Most common form
        # self.r_squared = self.SSE / self.SST      # Alternative form
        # self.r_squared = np.corrcoef(X, y)[0,1]**2 # From correlation

        return self

    def predict(self, X):
        return self.beta_0 + self.beta_1 * X

# Example with non-perfect data (realistic scenario)
def demonstrate_least_squares():
    np.random.seed(42)

    # Generate data with noise (realistic linear relationship)
    X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    true_beta_0 = 50
    true_beta_1 = 3
    noise = np.random.normal(0, 5, len(X))  # Add some noise
    y = true_beta_0 + true_beta_1 * X + noise

    print("REAL-WORLD LINEAR REGRESSION EXAMPLE")
    print("=" * 50)
    print(f"True relationship: y = {true_beta_0} + {true_beta_1}x + noise")
    print(f"Data points: {list(zip(X, np.round(y, 2)))}\n")

    # Fit the model
    model = SimpleLinearRegressionDetailed().fit(X, y)

    print("LEAST SQUARES RESULTS:")
    print("-" * 30)
    print(f"Fitted equation: ŷ = {model.beta_0:.4f} + {model.beta_1:.4f}x")
    print(f"\nSum of Squares Breakdown:")
    print(f"SSR (Sum of Squared Residuals): {model.SSR:.4f}")
    print(f"SST (Total Sum of Squares): {model.SST:.4f}")
    print(f"SSE (Sum of Squares Explained): {model.SSE:.4f}")
    print(f"SSR + SSE = {model.SSR + model.SSE:.4f} (should equal SST: {model.SST:.4f})")
    print(f"\nR-squared: {model.r_squared:.4f}")
    print(f"Proportion of variance explained: {model.r_squared*100:.1f}%")

    # Demonstrate SSR minimization
    print("\n" + "=" * 50)
    print("DEMONSTRATING SSR MINIMIZATION")
    print("-" * 30)

    # Try different beta_1 values to show SSR is minimized at our calculated β₁
    beta_1_test_values = np.linspace(model.beta_1 - 2, model.beta_1 + 2, 10)
    ssr_values = []

    for test_beta_1 in beta_1_test_values:
        # Keep β₀ fixed at optimal value for fair comparison
        test_y_pred = model.beta_0 + test_beta_1 * X
        ssr = np.sum((y - test_y_pred) ** 2)
        ssr_values.append(ssr)
        print(f"β₁ = {test_beta_1:.3f}, SSR = {ssr:.3f}")

    # Visualize
    visualize_regression(X, y, model, beta_1_test_values, ssr_values)

    return X, y, model

def visualize_regression(X, y, model, beta_1_test_values, ssr_values):
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # Plot 1: Data and regression line
    axes[0].scatter(X, y, color='blue', alpha=0.7, label='Actual Data')
    x_line = np.linspace(min(X) - 1, max(X) + 1, 100)
    y_line = model.predict(x_line)
    axes[0].plot(x_line, y_line, 'r-', linewidth=2, label='Regression Line')

    # Show residuals
    y_pred = model.predict(X)
    for xi, yi, ypi in zip(X, y, y_pred):
        axes[0].plot([xi, xi], [yi, ypi], 'k--', alpha=0.3)

    axes[0].set_xlabel('X (Independent Variable)')
    axes[0].set_ylabel('Y (Dependent Variable)')
    axes[0].set_title('Least Squares Regression Line')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # Plot 2: SSR as function of β₁
    axes[1].plot(beta_1_test_values, ssr_values, 'b-', linewidth=2)
    axes[1].axvline(x=model.beta_1, color='r', linestyle='--', alpha=0.7,
                   label=f'Minimum at β₁ = {model.beta_1:.3f}')
    axes[1].scatter(model.beta_1, model.SSR, color='red', s=100, zorder=5)
    axes[1].set_xlabel('β₁ (Slope)')
    axes[1].set_ylabel('SSR (Sum of Squared Residuals)')
    axes[1].set_title('SSR Minimization at Optimal β₁')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    # Plot 3: Residual plot
    residuals = y - y_pred
    axes[2].scatter(y_pred, residuals, color='green', alpha=0.7)
    axes[2].axhline(y=0, color='red', linestyle='--', alpha=0.5)
    axes[2].set_xlabel('Predicted Values (ŷ)')
    axes[2].set_ylabel('Residuals (y - ŷ)')
    axes[2].set_title('Residual Plot (Checking Model Assumptions)')
    axes[2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

# Statistical properties of the estimators
def statistical_properties():
    print("\n" + "=" * 60)
    print("STATISTICAL PROPERTIES OF β₀ AND β₁ ESTIMATORS")
    print("=" * 60)

    print("\nUnder the Gauss-Markov assumptions:")
    print("1. Linearity: Y = β₀ + β₁X + ε")
    print("2. Random sampling: (Xᵢ, Yᵢ) are i.i.d.")
    print("3. No perfect multicollinearity")
    print("4. Zero conditional mean: E(ε|X) = 0")
    print("5. Homoscedasticity: Var(ε|X) = σ²")
    print("6. No autocorrelation: Cov(εᵢ, εⱼ|X) = 0 for i ≠ j")

    print("\nThe OLS estimators have these properties:")
    print("✓ Unbiased: E(β̂₁) = β₁ and E(β̂₀) = β₀")
    print("✓ Consistent: As n → ∞, β̂ → β")
    print("✓ Efficient: Minimum variance among linear unbiased estimators (BLUE)")
    print("✓ Normally distributed (with large n or normal errors)")

# Run the demonstration
if __name__ == "__main__":
    X, y, model = demonstrate_least_squares()
    statistical_properties()

"""**1. Regression Parameters**

**1.1 The Slope (β1)**
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

def demonstrate_slope_derivation():
    """
    Demonstrate why β₁ = Cov(X,Y) / Var(X)
    """
    np.random.seed(42)

    # Generate correlated data
    n = 100
    X = np.random.normal(10, 2, n)  # Mean=10, SD=2

    # Create Y with a linear relationship plus noise
    true_slope = 2.5
    true_intercept = 5
    noise = np.random.normal(0, 3, n)
    Y = true_intercept + true_slope * X + noise

    print("SLOPE DERIVATION DEMONSTRATION")
    print("=" * 60)

    # Method 1: Direct calculation using deviations
    X_mean = np.mean(X)
    Y_mean = np.mean(Y)

    # Calculate numerator and denominator
    numerator = np.sum((X - X_mean) * (Y - Y_mean))
    denominator = np.sum((X - X_mean) ** 2)

    beta_1_direct = numerator / denominator

    # Method 2: Using covariance and variance
    # Note: np.cov returns covariance matrix, with bias by default (ddof=0)
    covariance_xy = np.cov(X, Y, ddof=0)[0, 1]  # Population covariance
    variance_x = np.var(X, ddof=0)  # Population variance
    beta_1_cov_var = covariance_xy / variance_x

    # Method 3: Using correlation and standard deviations
    correlation = np.corrcoef(X, Y)[0, 1]
    std_x = np.std(X, ddof=0)
    std_y = np.std(Y, ddof=0)
    beta_1_corr = correlation * (std_y / std_x)

    print("\n1. DIRECT CALCULATION:")
    print("-" * 40)
    print(f"∑(Xᵢ - X̄)(Yᵢ - Ȳ) = {numerator:.4f}")
    print(f"∑(Xᵢ - X̄)² = {denominator:.4f}")
    print(f"β₁ = {numerator:.4f} / {denominator:.4f} = {beta_1_direct:.4f}")

    print("\n2. COVARIANCE/VARIANCE METHOD:")
    print("-" * 40)
    print(f"Cov(X,Y) = {covariance_xy:.4f}")
    print(f"Var(X) = {variance_x:.4f}")
    print(f"β₁ = {covariance_xy:.4f} / {variance_x:.4f} = {beta_1_cov_var:.4f}")

    print("\n3. CORRELATION/STANDARD DEVIATION METHOD:")
    print("-" * 40)
    print(f"Correlation ρ = {correlation:.4f}")
    print(f"σ_Y = {std_y:.4f}")
    print(f"σ_X = {std_x:.4f}")
    print(f"β₁ = ρ * (σ_Y/σ_X) = {correlation:.4f} * ({std_y:.4f}/{std_x:.4f}) = {beta_1_corr:.4f}")

    print("\n4. VERIFICATION:")
    print("-" * 40)
    print(f"Method 1 (Direct): β₁ = {beta_1_direct:.6f}")
    print(f"Method 2 (Cov/Var): β₁ = {beta_1_cov_var:.6f}")
    print(f"Method 3 (Correlation): β₁ = {beta_1_corr:.6f}")
    print(f"All methods give the same result: {np.allclose([beta_1_direct, beta_1_cov_var, beta_1_corr], beta_1_direct)}")

    # Calculate intercept
    beta_0 = Y_mean - beta_1_direct * X_mean

    print(f"\n5. COMPLETE REGRESSION EQUATION:")
    print("-" * 40)
    print(f"Ȳ = {Y_mean:.4f}, X̄ = {X_mean:.4f}")
    print(f"β₀ = Ȳ - β₁X̄ = {Y_mean:.4f} - {beta_1_direct:.4f} * {X_mean:.4f} = {beta_0:.4f}")
    print(f"Regression Line: Ŷ = {beta_0:.4f} + {beta_1_direct:.4f}X")

    return X, Y, beta_0, beta_1_direct

def visualize_covariance_variance_relationship(X, Y, beta_1):
    """
    Visualize how covariance and variance relate to the slope
    """
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))

    X_mean = np.mean(X)
    Y_mean = np.mean(Y)

    # Plot 1: Raw data with means
    axes[0, 0].scatter(X, Y, alpha=0.6, color='blue')
    axes[0, 0].axhline(Y_mean, color='red', linestyle='--', alpha=0.7, label=f'Ȳ = {Y_mean:.2f}')
    axes[0, 0].axvline(X_mean, color='green', linestyle='--', alpha=0.7, label=f'X̄ = {X_mean:.2f}')
    axes[0, 0].set_xlabel('X')
    axes[0, 0].set_ylabel('Y')
    axes[0, 0].set_title('Data with Mean Lines')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # Plot 2: Deviations from means
    for i in range(min(20, len(X))):  # Plot first 20 points for clarity
        axes[0, 1].plot([X[i], X[i]], [Y[i], Y_mean], 'gray', alpha=0.3, linestyle=':')
        axes[0, 1].plot([X[i], X_mean], [Y[i], Y[i]], 'gray', alpha=0.3, linestyle=':')
    axes[0, 1].scatter(X, Y, alpha=0.6, color='blue')
    axes[0, 1].axhline(Y_mean, color='red', linestyle='--', alpha=0.7)
    axes[0, 1].axvline(X_mean, color='green', linestyle='--', alpha=0.7)
    axes[0, 1].set_xlabel('X')
    axes[0, 1].set_ylabel('Y')
    axes[0, 1].set_title('Deviations from Means')
    axes[0, 1].grid(True, alpha=0.3)

    # Plot 3: Products of deviations
    products = (X - X_mean) * (Y - Y_mean)
    colors = ['green' if p > 0 else 'red' for p in products]
    axes[0, 2].bar(range(len(products[:20])), products[:20], color=colors[:20], alpha=0.6)
    axes[0, 2].axhline(0, color='black', linestyle='-', alpha=0.5)
    axes[0, 2].set_xlabel('Data Point Index')
    axes[0, 2].set_ylabel('(Xᵢ - X̄)(Yᵢ - Ȳ)')
    axes[0, 2].set_title('Products of Deviations (Covariance Components)')
    axes[0, 2].grid(True, alpha=0.3, axis='y')

    # Plot 4: Squared deviations of X
    squared_deviations = (X - X_mean) ** 2
    axes[1, 0].bar(range(len(squared_deviations[:20])), squared_deviations[:20],
                   color='blue', alpha=0.6)
    axes[1, 0].set_xlabel('Data Point Index')
    axes[1, 0].set_ylabel('(Xᵢ - X̄)²')
    axes[1, 0].set_title('Squared Deviations of X (Variance Components)')
    axes[1, 0].grid(True, alpha=0.3, axis='y')

    # Plot 5: Regression line with covariance quadrants
    axes[1, 1].scatter(X, Y, alpha=0.6, color='blue')

    # Add quadrant shading based on covariance signs
    xlim = axes[1, 1].get_xlim()
    ylim = axes[1, 1].get_ylim()

    # Quadrant I: (X > X̄, Y > Ȳ) - Positive contribution to covariance
    axes[1, 1].fill_between([X_mean, xlim[1]], [Y_mean, Y_mean], [ylim[1], ylim[1]],
                           alpha=0.1, color='green', label='Positive Covariance')
    # Quadrant III: (X < X̄, Y < Ȳ) - Positive contribution to covariance
    axes[1, 1].fill_between([xlim[0], X_mean], [ylim[0], ylim[0]], [Y_mean, Y_mean],
                           alpha=0.1, color='green')
    # Quadrants II & IV: Negative contribution to covariance
    axes[1, 1].fill_between([xlim[0], X_mean], [Y_mean, Y_mean], [ylim[1], ylim[1]],
                           alpha=0.1, color='red', label='Negative Covariance')
    axes[1, 1].fill_between([X_mean, xlim[1]], [ylim[0], ylim[0]], [Y_mean, Y_mean],
                           alpha=0.1, color='red')

    # Plot regression line
    x_line = np.array([min(X), max(X)])
    y_line = beta_0 + beta_1 * x_line
    axes[1, 1].plot(x_line, y_line, 'black', linewidth=2, label='Regression Line')

    axes[1, 1].axhline(Y_mean, color='red', linestyle='--', alpha=0.7)
    axes[1, 1].axvline(X_mean, color='green', linestyle='--', alpha=0.7)
    axes[1, 1].set_xlabel('X')
    axes[1, 1].set_ylabel('Y')
    axes[1, 1].set_title('Covariance Quadrants and Regression Line')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

    # Plot 6: Slope interpretation
    axes[1, 2].scatter(X, Y, alpha=0.3, color='blue')
    axes[1, 2].plot(x_line, y_line, 'black', linewidth=2)

    # Annotate slope
    mid_x = (min(X) + max(X)) / 2
    mid_y = beta_0 + beta_1 * mid_x
    dx = 1  # One unit change in X
    dy = beta_1 * dx  # Corresponding change in Y

    # Draw arrow showing slope
    axes[1, 2].arrow(mid_x, mid_y, dx, 0, head_width=0.5, head_length=0.2,
                     fc='red', ec='red', alpha=0.7)
    axes[1, 2].arrow(mid_x + dx, mid_y, 0, dy, head_width=0.2, head_length=0.5,
                     fc='red', ec='red', alpha=0.7)
    axes[1, 2].text(mid_x + dx/2, mid_y - 0.5, 'ΔX = 1', ha='center', va='top',
                   fontsize=10, color='red')
    axes[1, 2].text(mid_x + dx + 0.2, mid_y + dy/2, f'ΔŶ = {beta_1:.2f}',
                   ha='left', va='center', fontsize=10, color='red')

    axes[1, 2].set_xlabel('X')
    axes[1, 2].set_ylabel('Y')
    axes[1, 2].set_title(f'Slope Interpretation: β₁ = {beta_1:.2f}')
    axes[1, 2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def statistical_insights():
    """
    Provide statistical insights about the slope formula
    """
    print("\n" + "=" * 60)
    print("STATISTICAL INSIGHTS: β₁ = Cov(X,Y) / Var(X)")
    print("=" * 60)

    print("\n1. UNITS AND INTERPRETATION:")
    print("-" * 40)
    print("• Cov(X,Y) has units: (units of X) × (units of Y)")
    print("• Var(X) has units: (units of X)²")
    print("• β₁ units: (units of Y) / (units of X)")
    print("• Interpretation: Change in Y per unit change in X")

    print("\n2. GEOMETRIC INTERPRETATION:")
    print("-" * 40)
    print("• The slope β₁ is the tangent of the angle between the")
    print("  regression line and the horizontal axis")
    print("• It represents the 'steepness' of the relationship")

    print("\n3. COVARIANCE SIGN DETERMINES SLOPE DIRECTION:")
    print("-" * 40)
    print("• Cov(X,Y) > 0 → β₁ > 0 → Positive relationship")
    print("• Cov(X,Y) < 0 → β₁ < 0 → Negative relationship")
    print("• Cov(X,Y) = 0 → β₁ = 0 → No linear relationship")

    print("\n4. VARIANCE AFFECTS SLOPE PRECISION:")
    print("-" * 40)
    print("• Larger Var(X) → More precise estimate of β₁")
    print("• Smaller Var(X) → Less precise estimate of β₁")
    print("• If Var(X) = 0 (all X are the same), β₁ is undefined")

    print("\n5. RELATIONSHIP TO CORRELATION:")
    print("-" * 40)
    print("• β₁ = ρ * (σ_Y / σ_X) where ρ = correlation coefficient")
    print("• Correlation (ρ) measures strength of linear relationship")
    print("• Slope (β₁) measures magnitude of relationship")
    print("• Same ρ can give different β₁ depending on scales")

    print("\n6. STANDARDIZED SLOPE:")
    print("-" * 40)
    print("• If we standardize both X and Y (z-scores):")
    print("  • β₁_standardized = correlation coefficient ρ")
    print("  • In standardized form: Ŷ_z = ρ * X_z")
    print("  • This is why ρ is called the 'standardized slope'")

# Run the demonstration
if __name__ == "__main__":
    X, Y, beta_0, beta_1 = demonstrate_slope_derivation()
    visualize_covariance_variance_relationship(X, Y, beta_1)
    statistical_insights()

"""**1.2 The Y-Intercept (β0)**"""

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

def demonstrate_intercept_properties():
    """
    Show that the regression line always passes through (X̄, Ȳ)
    """
    np.random.seed(42)

    # Create three different datasets
    datasets = []

    # Dataset 1: Positive relationship
    X1 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    Y1 = 3 + 2 * X1 + np.random.normal(0, 2, len(X1))

    # Dataset 2: Negative relationship
    X2 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    Y2 = 20 - 1.5 * X2 + np.random.normal(0, 2, len(X2))

    # Dataset 3: No relationship (random)
    X3 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    Y3 = np.random.normal(10, 3, len(X3))

    datasets = [(X1, Y1, "Positive Relationship"),
                (X2, Y2, "Negative Relationship"),
                (X3, Y3, "No Relationship")]

    print("INTERCEPT PROPERTIES DEMONSTRATION")
    print("=" * 60)

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    for idx, (X, Y, title) in enumerate(datasets):
        # Calculate regression parameters
        X_mean, Y_mean = np.mean(X), np.mean(Y)

        # Calculate β₁ and β₀
        covariance = np.cov(X, Y, ddof=0)[0, 1]
        variance_x = np.var(X, ddof=0)
        beta_1 = covariance / variance_x
        beta_0 = Y_mean - beta_1 * X_mean

        # Plot the data
        ax = axes[idx]
        ax.scatter(X, Y, alpha=0.7, label='Data Points')

        # Plot the centroid
        ax.scatter(X_mean, Y_mean, color='red', s=200, marker='*',
                  label=f'Centroid ({X_mean:.1f}, {Y_mean:.1f})', zorder=5)

        # Plot regression line
        x_line = np.array([min(X) - 1, max(X) + 1])
        y_line = beta_0 + beta_1 * x_line
        ax.plot(x_line, y_line, 'black', linewidth=2, label='Regression Line')

        # Mark where X=0 on the regression line
        y_at_zero = beta_0  # When X=0, Ŷ = β₀
        if min(x_line) <= 0 <= max(x_line):
            ax.scatter(0, y_at_zero, color='green', s=150, marker='s',
                      label=f'Intercept (0, {y_at_zero:.1f})', zorder=5)

        # Add grid lines through centroid
        ax.axhline(Y_mean, color='red', linestyle='--', alpha=0.3)
        ax.axvline(X_mean, color='red', linestyle='--', alpha=0.3)

        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.set_title(f'{title}\nŶ = {beta_0:.2f} + {beta_1:.2f}X')
        ax.legend(loc='best')
        ax.grid(True, alpha=0.3)

        # Print calculations
        print(f"\n{title}:")
        print("-" * 40)
        print(f"X̄ = {X_mean:.2f}, Ȳ = {Y_mean:.2f}")
        print(f"β₁ = {beta_1:.4f}")
        print(f"β₀ = Ȳ - β₁X̄ = {Y_mean:.2f} - {beta_1:.2f}×{X_mean:.2f} = {beta_0:.2f}")
        print(f"Check: When X = X̄ = {X_mean:.2f}:")
        print(f"  Ŷ = β₀ + β₁X̄ = {beta_0:.2f} + {beta_1:.2f}×{X_mean:.2f} = {beta_0 + beta_1*X_mean:.2f}")
        print(f"  This equals Ȳ = {Y_mean:.2f} ✓")

        if min(x_line) <= 0 <= max(x_line):
            print(f"When X = 0: Ŷ = β₀ = {beta_0:.2f}")
        else:
            print(f"Note: X=0 is outside the plotted range")

    plt.tight_layout()
    plt.show()

    return datasets

def intercept_interpretation_scenarios():
    """
    Show different interpretations of the intercept
    """
    print("\n" + "=" * 60)
    print("INTERCEPT INTERPRETATION IN DIFFERENT SCENARIOS")
    print("=" * 60)

    scenarios = [
        {
            "name": "Meaningful X=0",
            "X_label": "Hours of Study",
            "Y_label": "Exam Score",
            "X_range": [0, 10],
            "beta_0": 40,
            "beta_1": 5,
            "interpretation": "A student who doesn't study (X=0) is expected to score 40 points."
        },
        {
            "name": "X=0 is Impossible",
            "X_label": "Age (years)",
            "Y_label": "Height (cm)",
            "X_range": [1, 18],
            "beta_0": 75,
            "beta_1": 6,
            "interpretation": "The intercept (75 cm at age 0) is not meaningful but ensures the line passes through the centroid."
        },
        {
            "name": "Negative Intercept",
            "X_label": "Temperature (°C)",
            "Y_label": "Ice Cream Sales ($)",
            "X_range": [15, 35],
            "beta_0": -200,
            "beta_1": 20,
            "interpretation": "Negative intercept doesn't mean negative sales at 0°C; it's a mathematical artifact to fit the line through the centroid."
        },
        {
            "name": "Intercept = 0 (No Intercept Model)",
            "X_label": "Production Hours",
            "Y_label": "Units Produced",
            "X_range": [0, 100],
            "beta_0": 0,
            "beta_1": 10,
            "interpretation": "Forced through origin: if no hours worked, no units produced."
        }
    ]

    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    axes = axes.flatten()

    for idx, scenario in enumerate(scenarios):
        ax = axes[idx]

        # Generate synthetic data based on the scenario
        X = np.linspace(scenario["X_range"][0], scenario["X_range"][1], 20)
        Y = scenario["beta_0"] + scenario["beta_1"] * X + np.random.normal(0, 10, len(X))

        # Recalculate actual regression to match the data
        X_mean, Y_mean = np.mean(X), np.mean(Y)
        beta_1 = scenario["beta_1"]  # Keep the given slope for illustration
        beta_0 = Y_mean - beta_1 * X_mean

        # Plot
        ax.scatter(X, Y, alpha=0.7)

        # Plot regression line
        x_line = np.array([min(X) - 2, max(X) + 2])
        y_line = beta_0 + beta_1 * x_line
        ax.plot(x_line, y_line, 'black', linewidth=2)

        # Plot centroid
        ax.scatter(X_mean, Y_mean, color='red', s=150, marker='*', zorder=5)

        # Plot intercept if in range
        if x_line[0] <= 0 <= x_line[1]:
            ax.scatter(0, beta_0, color='green', s=100, marker='s', zorder=5)
            ax.text(0, beta_0, f' (0, {beta_0:.1f})', va='bottom')

        # Formatting
        ax.set_xlabel(scenario["X_label"])
        ax.set_ylabel(scenario["Y_label"])
        ax.set_title(f'{scenario["name"]}\nβ₀ = {beta_0:.1f}, β₁ = {beta_1:.1f}')
        ax.grid(True, alpha=0.3)

        # Print interpretation
        print(f"\n{scenario['name']}:")
        print(f"  Regression: Ŷ = {beta_0:.1f} + {beta_1:.1f}X")
        print(f"  Interpretation: {scenario['interpretation']}")
        print(f"  Centroid: (X̄={X_mean:.1f}, Ȳ={Y_mean:.1f})")

    plt.tight_layout()
    plt.show()

def proof_centroid_property():
    """
    Mathematical proof that the regression line passes through (X̄, Ȳ)
    """
    print("\n" + "=" * 60)
    print("MATHEMATICAL PROOF: REGRESSION LINE PASSES THROUGH (X̄, Ȳ)")
    print("=" * 60)

    # Create a simple dataset
    X = np.array([1, 2, 3, 4, 5])
    Y = np.array([2, 4, 5, 4, 5])

    print("\nDataset:")
    print("X:", X)
    print("Y:", Y)

    # Calculate statistics
    n = len(X)
    X_sum = np.sum(X)
    Y_sum = np.sum(Y)
    XY_sum = np.sum(X * Y)
    X2_sum = np.sum(X ** 2)

    X_mean = X_sum / n
    Y_mean = Y_sum / n

    print(f"\nCalculations:")
    print(f"n = {n}")
    print(f"ΣX = {X_sum}")
    print(f"ΣY = {Y_sum}")
    print(f"ΣXY = {XY_sum}")
    print(f"ΣX² = {X2_sum}")
    print(f"X̄ = {X_sum}/{n} = {X_mean}")
    print(f"Ȳ = {Y_sum}/{n} = {Y_mean}")

    # Calculate β₁
    beta_1_num = n * XY_sum - X_sum * Y_sum
    beta_1_den = n * X2_sum - X_sum ** 2
    beta_1 = beta_1_num / beta_1_den

    print(f"\nβ₁ = (nΣXY - ΣXΣY) / (nΣX² - (ΣX)²)")
    print(f"   = ({n}×{XY_sum} - {X_sum}×{Y_sum}) / ({n}×{X2_sum} - {X_sum}²)")
    print(f"   = ({beta_1_num}) / ({beta_1_den})")
    print(f"   = {beta_1}")

    # Calculate β₀
    beta_0 = Y_mean - beta_1 * X_mean

    print(f"\nβ₀ = Ȳ - β₁X̄")
    print(f"   = {Y_mean} - {beta_1}×{X_mean}")
    print(f"   = {beta_0}")

    # Verify the centroid property
    print(f"\nVerification:")
    print(f"Regression equation: Ŷ = {beta_0} + {beta_1}X")
    print(f"Plug in X = X̄ = {X_mean}:")
    print(f"  Ŷ = {beta_0} + {beta_1}×{X_mean}")
    print(f"    = {beta_0} + {beta_1 * X_mean}")
    print(f"    = {beta_0 + beta_1 * X_mean}")
    print(f"  This equals Ȳ = {Y_mean} ✓")

    # Alternative proof using algebra
    print("\n" + "-" * 60)
    print("ALTERNATIVE PROOF USING NORMAL EQUATIONS:")
    print("-" * 60)

    print("\nNormal Equations for Simple Linear Regression:")
    print("1) ΣY = nβ₀ + β₁ΣX")
    print("2) ΣXY = β₀ΣX + β₁ΣX²")

    print(f"\nFrom equation 1:")
    print(f"ΣY = nβ₀ + β₁ΣX")
    print(f"{Y_sum} = {n}β₀ + β₁×{X_sum}")
    print(f"Divide both sides by n = {n}:")
    print(f"{Y_sum}/{n} = β₀ + β₁×({X_sum}/{n})")
    print(f"{Y_mean} = β₀ + β₁×{X_mean}")
    print(f"Ȳ = β₀ + β₁X̄")
    print("This shows (X̄, Ȳ) satisfies the regression equation!")

def geometric_interpretation():
    """
    Visualize the geometric meaning of the intercept
    """
    print("\n" + "=" * 60)
    print("GEOMETRIC INTERPRETATION OF THE INTERCEPT")
    print("=" * 60)

    fig, axes = plt.subplots(2, 2, figsize=(12, 10))

    # Example 1: Standard case
    X1 = np.array([2, 3, 4, 5, 6])
    Y1 = np.array([3, 5, 7, 9, 11])

    beta_1_1 = 2
    beta_0_1 = np.mean(Y1) - beta_1_1 * np.mean(X1)

    ax1 = axes[0, 0]
    ax1.scatter(X1, Y1)
    x_line1 = np.array([0, 7])
    ax1.plot(x_line1, beta_0_1 + beta_1_1 * x_line1, 'r-')
    ax1.axhline(0, color='k', linestyle='-', alpha=0.3)
    ax1.axvline(0, color='k', linestyle='-', alpha=0.3)
    ax1.scatter(0, beta_0_1, color='g', s=100, marker='s')
    ax1.scatter(np.mean(X1), np.mean(Y1), color='r', s=100, marker='*')
    ax1.set_title('Standard Case: β₀ is where line crosses Y-axis')
    ax1.grid(True, alpha=0.3)

    # Example 2: When X=0 is not in data range
    X2 = np.array([10, 12, 14, 16, 18])
    Y2 = 100 + 5 * X2 + np.random.normal(0, 5, len(X2))

    beta_1_2 = 5
    beta_0_2 = np.mean(Y2) - beta_1_2 * np.mean(X2)

    ax2 = axes[0, 1]
    ax2.scatter(X2, Y2)
    x_line2 = np.array([8, 20])
    ax2.plot(x_line2, beta_0_2 + beta_1_2 * x_line2, 'r-')
    ax2.axhline(0, color='k', linestyle='-', alpha=0.3)
    ax2.axvline(0, color='k', linestyle='-', alpha=0.3)
    # Extrapolate to X=0
    ax2.scatter(0, beta_0_2, color='g', s=100, marker='s')
    ax2.scatter(np.mean(X2), np.mean(Y2), color='r', s=100, marker='*')
    ax2.set_title('Extrapolation: β₀ is outside data range')
    ax2.grid(True, alpha=0.3)

    # Example 3: Negative intercept
    X3 = np.array([5, 6, 7, 8, 9])
    Y3 = -10 + 8 * X3 + np.random.normal(0, 2, len(X3))

    beta_1_3 = 8
    beta_0_3 = np.mean(Y3) - beta_1_3 * np.mean(X3)

    ax3 = axes[1, 0]
    ax3.scatter(X3, Y3)
    x_line3 = np.array([0, 10])
    ax3.plot(x_line3, beta_0_3 + beta_1_3 * x_line3, 'r-')
    ax3.axhline(0, color='k', linestyle='-', alpha=0.3)
    ax3.axvline(0, color='k', linestyle='-', alpha=0.3)
    ax3.scatter(0, beta_0_3, color='g', s=100, marker='s')
    ax3.scatter(np.mean(X3), np.mean(Y3), color='r', s=100, marker='*')
    ax3.set_title('Negative Intercept: Line crosses below origin')
    ax3.grid(True, alpha=0.3)

    # Example 4: Centroid visualization
    X4 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])
    Y4 = np.array([2, 3, 5, 4, 6, 7, 8, 9, 10])

    beta_1_4 = np.cov(X4, Y4, ddof=0)[0, 1] / np.var(X4, ddof=0)
    beta_0_4 = np.mean(Y4) - beta_1_4 * np.mean(X4)

    ax4 = axes[1, 1]
    ax4.scatter(X4, Y4)

    # Draw lines from centroid to show balance
    X_mean, Y_mean = np.mean(X4), np.mean(Y4)

    for x, y in zip(X4, Y4):
        # Residual line
        y_pred = beta_0_4 + beta_1_4 * x
        ax4.plot([x, x], [y, y_pred], 'gray', alpha=0.5, linestyle='--')

    x_line4 = np.array([0, 10])
    ax4.plot(x_line4, beta_0_4 + beta_1_4 * x_line4, 'r-', linewidth=2)
    ax4.scatter(X_mean, Y_mean, color='r', s=200, marker='*', zorder=5)

    # Show that residuals sum to zero (geometric balance)
    residuals = Y4 - (beta_0_4 + beta_1_4 * X4)
    print(f"\nGeometric Balance Check:")
    print(f"Sum of residuals: {np.sum(residuals):.6f} (should be ~0)")
    print(f"Sum of (residuals × X): {np.sum(residuals * X4):.6f} (should be ~0)")

    ax4.set_title('Centroid as Balance Point\n(Residuals Balance Around Line)')
    ax4.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

# Run all demonstrations
if __name__ == "__main__":
    print("INTERCEPT IN SIMPLE LINEAR REGRESSION")
    print("=" * 60)
    print("\nKey Concept: β₀ = Ȳ - β₁X̄ ensures the regression line")
    print("passes through the centroid (X̄, Ȳ) of the data.\n")

    demonstrate_intercept_properties()
    intercept_interpretation_scenarios()
    proof_centroid_property()
    geometric_interpretation()

"""**2. Correlation and Model Fit**

**2.1 Pearson Correlation Coefficient (r)**
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

def demonstrate_correlation():
    """
    Demonstrate the correlation coefficient calculation and interpretation
    """
    np.random.seed(42)

    print("PEARSON'S CORRELATION COEFFICIENT (r)")
    print("=" * 60)

    # Create examples with different correlation strengths
    examples = []

    # Example 1: Perfect positive correlation
    X1 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    Y1 = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20])
    examples.append((X1, Y1, "Perfect Positive (r = +1)"))

    # Example 2: Strong positive correlation
    X2 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    Y2 = X2 + np.random.normal(0, 1, len(X2))
    examples.append((X2, Y2, "Strong Positive"))

    # Example 3: Weak positive correlation
    X3 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    Y3 = np.random.normal(5, 2, len(X3)) + 0.3 * X3
    examples.append((X3, Y3, "Weak Positive"))

    # Example 4: No correlation
    X4 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    Y4 = np.random.normal(10, 3, len(X4))
    examples.append((X4, Y4, "No Correlation (r ≈ 0)"))

    # Example 5: Weak negative correlation
    X5 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    Y5 = 20 - 0.3 * X5 + np.random.normal(0, 2, len(X5))
    examples.append((X5, Y5, "Weak Negative"))

    # Example 6: Perfect negative correlation
    X6 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    Y6 = np.array([20, 18, 16, 14, 12, 10, 8, 6, 4, 2])
    examples.append((X6, Y6, "Perfect Negative (r = -1)"))

    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    axes = axes.flatten()

    for idx, (X, Y, title) in enumerate(examples):
        ax = axes[idx]

        # Calculate correlation manually
        X_mean = np.mean(X)
        Y_mean = np.mean(Y)

        # Calculate numerator: Σ(Xᵢ - X̄)(Yᵢ - Ȳ)
        numerator = np.sum((X - X_mean) * (Y - Y_mean))

        # Calculate denominator components
        sum_sq_x = np.sum((X - X_mean) ** 2)
        sum_sq_y = np.sum((Y - Y_mean) ** 2)

        # Calculate r manually
        if sum_sq_x > 0 and sum_sq_y > 0:
            r_manual = numerator / (np.sqrt(sum_sq_x) * np.sqrt(sum_sq_y))
        else:
            r_manual = 0

        # Compare with numpy's built-in function
        r_numpy = np.corrcoef(X, Y)[0, 1]

        # Plot the data
        ax.scatter(X, Y, alpha=0.7)

        # Add regression line if correlation exists
        if abs(r_manual) > 0:
            # Calculate regression line
            beta_1 = r_manual * (np.std(Y) / np.std(X))
            beta_0 = np.mean(Y) - beta_1 * np.mean(X)

            # Plot regression line
            x_line = np.array([min(X), max(X)])
            y_line = beta_0 + beta_1 * x_line
            ax.plot(x_line, y_line, 'r-', linewidth=2, alpha=0.7)

        # Formatting
        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.set_title(f'{title}\nr = {r_manual:.3f} (manual), {r_numpy:.3f} (numpy)')
        ax.grid(True, alpha=0.3)

        # Print calculations for first example
        if idx == 0:
            print("\nCALCULATION DETAILS (Perfect Positive Correlation):")
            print("-" * 50)
            print(f"X = {X}")
            print(f"Y = {Y}")
            print(f"X̄ = {X_mean:.1f}, Ȳ = {Y_mean:.1f}")
            print(f"\nDeviations from means:")
            for i in range(len(X)):
                x_dev = X[i] - X_mean
                y_dev = Y[i] - Y_mean
                product = x_dev * y_dev
                print(f"  Point {i+1}: (Xᵢ - X̄) = {x_dev:.1f}, (Yᵢ - Ȳ) = {y_dev:.1f}, product = {product:.1f}")

            print(f"\nΣ(Xᵢ - X̄)(Yᵢ - Ȳ) = {numerator:.1f}")
            print(f"Σ(Xᵢ - X̄)² = {sum_sq_x:.1f}")
            print(f"Σ(Yᵢ - Ȳ)² = {sum_sq_y:.1f}")
            print(f"√[Σ(Xᵢ - X̄)²] = {np.sqrt(sum_sq_x):.1f}")
            print(f"√[Σ(Yᵢ - Ȳ)²] = {np.sqrt(sum_sq_y):.1f}")
            print(f"\nr = {numerator:.1f} / ({np.sqrt(sum_sq_x):.1f} × {np.sqrt(sum_sq_y):.1f}) = {r_manual:.3f}")

    plt.tight_layout()
    plt.show()

    return examples

def correlation_properties():
    """
    Explain properties and limitations of correlation
    """
    print("\n" + "=" * 60)
    print("PROPERTIES OF PEARSON'S CORRELATION COEFFICIENT")
    print("=" * 60)

    print("\n1. RANGE: -1 ≤ r ≤ 1")
    print("   • r = 1: Perfect positive linear relationship")
    print("   • r = -1: Perfect negative linear relationship")
    print("   • r = 0: No linear relationship")

    print("\n2. INTERPRETATION OF MAGNITUDE:")
    print("   • |r| ≥ 0.9: Very strong correlation")
    print("   • 0.7 ≤ |r| < 0.9: Strong correlation")
    print("   • 0.5 ≤ |r| < 0.7: Moderate correlation")
    print("   • 0.3 ≤ |r| < 0.5: Weak correlation")
    print("   • |r| < 0.3: Very weak or no correlation")

    print("\n3. IMPORTANT PROPERTIES:")
    print("   • Symmetric: r(X,Y) = r(Y,X)")
    print("   • Unitless: Not affected by scaling of X or Y")
    print("   • Linear only: Measures LINEAR relationships")
    print("   • Sensitive to outliers")

    print("\n4. RELATIONSHIP TO REGRESSION SLOPE:")
    print("   • β₁ = r × (σᵧ/σₓ)")
    print("   • r = β₁ × (σₓ/σᵧ)")
    print("   • r² = Coefficient of determination in regression")

    print("\n5. COMMON MISCONCEPTIONS:")
    print("   • Correlation ≠ Causation")
    print("   • Correlation only measures linear relationships")
    print("   • r close to 0 doesn't mean no relationship (could be nonlinear)")
    print("   • r is sensitive to the range of data")

def correlation_vs_covariance():
    """
    Demonstrate the difference between correlation and covariance
    """
    print("\n" + "=" * 60)
    print("CORRELATION vs COVARIANCE")
    print("=" * 60)

    # Create datasets with different scales
    np.random.seed(42)

    # Original data
    X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    Y = 2 * X + np.random.normal(0, 1, len(X))

    # Scaled versions
    X_scaled = X * 10  # Multiply X by 10
    Y_scaled = Y * 100  # Multiply Y by 100

    # Calculate covariance and correlation for each
    datasets = [
        ("Original", X, Y),
        ("X scaled by 10", X_scaled, Y),
        ("Y scaled by 100", X, Y_scaled),
        ("Both scaled", X_scaled, Y_scaled)
    ]

    print("\nEffect of Scaling on Covariance and Correlation:")
    print("-" * 50)
    print(f"{'Dataset':<20} {'Cov(X,Y)':<15} {'Corr(X,Y)':<10}")
    print("-" * 50)

    for name, X_data, Y_data in datasets:
        cov = np.cov(X_data, Y_data, ddof=0)[0, 1]
        corr = np.corrcoef(X_data, Y_data)[0, 1]
        print(f"{name:<20} {cov:<15.2f} {corr:<10.4f}")

    print("\nKey Insight:")
    print("• Covariance changes with scaling of X or Y")
    print("• Correlation remains the same regardless of scaling")
    print("• Correlation is the 'standardized' covariance")

def correlation_limitations():
    """
    Show examples where correlation can be misleading
    """
    print("\n" + "=" * 60)
    print("LIMITATIONS OF CORRELATION")
    print("=" * 60)

    np.random.seed(42)

    fig, axes = plt.subplots(2, 3, figsize=(15, 10))

    # Example 1: Nonlinear relationship (quadratic)
    X1 = np.linspace(-5, 5, 100)
    Y1 = X1 ** 2 + np.random.normal(0, 1, len(X1))
    r1 = np.corrcoef(X1, Y1)[0, 1]

    axes[0, 0].scatter(X1, Y1, alpha=0.6)
    axes[0, 0].set_title(f'Quadratic Relationship\nr = {r1:.3f} (misleading!)')
    axes[0, 0].set_xlabel('X')
    axes[0, 0].set_ylabel('Y = X² + noise')
    axes[0, 0].grid(True, alpha=0.3)

    # Example 2: Outliers can inflate correlation
    X2 = np.random.normal(0, 1, 50)
    Y2 = X2 + np.random.normal(0, 0.5, 50)
    # Add an outlier
    X2 = np.append(X2, 10)
    Y2 = np.append(Y2, 10)
    r2_with_outlier = np.corrcoef(X2, Y2)[0, 1]

    # Without outlier
    X2_no_outlier = X2[:-1]
    Y2_no_outlier = Y2[:-1]
    r2_no_outlier = np.corrcoef(X2_no_outlier, Y2_no_outlier)[0, 1]

    axes[0, 1].scatter(X2[:-1], Y2[:-1], alpha=0.6, label='Normal points')
    axes[0, 1].scatter(X2[-1], Y2[-1], color='red', s=100, label='Outlier')
    axes[0, 1].set_title(f'Outlier Effect\nr = {r2_no_outlier:.3f} → {r2_with_outlier:.3f}')
    axes[0, 1].set_xlabel('X')
    axes[0, 1].set_ylabel('Y')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # Example 3: Subgroups with different relationships
    X3_a = np.random.normal(0, 1, 30)
    Y3_a = X3_a + np.random.normal(0, 0.3, 30)
    X3_b = np.random.normal(5, 1, 30)
    Y3_b = -X3_b + 10 + np.random.normal(0, 0.3, 30)

    X3 = np.concatenate([X3_a, X3_b])
    Y3 = np.concatenate([Y3_a, Y3_b])
    r3 = np.corrcoef(X3, Y3)[0, 1]

    axes[0, 2].scatter(X3_a, Y3_a, alpha=0.6, label='Group A (positive)')
    axes[0, 2].scatter(X3_b, Y3_b, alpha=0.6, label='Group B (negative)')
    axes[0, 2].set_title(f'Subgroup Paradox\nOverall r = {r3:.3f} (near 0)')
    axes[0, 2].set_xlabel('X')
    axes[0, 2].set_ylabel('Y')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)

    # Example 4: Restricted range
    X4_full = np.random.normal(0, 5, 100)
    Y4_full = 0.8 * X4_full + np.random.normal(0, 2, 100)
    r4_full = np.corrcoef(X4_full, Y4_full)[0, 1]

    # Restrict X range
    mask = (X4_full > -1) & (X4_full < 1)
    X4_restricted = X4_full[mask]
    Y4_restricted = Y4_full[mask]
    r4_restricted = np.corrcoef(X4_restricted, Y4_restricted)[0, 1]

    axes[1, 0].scatter(X4_full, Y4_full, alpha=0.3, label=f'Full range: r = {r4_full:.3f}')
    axes[1, 0].scatter(X4_restricted, Y4_restricted, color='red', alpha=0.7,
                      label=f'Restricted: r = {r4_restricted:.3f}')
    axes[1, 0].set_title('Restricted Range Effect')
    axes[1, 0].set_xlabel('X')
    axes[1, 0].set_ylabel('Y')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # Example 5: Spurious correlation (common cause)
    time = np.arange(2000, 2020)
    # Unrelated variables that both increase over time
    var1 = 100 + 5 * (time - 2000) + np.random.normal(0, 2, len(time))
    var2 = 50 + 8 * (time - 2000) + np.random.normal(0, 3, len(time))
    r5 = np.corrcoef(var1, var2)[0, 1]

    axes[1, 1].scatter(var1, var2, alpha=0.7)
    for i, year in enumerate(time):
        axes[1, 1].text(var1[i], var2[i], str(year), fontsize=8, alpha=0.7)
    axes[1, 1].set_title(f'Spurious Correlation over Time\nr = {r5:.3f}')
    axes[1, 1].set_xlabel('Variable 1 (e.g., Ice cream sales)')
    axes[1, 1].set_ylabel('Variable 2 (e.g., Shark attacks)')
    axes[1, 1].grid(True, alpha=0.3)

    # Example 6: Heteroscedasticity
    X6 = np.linspace(1, 10, 100)
    Y6 = 2 * X6 + np.random.normal(0, X6, len(X6))  # Noise increases with X
    r6 = np.corrcoef(X6, Y6)[0, 1]

    axes[1, 2].scatter(X6, Y6, alpha=0.6)
    axes[1, 2].set_title(f'Heteroscedastic Data\nr = {r6:.3f}')
    axes[1, 2].set_xlabel('X')
    axes[1, 2].set_ylabel('Y')
    axes[1, 2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def correlation_and_regression():
    """
    Show the relationship between correlation and regression
    """
    print("\n" + "=" * 60)
    print("RELATIONSHIP BETWEEN CORRELATION AND REGRESSION")
    print("=" * 60)

    np.random.seed(42)

    # Generate data with known correlation
    X = np.random.normal(0, 1, 100)
    Y = 0.7 * X + np.random.normal(0, 0.7, 100)

    # Calculate statistics
    r = np.corrcoef(X, Y)[0, 1]
    beta_1 = r * (np.std(Y) / np.std(X))
    beta_0 = np.mean(Y) - beta_1 * np.mean(X)

    # Calculate R-squared
    Y_pred = beta_0 + beta_1 * X
    SSR = np.sum((Y - Y_pred) ** 2)
    SST = np.sum((Y - np.mean(Y)) ** 2)
    R_squared = 1 - (SSR / SST)

    print(f"\nData Statistics:")
    print(f"• Correlation (r): {r:.4f}")
    print(f"• R-squared (r²): {r**2:.4f}")
    print(f"• Regression R-squared: {R_squared:.4f}")
    print(f"• Slope (β₁): {beta_1:.4f}")
    print(f"• Intercept (β₀): {beta_0:.4f}")

    print(f"\nVerification:")
    print(f"r² = {r**2:.4f} = R-squared = {R_squared:.4f} ✓")
    print(f"β₁ = r × (σᵧ/σₓ) = {r:.4f} × ({np.std(Y):.4f}/{np.std(X):.4f}) = {beta_1:.4f} ✓")

    # Visualization
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))

    # Plot 1: Data with regression line
    axes[0].scatter(X, Y, alpha=0.6)
    x_line = np.array([min(X), max(X)])
    y_line = beta_0 + beta_1 * x_line
    axes[0].plot(x_line, y_line, 'r-', linewidth=2, label=f'Ŷ = {beta_0:.2f} + {beta_1:.2f}X')
    axes[0].set_xlabel('X')
    axes[0].set_ylabel('Y')
    axes[0].set_title(f'Linear Regression\nr = {r:.3f}, R² = {R_squared:.3f}')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # Plot 2: Explained vs Unexplained variance
    explained_variance = R_squared * 100
    unexplained_variance = (1 - R_squared) * 100

    labels = ['Explained by X', 'Unexplained (Error)']
    sizes = [explained_variance, unexplained_variance]
    colors = ['lightgreen', 'lightcoral']

    axes[1].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
    axes[1].set_title(f'Variance Decomposition\n(R² = {R_squared:.3f})')

    plt.tight_layout()
    plt.show()

    print(f"\nInterpretation:")
    print(f"• {explained_variance:.1f}% of variance in Y is explained by X")
    print(f"• {unexplained_variance:.1f}% is unexplained (due to other factors or random error)")

# Run all demonstrations
if __name__ == "__main__":
    print("PEARSON'S CORRELATION COEFFICIENT (r)")
    print("=" * 60)
    print("\nFormula: r = Σ(Xᵢ - X̄)(Yᵢ - Ȳ) / [√Σ(Xᵢ - X̄)² × √Σ(Yᵢ - Ȳ)²]")
    print("Range: -1 ≤ r ≤ 1")

    examples = demonstrate_correlation()
    correlation_properties()
    correlation_vs_covariance()
    correlation_limitations()
    correlation_and_regression()

"""**2.2 Coefficient of Determination (R2)**"""

import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

def demonstrate_r_squared():
    """
    Comprehensive demonstration of R² calculation and interpretation
    """
    np.random.seed(42)

    print("R-SQUARED (COEFFICIENT OF DETERMINATION)")
    print("=" * 60)
    print("Key Concept: R² = r² = Proportion of variance in Y explained by X")
    print("=" * 60)

    # Create datasets with different R² values
    datasets = []

    # Dataset 1: High R² (strong linear relationship)
    X1 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    Y1 = 50 + 5 * X1 + np.random.normal(0, 3, len(X1))
    datasets.append((X1, Y1, "High R² (Strong Linear Relationship)"))

    # Dataset 2: Medium R²
    X2 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    Y2 = 50 + 5 * X2 + np.random.normal(0, 10, len(X2))
    datasets.append((X2, Y2, "Medium R²"))

    # Dataset 3: Low R² (weak relationship)
    X3 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    Y3 = 50 + np.random.normal(0, 15, len(X3))
    datasets.append((X3, Y3, "Low R² (Weak Relationship)"))

    # Dataset 4: Perfect R² = 1
    X4 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    Y4 = 50 + 5 * X4
    datasets.append((X4, Y4, "Perfect Fit (R² = 1)"))

    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    axes = axes.flatten()

    for idx, (X, Y, title) in enumerate(datasets):
        ax = axes[idx]

        # Calculate regression parameters
        n = len(X)
        X_mean = np.mean(X)
        Y_mean = np.mean(Y)

        # Calculate r manually
        numerator = np.sum((X - X_mean) * (Y - Y_mean))
        denominator = np.sqrt(np.sum((X - X_mean)**2) * np.sum((Y - Y_mean)**2))
        r = numerator / denominator if denominator != 0 else 0

        # Calculate regression coefficients
        beta_1 = r * (np.std(Y, ddof=1) / np.std(X, ddof=1)) if np.std(X, ddof=1) != 0 else 0
        beta_0 = Y_mean - beta_1 * X_mean

        # Calculate predicted values
        Y_pred = beta_0 + beta_1 * X

        # Calculate R² using three different methods
        # Method 1: r²
        R2_method1 = r**2

        # Method 2: 1 - SSR/SST
        SSR = np.sum((Y - Y_pred)**2)  # Sum of Squared Residuals
        SST = np.sum((Y - Y_mean)**2)  # Total Sum of Squares
        SSE = np.sum((Y_pred - Y_mean)**2)  # Explained Sum of Squares
        R2_method2 = 1 - (SSR / SST) if SST != 0 else 0

        # Method 3: SSE/SST
        R2_method3 = SSE / SST if SST != 0 else 0

        # Plot data and regression line
        ax.scatter(X, Y, alpha=0.7, label='Actual Data')

        # Plot regression line
        x_line = np.linspace(min(X) - 1, max(X) + 1, 100)
        y_line = beta_0 + beta_1 * x_line
        ax.plot(x_line, y_line, 'r-', linewidth=2, label='Regression Line')

        # Plot residuals
        for xi, yi, ypi in zip(X, Y, Y_pred):
            ax.plot([xi, xi], [yi, ypi], 'gray', linestyle='--', alpha=0.5)

        # Plot mean line
        ax.axhline(Y_mean, color='green', linestyle='--', alpha=0.7, label=f'Mean (Ȳ = {Y_mean:.1f})')

        ax.set_xlabel('X')
        ax.set_ylabel('Y')
        ax.set_title(f'{title}\nR² = {R2_method1:.3f}')
        ax.legend(loc='best')
        ax.grid(True, alpha=0.3)

        # Print detailed calculations for first dataset
        if idx == 0:
            print(f"\nDETAILED CALCULATIONS FOR {title}:")
            print("-" * 50)
            print(f"Data points: X = {X}, Y = {np.round(Y, 2)}")
            print(f"\nStep 1: Calculate means:")
            print(f"  X̄ = {X_mean:.2f}, Ȳ = {Y_mean:.2f}")

            print(f"\nStep 2: Calculate correlation (r):")
            print(f"  Numerator: Σ(Xᵢ - X̄)(Yᵢ - Ȳ) = {numerator:.2f}")
            print(f"  Denominator: √[Σ(Xᵢ - X̄)² × Σ(Yᵢ - Ȳ)²] = {denominator:.2f}")
            print(f"  r = {numerator:.2f} / {denominator:.2f} = {r:.4f}")

            print(f"\nStep 3: Calculate regression coefficients:")
            print(f"  β₁ = r × (σᵧ/σₓ) = {r:.4f} × ({np.std(Y, ddof=1):.2f}/{np.std(X, ddof=1):.2f}) = {beta_1:.4f}")
            print(f"  β₀ = Ȳ - β₁X̄ = {Y_mean:.2f} - {beta_1:.4f} × {X_mean:.2f} = {beta_0:.4f}")

            print(f"\nStep 4: Calculate predicted values (Ŷ):")
            for i in range(n):
                print(f"  Point {i+1}: Ŷ = {beta_0:.2f} + {beta_1:.2f} × {X[i]} = {Y_pred[i]:.2f}")

            print(f"\nStep 5: Calculate Sum of Squares:")
            print(f"  SSR (Residual) = Σ(Y - Ŷ)² = {SSR:.2f}")
            print(f"  SST (Total) = Σ(Y - Ȳ)² = {SST:.2f}")
            print(f"  SSE (Explained) = Σ(Ŷ - Ȳ)² = {SSE:.2f}")
            print(f"  Check: SST = SSR + SSE? {SST:.2f} ≈ {SSR + SSE:.2f} = {SSR + SSE:.2f} ✓")

            print(f"\nStep 6: Calculate R² using different methods:")
            print(f"  Method 1 (r²): R² = ({r:.4f})² = {R2_method1:.4f}")
            print(f"  Method 2 (1 - SSR/SST): R² = 1 - ({SSR:.2f}/{SST:.2f}) = {R2_method2:.4f}")
            print(f"  Method 3 (SSE/SST): R² = {SSE:.2f}/{SST:.2f} = {R2_method3:.4f}")
            print(f"  All methods give: R² = {R2_method1:.4f} ✓")

            print(f"\nInterpretation: {R2_method1*100:.1f}% of variance in Y is explained by X")

    plt.tight_layout()
    plt.show()

    return datasets

def variance_decomposition_visualization():
    """
    Visual breakdown of total variance into explained and unexplained components
    """
    np.random.seed(42)

    # Create a clear example dataset
    X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    Y = 50 + 3 * X + np.random.normal(0, 5, len(X))

    # Calculate regression
    X_mean, Y_mean = np.mean(X), np.mean(Y)
    beta_1 = np.sum((X - X_mean) * (Y - Y_mean)) / np.sum((X - X_mean)**2)
    beta_0 = Y_mean - beta_1 * X_mean
    Y_pred = beta_0 + beta_1 * X

    # Calculate sums of squares
    SST = np.sum((Y - Y_mean)**2)  # Total variation
    SSR = np.sum((Y - Y_pred)**2)  # Unexplained variation
    SSE = np.sum((Y_pred - Y_mean)**2)  # Explained variation
    R2 = 1 - (SSR / SST)

    print(f"\n" + "=" * 60)
    print("VARIANCE DECOMPOSITION VISUALIZATION")
    print("=" * 60)

    fig, axes = plt.subplots(2, 3, figsize=(15, 10))

    # Plot 1: Data with regression line and residuals
    axes[0, 0].scatter(X, Y, alpha=0.7, label='Actual Data')
    x_line = np.linspace(min(X)-1, max(X)+1, 100)
    y_line = beta_0 + beta_1 * x_line
    axes[0, 0].plot(x_line, y_line, 'r-', linewidth=2, label='Regression Line')
    axes[0, 0].axhline(Y_mean, color='green', linestyle='--', alpha=0.7, label='Mean of Y')

    # Draw residuals
    for xi, yi, ypi in zip(X, Y, Y_pred):
        axes[0, 0].plot([xi, xi], [yi, ypi], 'gray', linestyle='--', alpha=0.5)

    axes[0, 0].set_xlabel('X')
    axes[0, 0].set_ylabel('Y')
    axes[0, 0].set_title(f'Regression Line with Residuals\nR² = {R2:.3f}')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # Plot 2: Total variance (SST) - deviations from mean
    axes[0, 1].scatter(X, Y, alpha=0.7)
    axes[0, 1].axhline(Y_mean, color='green', linestyle='-', alpha=0.7, linewidth=2)

    # Draw lines from each point to mean line
    for xi, yi in zip(X, Y):
        axes[0, 1].plot([xi, xi], [yi, Y_mean], 'blue', linestyle='--', alpha=0.5)

    axes[0, 1].set_xlabel('X')
    axes[0, 1].set_ylabel('Y')
    axes[0, 1].set_title(f'Total Variance (SST = {SST:.1f})\nDeviations from Mean')
    axes[0, 1].grid(True, alpha=0.3)

    # Plot 3: Explained variance (SSE) - deviations of predictions from mean
    axes[0, 2].scatter(X, Y, alpha=0.3, color='gray', label='Actual Data')
    axes[0, 2].scatter(X, Y_pred, alpha=0.7, color='red', label='Predictions')
    axes[0, 2].axhline(Y_mean, color='green', linestyle='-', alpha=0.7, linewidth=2)

    # Draw lines from predictions to mean
    for xi, ypi in zip(X, Y_pred):
        axes[0, 2].plot([xi, xi], [ypi, Y_mean], 'red', linestyle='--', alpha=0.5)

    axes[0, 2].set_xlabel('X')
    axes[0, 2].set_ylabel('Y')
    axes[0, 2].set_title(f'Explained Variance (SSE = {SSE:.1f})\nPredictions vs Mean')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)

    # Plot 4: Unexplained variance (SSR) - residuals
    axes[1, 0].scatter(X, Y, alpha=0.3, color='gray', label='Actual Data')
    axes[1, 0].scatter(X, Y_pred, alpha=0.7, color='red', label='Predictions')

    # Draw residuals
    for xi, yi, ypi in zip(X, Y, Y_pred):
        axes[1, 0].plot([xi, xi], [yi, ypi], 'blue', linestyle='--', alpha=0.5)

    axes[1, 0].set_xlabel('X')
    axes[1, 0].set_ylabel('Y')
    axes[1, 0].set_title(f'Unexplained Variance (SSR = {SSR:.1f})\nResiduals')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # Plot 5: Variance decomposition pie chart
    explained_pct = (SSE / SST) * 100
    unexplained_pct = (SSR / SST) * 100

    labels = [f'Explained by X\n{explained_pct:.1f}%',
              f'Unexplained\n{unexplained_pct:.1f}%']
    sizes = [explained_pct, unexplained_pct]
    colors = ['lightgreen', 'lightcoral']

    axes[1, 1].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%',
                   startangle=90, explode=(0.1, 0))
    axes[1, 1].set_title(f'Variance Decomposition\nR² = {R2:.3f}')

    # Plot 6: R² interpretation visualization
    # Create a bar chart showing variance components
    components = ['Total Variance (SST)', 'Explained (SSE)', 'Unexplained (SSR)']
    values = [SST, SSE, SSR]
    colors_bar = ['gray', 'green', 'red']

    bars = axes[1, 2].bar(components, values, color=colors_bar, alpha=0.7)
    axes[1, 2].set_ylabel('Sum of Squares')
    axes[1, 2].set_title('Variance Components Comparison')
    axes[1, 2].grid(True, alpha=0.3, axis='y')

    # Add value labels on bars
    for bar, value in zip(bars, values):
        height = bar.get_height()
        axes[1, 2].text(bar.get_x() + bar.get_width()/2., height,
                       f'{value:.1f}', ha='center', va='bottom')

    plt.tight_layout()
    plt.show()

    print(f"\nVariance Decomposition Summary:")
    print(f"Total Variance (SST): {SST:.2f}")
    print(f"Explained Variance (SSE): {SSE:.2f} ({explained_pct:.1f}%)")
    print(f"Unexplained Variance (SSR): {SSR:.2f} ({unexplained_pct:.1f}%)")
    print(f"R² = SSE/SST = {SSE:.2f}/{SST:.2f} = {R2:.4f}")
    print(f"Interpretation: {explained_pct:.1f}% of variance in Y is explained by X")

def r_squared_properties_and_limitations():
    """
    Explain properties, interpretation, and limitations of R²
    """
    print("\n" + "=" * 60)
    print("PROPERTIES AND LIMITATIONS OF R²")
    print("=" * 60)

    print("\n1. KEY PROPERTIES:")
    print("-" * 40)
    print("• Range: 0 ≤ R² ≤ 1")
    print("• R² = 1: Perfect fit (all points on regression line)")
    print("• R² = 0: No linear relationship (regression line = mean line)")
    print("• R² = r² in simple linear regression")
    print("• R² never decreases when adding more variables (in multiple regression)")

    print("\n2. INTERPRETATION GUIDE:")
    print("-" * 40)
    print("• R² > 0.9: Excellent fit")
    print("• 0.7 ≤ R² < 0.9: Good fit")
    print("• 0.5 ≤ R² < 0.7: Moderate fit")
    print("• 0.3 ≤ R² < 0.5: Weak fit")
    print("• R² < 0.3: Poor fit (but context matters!)")

    print("\n3. WHAT R² MEASURES:")
    print("-" * 40)
    print("✓ Proportion of variance in Y explained by X")
    print("✓ How well the regression line fits the data")
    print("✓ Strength of linear relationship")

    print("\n4. WHAT R² DOES NOT MEASURE:")
    print("-" * 40)
    print("✗ Causation (X causing Y)")
    print("✗ Whether the model is correct (only if linear)")
    print("✗ Whether coefficients are statistically significant")
    print("✗ Whether predictors are relevant (in multiple regression)")
    print("✗ Magnitude of effect (a small effect can have high R²)")

    print("\n5. COMMON MISCONCEPTIONS:")
    print("-" * 40)
    print("Myth: 'High R² means a good model'")
    print("  Truth: R² can be high with incorrect model specification")

    print("\nMyth: 'Low R² means the model is useless'")
    print("  Truth: Even low R² can be meaningful in some fields")
    print("  (e.g., social sciences often have R² < 0.3)")

    print("\nMyth: 'R² close to 1 is always desirable'")
    print("  Truth: R² = 1 often indicates overfitting or data issues")

    print("\n6. LIMITATIONS AND PITFALLS:")
    print("-" * 40)

    # Create visualization for limitations
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))

    # Example 1: R² can be inflated by outliers
    np.random.seed(42)
    X1 = np.random.normal(0, 1, 30)
    Y1 = 2 * X1 + np.random.normal(0, 0.5, 30)
    # Add an outlier
    X1 = np.append(X1, 5)
    Y1 = np.append(Y1, 5)

    # Calculate R² with and without outlier
    r_with = np.corrcoef(X1, Y1)[0, 1]**2
    r_without = np.corrcoef(X1[:-1], Y1[:-1])[0, 1]**2

    axes[0, 0].scatter(X1[:-1], Y1[:-1], alpha=0.6, label='Normal points')
    axes[0, 0].scatter(X1[-1], Y1[-1], color='red', s=100, label='Outlier')

    # Add regression lines
    for X_data, Y_data, label, color in [(X1, Y1, f'With outlier (R²={r_with:.3f})', 'red'),
                                         (X1[:-1], Y1[:-1], f'Without outlier (R²={r_without:.3f})', 'blue')]:
        beta_1 = np.cov(X_data, Y_data, ddof=1)[0, 1] / np.var(X_data, ddof=1)
        beta_0 = np.mean(Y_data) - beta_1 * np.mean(X_data)
        x_line = np.array([min(X_data), max(X_data)])
        y_line = beta_0 + beta_1 * x_line
        axes[0, 0].plot(x_line, y_line, color=color, alpha=0.7, label=label)

    axes[0, 0].set_title('Outliers Can Inflate R²')
    axes[0, 0].set_xlabel('X')
    axes[0, 0].set_ylabel('Y')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # Example 2: Same R², different relationships
    X2 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    # Two different datasets with same R²
    Y2a = 5 + 2 * X2 + np.random.normal(0, 2, len(X2))
    Y2b = 15 + 0.5 * X2 + np.random.normal(0, 0.5, len(X2))

    r2a = np.corrcoef(X2, Y2a)[0, 1]**2
    r2b = np.corrcoef(X2, Y2b)[0, 1]**2

    axes[0, 1].scatter(X2, Y2a, alpha=0.6, label=f'Dataset A (R²={r2a:.3f})')
    axes[0, 1].scatter(X2, Y2b, alpha=0.6, label=f'Dataset B (R²={r2b:.3f})')

    # Add regression lines
    for Y_data, color in [(Y2a, 'blue'), (Y2b, 'red')]:
        beta_1 = np.cov(X2, Y_data, ddof=1)[0, 1] / np.var(X2, ddof=1)
        beta_0 = np.mean(Y_data) - beta_1 * np.mean(X2)
        y_line = beta_0 + beta_1 * X2
        axes[0, 1].plot(X2, y_line, color=color, alpha=0.7)

    axes[0, 1].set_title('Same R², Different Relationships')
    axes[0, 1].set_xlabel('X')
    axes[0, 1].set_ylabel('Y')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # Example 3: R² depends on data range
    X3_full = np.random.uniform(0, 10, 100)
    Y3_full = 2 * X3_full + np.random.normal(0, 3, 100)
    r_full = np.corrcoef(X3_full, Y3_full)[0, 1]**2

    # Restricted range
    mask = (X3_full > 4) & (X3_full < 6)
    X3_restricted = X3_full[mask]
    Y3_restricted = Y3_full[mask]
    r_restricted = np.corrcoef(X3_restricted, Y3_restricted)[0, 1]**2

    axes[1, 0].scatter(X3_full, Y3_full, alpha=0.3, label=f'Full range (R²={r_full:.3f})')
    axes[1, 0].scatter(X3_restricted, Y3_restricted, color='red', alpha=0.7,
                      label=f'Restricted range (R²={r_restricted:.3f})')
    axes[1, 0].set_title('R² Depends on Data Range')
    axes[1, 0].set_xlabel('X')
    axes[1, 0].set_ylabel('Y')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # Example 4: Non-linear relationship with moderate R²
    X4 = np.linspace(-5, 5, 100)
    Y4 = X4**2 + np.random.normal(0, 2, 100)
    r_nonlinear = np.corrcoef(X4, Y4)[0, 1]**2

    axes[1, 1].scatter(X4, Y4, alpha=0.6)

    # Add linear regression line
    beta_1 = np.cov(X4, Y4, ddof=1)[0, 1] / np.var(X4, ddof=1)
    beta_0 = np.mean(Y4) - beta_1 * np.mean(X4)
    y_line = beta_0 + beta_1 * X4
    axes[1, 1].plot(X4, y_line, 'r-', alpha=0.7, label=f'Linear fit (R²={r_nonlinear:.3f})')

    # Add quadratic fit for comparison
    coeffs = np.polyfit(X4, Y4, 2)
    y_quad = coeffs[0]*X4**2 + coeffs[1]*X4 + coeffs[2]
    ssr_quad = np.sum((Y4 - y_quad)**2)
    sst_quad = np.sum((Y4 - np.mean(Y4))**2)
    r2_quad = 1 - (ssr_quad / sst_quad)

    axes[1, 1].plot(X4, y_quad, 'g--', alpha=0.7, label=f'Quadratic fit (R²={r2_quad:.3f})')

    axes[1, 1].set_title('Non-linear Relationship\nLinear R² misleading')
    axes[1, 1].set_xlabel('X')
    axes[1, 1].set_ylabel('Y')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

    print("\n7. ADJUSTED R² (FOR MULTIPLE REGRESSION):")
    print("-" * 40)
    print("• Problem: R² always increases with more variables")
    print("• Solution: Adjusted R² penalizes adding useless variables")
    print("• Formula: Adj-R² = 1 - [(1-R²)(n-1)/(n-p-1)]")
    print("  where n = sample size, p = number of predictors")
    print("• Adj-R² ≤ R², and can be negative")

def practical_r_squared_examples():
    """
    Real-world examples of R² interpretation
    """
    print("\n" + "=" * 60)
    print("PRACTICAL R² INTERPRETATION EXAMPLES")
    print("=" * 60)

    examples = [
        {
            "context": "Physics Experiment",
            "relationship": "Distance fallen vs. time²",
            "typical_R2": "0.99+",
            "interpretation": "Almost perfect fit - physics laws are precise"
        },
        {
            "context": "Economics",
            "relationship": "GDP growth vs. investment rate",
            "typical_R2": "0.6-0.8",
            "interpretation": "Good fit - investment explains most economic growth"
        },
        {
            "context": "Psychology",
            "relationship": "Test anxiety vs. exam performance",
            "typical_R2": "0.1-0.3",
            "interpretation": "Weak fit - many other factors affect performance"
        },
        {
            "context": "Medicine",
            "relationship": "Cholesterol level vs. heart disease risk",
            "typical_R2": "0.2-0.4",
            "interpretation": "Moderate fit - important but not sole factor"
        },
        {
            "context": "Marketing",
            "relationship": "Ad spending vs. sales",
            "typical_R2": "0.4-0.7",
            "interpretation": "Reasonable fit - ads matter but other factors too"
        }
    ]

    print(f"\n{'Context':<20} {'Relationship':<30} {'Typical R²':<15} {'Interpretation':<40}")
    print("-" * 105)
    for ex in examples:
        print(f"{ex['context']:<20} {ex['relationship']:<30} {ex['typical_R2']:<15} {ex['interpretation']:<40}")

# Run all demonstrations
if __name__ == "__main__":
    print("R-SQUARED: COEFFICIENT OF DETERMINATION")
    print("=" * 60)
    print("Three equivalent definitions:")
    print("1. R² = r² (square of correlation coefficient)")
    print("2. R² = 1 - (SSR/SST)")
    print("3. R² = SSE/SST")

    datasets = demonstrate_r_squared()
    variance_decomposition_visualization()
    r_squared_properties_and_limitations()
    practical_r_squared_examples()

"""**C. Task 1: Data Selection and Initial Visualization**"""

if __name__ == "__main__":
    print("R-SQUARED: COEFFICIENT OF DETERMINATION")
    print("=" * 60)
    print("Three equivalent definitions:")
    print("1. R² = r² (square of correlation coefficient)")
    print("2. R² = 1 - (SSR/SST)")
    print("3. R² = SSE/SST")

    datasets = demonstrate_r_squared()
    variance_decomposition_visualization()
    r_squared_properties_and_limitations()
    practical_r_squared_examples()

"""**D. Task 2: Manual Calculation of Regression Parameters**"""

import numpy as np
import matplotlib.pyplot as plt

def manual_simple_linear_regression(X, Y):
    """
    Perform simple linear regression manually without using built-in regression functions

    Parameters:
    -----------
    X : array-like, independent variable
    Y : array-like, dependent variable

    Returns:
    --------
    dict containing all calculated components and results
    """

    # Convert to numpy arrays for easier computation
    X = np.array(X, dtype=float)
    Y = np.array(Y, dtype=float)

    # Step 0: Basic checks
    if len(X) != len(Y):
        raise ValueError("X and Y must have the same length")

    n = len(X)

    print("=" * 70)
    print("MANUAL SIMPLE LINEAR REGRESSION CALCULATION")
    print("=" * 70)
    print(f"Sample size: n = {n}")
    print(f"X values: {X}")
    print(f"Y values: {Y}")
    print()

    # -----------------------------------------------------------------
    # D.1: Calculate Intermediate Values (Step-by-step)
    # -----------------------------------------------------------------

    print("D.1: CALCULATE INTERMEDIATE VALUES")
    print("-" * 40)

    # Calculate means
    X_mean = np.sum(X) / n
    Y_mean = np.sum(Y) / n
    print(f"1. Calculate means:")
    print(f"   X̄ = ΣX / n = {np.sum(X)} / {n} = {X_mean:.4f}")
    print(f"   Ȳ = ΣY / n = {np.sum(Y)} / {n} = {Y_mean:.4f}")
    print()

    # Calculate deviations from means
    X_dev = X - X_mean
    Y_dev = Y - Y_mean
    print(f"2. Calculate deviations from means:")
    print(f"   Xᵢ - X̄: {X_dev.round(4)}")
    print(f"   Yᵢ - Ȳ: {Y_dev.round(4)}")
    print()

    # Calculate products of deviations (for numerator)
    products = X_dev * Y_dev
    print(f"3. Calculate products (Xᵢ - X̄)(Yᵢ - Ȳ):")
    for i in range(n):
        print(f"   Point {i+1}: ({X_dev[i]:.4f}) × ({Y_dev[i]:.4f}) = {products[i]:.4f}")

    # Calculate numerator: Σ(Xᵢ - X̄)(Yᵢ - Ȳ)
    numerator = np.sum(products)
    print(f"   Sum of products (Numerator of β₁): Σ = {numerator:.4f}")
    print()

    # Calculate squared deviations of X (for denominator)
    X_dev_squared = X_dev ** 2
    print(f"4. Calculate squared deviations of X (Xᵢ - X̄)²:")
    for i in range(n):
        print(f"   Point {i+1}: ({X_dev[i]:.4f})² = {X_dev_squared[i]:.4f}")

    # Calculate denominator: Σ(Xᵢ - X̄)²
    denominator = np.sum(X_dev_squared)
    print(f"   Sum of squared deviations (Denominator of β₁): Σ = {denominator:.4f}")
    print()

    # Also calculate squared deviations of Y for R² calculation
    Y_dev_squared = Y_dev ** 2
    sum_Y_dev_squared = np.sum(Y_dev_squared)

    # -----------------------------------------------------------------
    # D.2: Compute Slope (β₁)
    # -----------------------------------------------------------------

    print("D.2: COMPUTE SLOPE (β₁)")
    print("-" * 40)
    print(f"β₁ = [Σ(Xᵢ - X̄)(Yᵢ - Ȳ)] / [Σ(Xᵢ - X̄)²]")
    print(f"   = {numerator:.4f} / {denominator:.4f}")

    if denominator == 0:
        raise ValueError("Denominator is zero. All X values are identical.")

    beta_1 = numerator / denominator
    print(f"   = {beta_1:.6f}")
    print(f"\nInterpretation: For each 1-unit increase in X, Y is expected to")
    print(f"change by {beta_1:.4f} units.")
    print()

    # -----------------------------------------------------------------
    # D.3: Compute Intercept (β₀)
    # -----------------------------------------------------------------

    print("D.3: COMPUTE INTERCEPT (β₀)")
    print("-" * 40)
    print(f"β₀ = Ȳ - β₁X̄")
    print(f"   = {Y_mean:.4f} - ({beta_1:.6f} × {X_mean:.4f})")
    print(f"   = {Y_mean:.4f} - {beta_1 * X_mean:.6f}")

    beta_0 = Y_mean - beta_1 * X_mean
    print(f"   = {beta_0:.6f}")
    print(f"\nInterpretation: When X = 0, the predicted value of Y is {beta_0:.4f}.")
    print()

    # -----------------------------------------------------------------
    # D.4: Formulate the Model
    # -----------------------------------------------------------------

    print("D.4: FORMULATE THE MODEL")
    print("-" * 40)
    print(f"Complete estimated regression equation:")
    print(f"Ŷ = β₀ + β₁X")
    print(f"Ŷ = {beta_0:.6f} + {beta_1:.6f}X")
    print()

    # -----------------------------------------------------------------
    # Additional Calculations: R-squared and Predictions
    # -----------------------------------------------------------------

    print("ADDITIONAL CALCULATIONS")
    print("-" * 40)

    # Calculate predicted values
    Y_pred = beta_0 + beta_1 * X
    print(f"1. Predicted values (Ŷ):")
    for i in range(n):
        print(f"   Point {i+1}: Ŷ = {beta_0:.4f} + {beta_1:.4f} × {X[i]} = {Y_pred[i]:.4f}")
    print()

    # Calculate residuals (errors)
    residuals = Y - Y_pred
    print(f"2. Residuals (Y - Ŷ):")
    for i in range(n):
        print(f"   Point {i+1}: {Y[i]:.4f} - {Y_pred[i]:.4f} = {residuals[i]:.4f}")
    print()

    # Calculate Sum of Squared Residuals (SSR)
    SSR = np.sum(residuals ** 2)
    print(f"3. Sum of Squared Residuals (SSR):")
    print(f"   SSR = Σ(Yᵢ - Ŷᵢ)² = {SSR:.6f}")

    # Calculate Total Sum of Squares (SST)
    SST = sum_Y_dev_squared
    print(f"4. Total Sum of Squares (SST):")
    print(f"   SST = Σ(Yᵢ - Ȳ)² = {SST:.6f}")

    # Calculate R-squared
    R_squared = 1 - (SSR / SST) if SST != 0 else 0
    print(f"5. R-squared (Coefficient of Determination):")
    print(f"   R² = 1 - (SSR / SST) = 1 - ({SSR:.6f} / {SST:.6f}) = {R_squared:.6f}")
    print(f"   Interpretation: {R_squared*100:.2f}% of variance in Y is explained by X")
    print()

    # Calculate correlation (for verification)
    correlation = numerator / (np.sqrt(denominator) * np.sqrt(SST))
    print(f"6. Correlation coefficient (r):")
    print(f"   r = Σ(Xᵢ - X̄)(Yᵢ - Ȳ) / [√Σ(Xᵢ - X̄)² × √Σ(Yᵢ - Ȳ)²]")
    print(f"   r = {numerator:.6f} / (√{denominator:.6f} × √{SST:.6f})")
    print(f"   r = {numerator:.6f} / ({np.sqrt(denominator):.6f} × {np.sqrt(SST):.6f})")
    print(f"   r = {correlation:.6f}")
    print(f"   Check: r² = ({correlation:.6f})² = {correlation**2:.6f} = R² ✓")
    print()

    # -----------------------------------------------------------------
    # Verification of Results
    # -----------------------------------------------------------------

    print("VERIFICATION")
    print("-" * 40)

    # Verify that the line passes through (X̄, Ȳ)
    y_at_mean = beta_0 + beta_1 * X_mean
    print(f"1. Check if regression line passes through (X̄, Ȳ):")
    print(f"   At X = X̄ = {X_mean:.4f}: Ŷ = {beta_0:.6f} + {beta_1:.6f} × {X_mean:.4f} = {y_at_mean:.6f}")
    print(f"   This equals Ȳ = {Y_mean:.4f} ✓")
    print()

    # Verify that residuals sum to zero
    sum_residuals = np.sum(residuals)
    print(f"2. Check if sum of residuals equals zero:")
    print(f"   Σ(Yᵢ - Ŷᵢ) = {sum_residuals:.10f} ≈ 0 ✓")
    print()

    # Verify that residuals are uncorrelated with X
    residuals_X_correlation = np.sum((X - X_mean) * residuals) / denominator if denominator != 0 else 0
    print(f"3. Check if residuals are uncorrelated with X:")
    print(f"   Σ(Xᵢ - X̄)(Yᵢ - Ŷᵢ) = {residuals_X_correlation * denominator:.10f} ≈ 0 ✓")
    print()

    # Return all results
    results = {
        'n': n,
        'X_mean': X_mean,
        'Y_mean': Y_mean,
        'numerator': numerator,
        'denominator': denominator,
        'beta_1': beta_1,
        'beta_0': beta_0,
        'equation': f"Ŷ = {beta_0:.6f} + {beta_1:.6f}X",
        'Y_pred': Y_pred,
        'residuals': residuals,
        'SSR': SSR,
        'SST': SST,
        'R_squared': R_squared,
        'correlation': correlation,
        'sum_residuals': sum_residuals
    }

    return results

def visualize_regression(X, Y, results):
    """
    Create visualizations for the regression results
    """
    fig, axes = plt.subplots(2, 2, figsize=(14, 12))

    X = np.array(X)
    Y = np.array(Y)
    beta_0 = results['beta_0']
    beta_1 = results['beta_1']
    Y_pred = results['Y_pred']
    X_mean = results['X_mean']
    Y_mean = results['Y_mean']

    # Plot 1: Data points and regression line
    axes[0, 0].scatter(X, Y, color='blue', alpha=0.7, s=80, label='Actual Data')

    # Plot regression line
    x_line = np.linspace(min(X) - 1, max(X) + 1, 100)
    y_line = beta_0 + beta_1 * x_line
    axes[0, 0].plot(x_line, y_line, 'red', linewidth=2, label='Regression Line')

    # Plot residuals
    for xi, yi, ypi in zip(X, Y, Y_pred):
        axes[0, 0].plot([xi, xi], [yi, ypi], 'gray', linestyle='--', alpha=0.5)

    # Plot centroid
    axes[0, 0].scatter(X_mean, Y_mean, color='green', s=200, marker='*',
                      label=f'Centroid ({X_mean:.2f}, {Y_mean:.2f})', zorder=5)

    axes[0, 0].set_xlabel('X (Independent Variable)')
    axes[0, 0].set_ylabel('Y (Dependent Variable)')
    axes[0, 0].set_title(f'Regression Line: Ŷ = {beta_0:.4f} + {beta_1:.4f}X\nR² = {results["R_squared"]:.4f}')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # Plot 2: Residual plot
    axes[0, 1].scatter(Y_pred, results['residuals'], color='green', alpha=0.7, s=80)
    axes[0, 1].axhline(y=0, color='red', linestyle='--', alpha=0.5)
    axes[0, 1].set_xlabel('Predicted Values (Ŷ)')
    axes[0, 1].set_ylabel('Residuals (Y - Ŷ)')
    axes[0, 1].set_title('Residual Plot\n(Checking for Patterns)')
    axes[0, 1].grid(True, alpha=0.3)

    # Plot 3: Components visualization
    # Show deviations for a few points
    X_dev = X - X_mean
    Y_dev = Y - Y_mean

    # Select a subset for clarity
    indices = range(min(5, len(X)))

    # Plot points
    axes[1, 0].scatter(X, Y, color='blue', alpha=0.5, s=60)
    axes[1, 0].axhline(Y_mean, color='green', linestyle='--', alpha=0.7, label='Mean of Y')
    axes[1, 0].axvline(X_mean, color='green', linestyle='--', alpha=0.7, label='Mean of X')

    # Draw deviation lines for selected points
    for i in indices:
        # X deviation
        axes[1, 0].plot([X[i], X_mean], [Y[i], Y[i]], 'orange', alpha=0.5, linestyle=':')
        # Y deviation
        axes[1, 0].plot([X[i], X[i]], [Y[i], Y_mean], 'purple', alpha=0.5, linestyle=':')
        # Point label
        axes[1, 0].text(X[i], Y[i], f'  ({X_dev[i]:.1f}, {Y_dev[i]:.1f})',
                       fontsize=8, alpha=0.7)

    axes[1, 0].set_xlabel('X')
    axes[1, 0].set_ylabel('Y')
    axes[1, 0].set_title('Deviations from Means\n(Xᵢ - X̄, Yᵢ - Ȳ)')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # Plot 4: Variance decomposition
    explained_pct = (results['R_squared']) * 100
    unexplained_pct = 100 - explained_pct

    labels = [f'Explained by X\n{explained_pct:.1f}%',
              f'Unexplained\n{unexplained_pct:.1f}%']
    sizes = [explained_pct, unexplained_pct]
    colors = ['lightgreen', 'lightcoral']

    axes[1, 1].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%',
                   startangle=90, explode=(0.05, 0))
    axes[1, 1].set_title(f'Variance Decomposition\n(R² = {results["R_squared"]:.4f})')

    plt.tight_layout()
    plt.show()

def example_application():
    """
    Apply the manual regression to example data
    """
    print("\n" + "=" * 70)
    print("EXAMPLE APPLICATION")
    print("=" * 70)

    # Example 1: Simple data (hours studied vs exam score)
    print("\nExample 1: Hours Studied vs Exam Score")
    print("-" * 40)

    X1 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])  # Hours studied
    Y1 = np.array([55, 60, 65, 70, 75, 80, 85, 90, 95, 100])  # Exam score

    results1 = manual_simple_linear_regression(X1, Y1)

    # Make a prediction
    print("\nPrediction for X = 3.5 hours:")
    pred1 = results1['beta_0'] + results1['beta_1'] * 3.5
    print(f"   Ŷ = {results1['beta_0']:.4f} + {results1['beta_1']:.4f} × 3.5 = {pred1:.2f}")

    visualize_regression(X1, Y1, results1)

    # Example 2: Realistic data with noise
    print("\n" + "=" * 70)
    print("Example 2: Realistic Data with Noise")
    print("=" * 70)

    np.random.seed(42)
    X2 = np.array([10, 12, 15, 18, 20, 22, 25, 28, 30, 32])  # Age
    Y2 = 50 + 2.5 * X2 + np.random.normal(0, 5, len(X2))  # Income with noise

    print(f"\nX (Age): {X2}")
    print(f"Y (Income in $1000s): {Y2.round(2)}")
    print()

    results2 = manual_simple_linear_regression(X2, Y2)

    # Make a prediction
    print("\nPrediction for X = 35 years:")
    pred2 = results2['beta_0'] + results2['beta_1'] * 35
    print(f"   Ŷ = {results2['beta_0']:.4f} + {results2['beta_1']:.4f} × 35 = ${pred2:.2f}k")

    visualize_regression(X2, Y2, results2)

    # Example 3: Weak relationship
    print("\n" + "=" * 70)
    print("Example 3: Weak Relationship (Low R²)")
    print("=" * 70)

    X3 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    Y3 = np.array([20, 25, 15, 30, 18, 35, 22, 28, 19, 32])

    print(f"\nX: {X3}")
    print(f"Y: {Y3}")
    print()

    results3 = manual_simple_linear_regression(X3, Y3)
    visualize_regression(X3, Y3, results3)

# Run the example
if __name__ == "__main__":
    example_application()

"""**E. Task 3: Visualization of the Fit and Interpretation**"""

import numpy as np
import matplotlib.pyplot as plt

# 1. Enter your data here (EASY!)
X = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Change these!
Y = [3, 5, 7, 9, 11, 13, 15, 17, 19, 21]  # Change these!

# 2. Convert to numpy (automatic)
X = np.array(X)
Y = np.array(Y)

# 3. Calculate regression (automatic)
X_mean, Y_mean = np.mean(X), np.mean(Y)
beta_1 = np.sum((X - X_mean) * (Y - Y_mean)) / np.sum((X - X_mean) ** 2)
beta_0 = Y_mean - beta_1 * X_mean

# 4. Make predictions (automatic)
predictions = beta_0 + beta_1 * X

# 5. Show results (simple!)
print("=" * 40)
print("REGRESSION RESULTS:")
print("=" * 40)
print(f"Equation: Ŷ = {beta_0:.4f} + {beta_1:.4f}X")
print(f"\nInterpretation:")
print(f"• Slope: {beta_1:.4f} (Y changes by {beta_1:.4f} per 1 unit of X)")
print(f"• Intercept: {beta_0:.4f} (Y when X = 0)")
print(f"\nPredictions for your data:")
for i in range(len(X)):
    print(f"  X={X[i]}: Actual Y={Y[i]}, Predicted Ŷ={predictions[i]:.2f}")

# 6. Plot (automatic!)
plt.figure(figsize=(10, 6))
plt.scatter(X, Y, color='blue', s=100, alpha=0.7, label='Actual Data')
plt.plot(X, predictions, color='red', linewidth=3, label='Regression Line')
plt.xlabel('X (Independent Variable)', fontsize=12)
plt.ylabel('Y (Dependent Variable)', fontsize=12)
plt.title(f'Simple Linear Regression: Ŷ = {beta_0:.2f} + {beta_1:.2f}X', fontsize=14)
plt.legend(fontsize=12)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

"""**F. Task 4: Strength of Relationship**"""

import numpy as np

# Your data - just change these!
X = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
Y = [2, 4, 5, 7, 6, 8, 9, 11, 10, 12]

# Convert to numpy
X, Y = np.array(X), np.array(Y)

# 1. Calculate Correlation (r)
X_mean, Y_mean = np.mean(X), np.mean(Y)

numerator = np.sum((X - X_mean) * (Y - Y_mean))
denominator = np.sqrt(np.sum((X - X_mean)**2) * np.sum((Y - Y_mean)**2))
r = numerator / denominator

# 2. Calculate R²
R_squared = r**2

print("CORRELATION & R-SQUARED ANALYSIS")
print("="*50)
print(f"1. Correlation (r) = {r:.4f}")
print(f"2. R-squared (R²) = {R_squared:.4f}")
print(f"   (Note: R² = r² = {r:.4f}² = {R_squared:.4f})")

print("\n3. RELATIONSHIP ASSESSMENT:")
print("-"*30)

# Direction
if r > 0:
    direction = "POSITIVE (as X increases, Y tends to increase)"
elif r < 0:
    direction = "NEGATIVE (as X increases, Y tends to decrease)"
else:
    direction = "NO LINEAR RELATIONSHIP"

# Strength based on |r|
r_abs = abs(r)
if r_abs >= 0.9:
    strength = "VERY STRONG"
elif r_abs >= 0.7:
    strength = "STRONG"
elif r_abs >= 0.5:
    strength = "MODERATE"
elif r_abs >= 0.3:
    strength = "WEAK"
else:
    strength = "VERY WEAK or NO"

print(f"• Direction: {direction}")
print(f"• Strength: {strength} (|r| = {r_abs:.4f})")

# R² Interpretation
print(f"\n4. R² INTERPRETATION:")
print("-"*30)
print(f"• {R_squared*100:.2f}% of the variation in Y")
print(f"  can be explained by the linear relationship with X.")
print(f"• {100 - R_squared*100:.2f}% is due to other factors or random variation.")

# Quick visual of relationship
print(f"\n5. VISUAL CHECK:")
print("-"*30)
print(f"r = {r:+.4f} means:")
if r > 0.7:
    print("  Points cluster closely around an upward-sloping line")
elif r > 0:
    print("  General upward trend, but more scatter")
elif r < -0.7:
    print("  Points cluster closely around a downward-sloping line")
elif r < 0:
    print("  General downward trend, but more scatter")
else:
    print("  No clear linear pattern")

# Run this in Google Colab - No installation needed!
import numpy as np
import matplotlib.pyplot as plt

# Click here and enter your data
X = [1, 2, 3, 4, 5]  # Click and type your X values
Y = [2, 4, 5, 4, 5]  # Click and type your Y values

# Everything runs automatically
X, Y = np.array(X), np.array(Y)
beta_1 = np.sum((X-np.mean(X))*(Y-np.mean(Y))) / np.sum((X-np.mean(X))**2)
beta_0 = np.mean(Y) - beta_1 * np.mean(X)
r = np.sum((X-np.mean(X))*(Y-np.mean(Y))) / np.sqrt(np.sum((X-np.mean(X))**2)*np.sum((Y-np.mean(Y))**2))

print(f"Equation: Ŷ = {beta_0:.2f} + {beta_1:.2f}X")
print(f"Correlation: r = {r:.3f}")
print(f"R² = {r**2:.3f} ({r**2*100:.1f}% explained)")
print(f"For each 1 unit of X, Y changes by {beta_1:.3f}")

plt.scatter(X, Y, label='Your Data')
plt.plot(X, beta_0 + beta_1*X, 'r-', label='Best Fit Line')
plt.legend()
plt.show()

"""**G. Task 5: Reflection**"""

import numpy as np
def quick_summary(X, Y):
    """Quick regression summary"""
    X, Y = np.array(X), np.array(Y)
    beta_1 = np.sum((X-np.mean(X))*(Y-np.mean(Y))) / np.sum((X-np.mean(X))**2)
    beta_0 = np.mean(Y) - beta_1 * np.mean(X)
    r = np.sum((X-np.mean(X))*(Y-np.mean(Y))) / np.sqrt(np.sum((X-np.mean(X))**2)*np.sum((Y-np.mean(Y))**2))
    R2 = r**2
    print(f"\n1. R² SUPPORT:")
    print(f"   R² = {R2:.4f} ({R2*100:.1f}% explained)")
    if R2 > 0.7:
        print(f"   Strongly supports visual assessment")
    else:
        print(f"   Suggests weaker relationship than visual might indicate")
    print(f"\n2. REAL-WORLD APPLICATION:")
    if R2 > 0.9:
        print("   High-confidence predictions (e.g., engineering, finance)")
    elif R2 > 0.7:
        print("   Moderate-confidence forecasting (e.g., business, marketing)")
    else:
        print("   Exploratory analysis only (e.g., research, hypothesis testing)")

    print(f"\nEquation: Ŷ = {beta_0:.2f} + {beta_1:.2f}X")
quick_summary([1,2,3,4,5], [2,4,6,8,10])

import numpy as np
import matplotlib.pyplot as plt
def quick_summary(X, Y, show_graph=True):
    """Quick regression summary with optional graph"""
    X, Y = np.array(X), np.array(Y)
    beta_1 = np.sum((X-np.mean(X))*(Y-np.mean(Y))) / np.sum((X-np.mean(X))**2)
    beta_0 = np.mean(Y) - beta_1 * np.mean(X)
    r = np.sum((X-np.mean(X))*(Y-np.mean(Y))) / np.sqrt(np.sum((X-np.mean(X))**2)*np.sum((Y-np.mean(Y))**2))
    R2 = r**2
    if R2 >= 0.9:
        visual_assessment = "Excellent"
        application = "High-confidence predictions"
    elif R2 >= 0.7:
        visual_assessment = "Good"
        application = "Moderate-confidence forecasting"
    elif R2 >= 0.5:
        visual_assessment = "Moderate"
        application = "Useful for trend analysis"
    else:
        visual_assessment = "Weak"
        application = "Exploratory analysis only"
    if show_graph:
        create_regression_plot(X, Y, beta_0, beta_1, r, R2, visual_assessment, application)
    return {
        'beta_0': beta_0,
        'beta_1': beta_1,
        'r': r,
        'R2': R2,
        'visual_assessment': visual_assessment,
        'application': application
    }
def create_regression_plot(X, Y, beta_0, beta_1, r, R2, visual_assessment, application):
    plt.figure(figsize=(5, 5))
    ax1 = plt.subplot2grid((3, 3), (0, 0), colspan=3, rowspan=2)
    scatter = ax1.scatter(X, Y, color='blue', s=30, alpha=0.7,
                         edgecolor='black', linewidth=0.5, label='Actual Data')
    x_line = np.linspace(min(X) - 1, max(X) + 1, 100)
    y_line = beta_0 + beta_1 * x_line
    ax1.plot(x_line, y_line, 'red', linewidth=2, label='Regression Line', zorder=5)
    predictions = beta_0 + beta_1 * X
    residuals = Y - predictions
    for xi, yi, ypi in zip(X, Y, predictions):
        ax1.plot([xi, xi], [yi, ypi], 'gray', linestyle='--', alpha=0.4, linewidth=0.5)
    X_mean, Y_mean = np.mean(X), np.mean(Y)
    ax1.scatter(X_mean, Y_mean, color='green', s=80, marker='*',
                label=f'Centroid', zorder=6)
    ax1.set_xlabel('X (Independent Variable)', fontsize=8)
    ax1.set_ylabel('Y (Dependent Variable)', fontsize=8)
    ax1.set_title(f'Ŷ = {beta_0:.4f} + {beta_1:.4f}X',
                 fontsize=10, fontweight='bold')
    ax1.legend(loc='best', fontsize=6)
    ax1.grid(True, alpha=0.3)
    ax1.tick_params(axis='both', which='major', labelsize=6)
    info_text = f'R² = {R2:.4f}\n{R2*100:.1f}% explained\nFit: {visual_assessment}'
    props = dict(boxstyle='round', facecolor='wheat', alpha=0.8)
    ax1.text(0.02, 0.98, info_text, transform=ax1.transAxes, fontsize=6,
             verticalalignment='top', bbox=props)
if __name__ == "__main__":
    np.random.seed(42)
    X2 = [10, 15, 20, 25, 30, 35, 40, 45, 50, 55]
    Y2 = [25 + 0.8*x + np.random.normal(0, 10) for x in X2]
    results2 = quick_summary(X2, Y2, show_graph=True)

"""**Analysis and Conclusion**"""

print("""Regression analysis is used to understand the relationship between crop yield and factors such as rainfall, temperature, fertilizer use, and irrigation practices.
Only linear relationships are considered: If the true relationship is non-linear, the regression model may give incorrect results. Effect of outliers: Extremely high
or low values can influence the results and reduce accuracy. No cause-and-effect conclusion: A strong relationship or high R² value does not prove causation; it only
shows association.""")

"""Final Conclusion"""

print("""Agriculture Crop Yield dataset using various statistical and probability concepts to understand the factors influencing agricultural productivity.This analysis
average crop yield is approximately 3.6 tons per hectare, with a mean rainfall of about 820 mm and an average temperature ranging between 25–26°C. The dataset includes
multiple regions, soil types, crops, and weather conditions, making the analysis. Sampling produced the most accurate estimate of mean yield, while simple random
sampling was easier to apply but slightly less precise. Cluster sampling showed the highest variability. Data visualization tools such as histograms and bar charts
revealed that the South region and cloudy weather are most associated with higher yields. indicated a weak linear relationship between rainfall and temperature, with
an 𝑅^2 value of 0.1058, meaning most variation is explained by other factors. The crop yields are influenced by multiple factors, with regional and environmental
conditions playing significant roles. For instance, the South region and cloudy weather showed the highest conditional probabilities for high yield while fertilizer
use increased the likelihood of high yield from 50% to 75%
The analysis began with a clear focus on rice cultivation in Bangladesh, establishing the importance of environmental and agronomic variables such as soil type, rainfall,
temperature, and farming practices. Through descriptive statistics, the dataset’s central tendencies and variabilities were quantified, revealing an average yield of
approximately 3.6 tons/ha and setting the stage for deeper investigation. The application of probability sampling methods—including simple random, systematic, stratified,
and cluster sampling—highlighted the importance of sampling design in estimating population parameters accurately, with stratified sampling emerging as the most reliable
approach for this dataset.Data visualization through histograms, bar charts, ogives, and frequency polygons effectively communicated distribution patterns, regional
disparities and cumulative trends.
This milestone histograms, bar charts, and ogive charts, the analysis reveals important patterns. Bayes' Theorem to "invert" probabilities. While we can easily find the
probability of a high yield given a southern location. The analysis checks if the yield data a Normal Distribution. This is important because many statistical
tests assume normaly. The project finds that while the data is roughly normal the theoretical "bell curve. The importance of checking assumptions before
certain models.
Regression analysis numerical variables like rainfall and yield. That show that while certain factors have a measurable relationship with yield. The linear models
alone often explain a limited portion of the total variation""")