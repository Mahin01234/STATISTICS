# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mVLr7AM1dwoGIk4TUWnhkT7ggFcoTrN0

**[Milestone 1](https://)**

**Project Name : Agriculture Crop Yield**

**The dataset**
"""

df = pd.read_csv('crop_yield.csv')
df

"""**At least 10 columns with varying data types**

**At least 100 rows (data points).**
"""

display(df.head(100))

df.info()

df.sample(10)

df.describe()

df.sample(10)

"""**[Milestone 2](https://)**

**Sampling Assignment**

**Implementing Probability Sampling Methods in Python**

Instructions

Upload your dataset (minimum 200 rows), then complete all parts A–F.

**Part A — Setup**
**Report dataset size (rows, columns)**
"""

import pandas as pd
import numpy as np


df = pd.read_csv('/content/crop_yield.csv')
df.head(10)

print("Dataset size:", df.shape)
population_mean = df['Temperature_Celsius'].mean()

"""**Part B — Simple Random Sampling (20 points)**"""

import pandas as pd
import numpy as np

df = pd.read_csv('/content/crop_yield.csv')
df.head(50)

sample_df = df.sample(n=50, random_state=42)
sample_mean = sample_df['Temperature_Celsius'].mean()


print(f"Population Mean (Temperature_Celsius): {population_mean:.2f}")
print(f"Sample Mean (Temperature_Celsius, n=50): {sample_mean:.2f}")

"""
**Part B — Simple Random Sampling**
"""

sample_size = 50
srs = df.sample(n=sample_size, random_state=42)
display(srs.head())
population_mean = df['Temperature_Celsius'].mean()
print("Population mean:", population_mean)
srs_mean = srs['Temperature_Celsius'].mean()
print("Sample mean:", srs_mean)

"""**Part C — Systematic Sampling**"""

n = 50
k = len(df) // n
start = np.random.randint(0, k)
sys_sample = df.iloc[start::k][:n]
display(sys_sample.head())
sys_mean = sys_sample['Temperature_Celsius'].mean()
print ("\n")
print("Sample mean:", sys_mean)

"""**Part D — Stratified Sampling**"""

strata_col = "Region"  # Corrected to an existing column
sample_size = 50

# proportional fraction for each group
frac = sample_size / len(df)

# stratified sample
stratified_sample = df.groupby(strata_col, group_keys=False).sample(frac=frac, random_state=42)

display(stratified_sample.head())
strat_mean = stratified_sample['Temperature_Celsius'].mean() # Corrected column name

print ("\n")
print("Sample mean:", strat_mean)

"""**Part E — Cluster Sampling**"""

df['cluster_id'] = df.index // (len(df)//10)  # 10 clusters
selected_clusters = np.random.choice(df['cluster_id'].unique(), size=2, replace=False)
cluster_sample = df[df['cluster_id'].isin(selected_clusters)]
print("Selected clusters:", selected_clusters)
display(cluster_sample.head())
cluster_mean = cluster_sample['Temperature_Celsius'].mean() # Corrected column name

print ("\n")
print("Sample mean:", cluster_mean)

"""
**Part F — Comparison & Reflection**


Compare sample means vs population mean, then write your reflection."""

df['cluster_id'] = df.index // (len(df)//10)  # 10 clusters
selected_clusters = np.random.choice(df['cluster_id'].unique(), size=2, replace=False)
cluster_sample = df[df['cluster_id'].isin(selected_clusters)]
print("Selected clusters:", selected_clusters)
cluster_sample.head()

comparison = pd.DataFrame({
    'Method': ['Simple Random', 'Systematic', 'Stratified', 'Cluster'],
    'Sample Mean': [srs_mean, sys_mean, strat_mean, cluster_mean],
    'Population Mean': [population_mean]*4,
    'Difference': [(srs_mean - population_mean),
                   (sys_mean - population_mean),
                   (strat_mean - population_mean),
                   (cluster_mean - population_mean)]
})
print(comparison)

"""**Part F — Comparison & Reflection**"""

print ("Compare sample means vs population mean, then write your reflection.")

print ("In this milestone, I applied four probability sampling methods to the\nAgricultureCrop Yield dataset from Kaggle, which includes crop production\ndata across multiple countries.\nThe goal was to compare Simple Random Sampling, Systematic Sampling,\nStratified Sampling and Cluster Sampling in estimating the population mean of\ncrop yield, which was 32.337344 t/ha.\nStratified sampling produced the most accurate result with a mean of 32.3276\nt/ha, as proportional allocation preserved the distribution of crop types and\nregions.\nSimple Random Sampling yielded 32.25 t/ha, slightly lower, while systematic\nsampling gave 32.3872 t/ha, slightly higher.\nCluster sampling showed the largest deviation at 32.5075 t/ha due to\npotential homogeneity within clusters.\nIn terms of implementation, Simple Random Sampling was easiest, requiring\nminimal code.\nSystematic sampling was straightforward with a defined step size, while\nstratified sampling needed careful grouping.\nCluster sampling was simple but required thoughtful cluster selection.")

print("Overall, stratified sampling ensured maximum accuracy, and Simple Random\nSam- pling was the simplest to implement.")

"""[**Milestone 3**](https://)


**Frequency Distribution and Data Visualization**

**Part 1: Data Loading and Preparation**


In this section, we will load the dataset and import the necessary libraries for our analysis.

Note: Remove the HASHTAG in the next cell to allow installation of the packages required [IF NEEDED]
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Dataset
file_path = '/content/crop_yield.csv'
df = pd.read_csv(file_path)

# Dataset
print("Dataset Shape:", df.shape)
print("\nColumns in Dataset:\n", df.columns)
print("\nFirst 5 Rows:\n", df.head())
print("\nMissing Values:\n", df.isnull().sum())

import pandas as pd
import numpy as np

df = pd.read_csv('/content/crop_yield.csv')

df.head()

"""**Part 2: Frequency Distribution Table**

Here, we will select a column and construct a frequency distribution table.
"""

import pandas as pd
import numpy as np


df = pd.read_csv('/content/crop_yield.csv')


column_to_analyze = 'Crop'



freq_table = pd.DataFrame(df[column_to_analyze].value_counts()).reset_index()
freq_table.columns = ['Class/Category', 'Frequency (f)']
freq_table = freq_table.sort_values(by='Class/Category').reset_index(drop=True)


total_count = freq_table['Frequency (f)'].sum()
freq_table['Relative Frequency (rf)'] = freq_table['Frequency (f)'] / total_count


freq_table['Cumulative Frequency (cf)'] = freq_table['Frequency (f)'].cumsum()


freq_table['Relative Cumulative Frequency (rcf)'] = freq_table['Relative Frequency (rf)'].cumsum()

print(" Frequency Distribution Table for:", column_to_analyze)
freq_table

column_to_analyze = 'column_name'

column_to_analyze = 'Yield_tons_per_hectare'
freq_table = pd.DataFrame(df[column_to_analyze].value_counts()).reset_index()
freq_table.columns = ['Class/Category', 'Frequency (f)']
freq_table = freq_table.sort_values(by='Class/Category').reset_index(drop=True)


total_count = freq_table['Frequency (f)'].sum()
freq_table['Relative Frequency (rf)'] = freq_table['Frequency (f)'] / total_count

freq_table['Cumulative Frequency (cf)'] = freq_table['Frequency (f)'].cumsum()


freq_table['Relative Cumulative Frequency (rcf)'] = freq_table['Relative Frequency (rf)'].cumsum()



print("Frequency Distribution Table for '" + column_to_analyze + "'")
freq_table

"""**Part 3: Graphical Representation**


In this section, we will visualize the data distribution using various charts.
"""

df.sample(10)

"""**Bar Chat**"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

df = pd.read_csv('/content/crop_yield.csv')

sample_20_df = df.sample(n=10, random_state=20)
print("First 5 rows of the 20-row sample:")
display(sample_20_df.head())


crop_counts_sample = sample_20_df['Crop'].value_counts().reset_index()
crop_counts_sample.columns = ['Crop Type', 'Frequency']


plt.figure(figsize=(10, 6))
sns.barplot(x='Crop Type', y='Frequency', data=crop_counts_sample, palette='viridis', hue='Crop Type', legend=False)
plt.title('Bar Chat of Yield Frequency Distribution of Crop Types (20-Row Sample)')
plt.xlabel('Crop Type')
plt.ylabel('Frequency')
plt.xticks(rotation=40, ha='right')
plt.grid(axis='y', alpha=0.7)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd # Ensure pandas is imported

df = pd.read_csv('/content/crop_yield.csv') # Reload the original DataFrame

sample_30_df = df.sample(n=30, random_state=20)
print("First 5 rows of the 30-row sample:")
display(sample_30_df.head())


plt.figure(figsize=(10, 6))
sns.histplot(sample_30_df['Yield_tons_per_hectare'], kde=True, bins=5, color='skyblue')
plt.title('Histogram of Yield (tons per hectare) (30-Row Sample)')
plt.xlabel('Yield (tons per hectare)')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.7)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

df = pd.read_csv('/content/crop_yield.csv')
sample_30_df = df.sample(n=30, random_state=20)
print("First 5 rows of the 30-row sample:")
display(sample_30_df.head())


plt.figure(figsize=(10, 6))
sns.ecdfplot(data=sample_30_df, x='Yield_tons_per_hectare', color='skyblue')
plt.title('Ogive Chart (Cumulative Frequency) of Yield (tons per hectare) (30-Row Sample)')
plt.xlabel('Yield (tons per hectare)')
plt.ylabel('Cumulative Frequency / Proportion')
plt.grid(axis='both', alpha=0.7)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np



hist, bin_edges = np.histogram(sample_30_df['Yield_tons_per_hectare'], bins=5)


bin_midpoints = (bin_edges[:-1] + bin_edges[1:]) / 2


plt.figure(figsize=(10, 6))
plt.plot(bin_midpoints, hist, marker='o', linestyle='-', color='purple')
plt.title('Frequency Polygon of Yield (tons per hectare) (30-Row Sample)')
plt.xlabel('Yield (tons per hectare)')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.7)
plt.tight_layout()
plt.show()

"""**Rainfall**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


rainfall_data = {
    'Rainfall_mm': [347.733856, 191.661333, 985.486244, 230.494966, 944.241902],
    'Region': ['South', 'North', 'East', 'West', 'North'],
    'Crop': ['Maize', 'Wheat', 'Rice', 'Rice', 'Barley']
}

df = pd.DataFrame(rainfall_data)

print("Rainfall Table:")
print("=" * 45)
print(df[['Rainfall_mm', 'Region', 'Crop']].to_string(index=False))
print("=" * 45)


# Ensure df_full is loaded (assuming it should be the crop_yield.csv dataset)
df_full = pd.read_csv('/content/crop_yield.csv')

# Data to be plotted is Rainfall_mm from the full dataset
temp_data = df_full['Rainfall_mm'] # Renamed from temp_data to avoid confusion with actual temperature

plt.figure(figsize=(12, 8))


plt.subplot(2, 2, 1) # Standardized to 2x2 grid
plt.hist(temp_data, bins=20, color='lightblue', edgecolor='black')
plt.title('1. Histogram (Rainfall)')
plt.xlabel('Rainfall (mm)') # Corrected label
plt.ylabel('Frequency')

plt.subplot(2, 2, 2) # Standardized to 2x2 grid
sorted_data = np.sort(temp_data)
cumulative = np.arange(1, len(sorted_data) + 1)
plt.plot(sorted_data, cumulative, 'r-')
plt.title('2. Ogive Chart (Rainfall)')
plt.xlabel('Rainfall (mm)') # Corrected label
plt.ylabel('Cumulative Frequency')


plt.subplot(2, 2, 3) # Standardized to 2x2 grid (moved to next position)
counts, bin_edges = np.histogram(temp_data, bins=20)
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
plt.plot(bin_centers, counts, 'g-o')
plt.title('3. Frequency Polygon (Rainfall)')
plt.xlabel('Rainfall (mm)') # Corrected label
plt.ylabel('Frequency')

# Adding a placeholder for the 4th plot, or adjust layout if only 3 plots are desired
# For now, let's keep 4 slots and leave the 4th empty or add a simple plot
plt.subplot(2, 2, 4) # Added for completeness of 2x2 grid
plt.title('4. Placeholder')
plt.axis('off') # Turn off axes for empty plot


plt.tight_layout()
plt.show()

"""**Temperature Celsius**"""

import matplotlib.pyplot as plt
import numpy as np


temp_data = df_full['Temperature_Celsius']

plt.figure(figsize=(12, 8))


plt.subplot(2, 2, 1)
plt.hist(temp_data, bins=20, color='lightblue', edgecolor='black')
plt.title('1. Histogram')
plt.xlabel('Temperature (°C)')
plt.ylabel('Frequency')

plt.subplot(2, 2, 2)
sorted_data = np.sort(temp_data)
cumulative = np.arange(1, len(sorted_data) + 1)
plt.plot(sorted_data, cumulative, 'r-')
plt.title('2. Ogive Chart')
plt.xlabel('Temperature (°C)')
plt.ylabel('Cumulative Frequency')


plt.subplot(2, 2, 3)
counts, bin_edges = np.histogram(temp_data, bins=20)
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
plt.plot(bin_centers, counts, 'g-o')
plt.title('3. Frequency Polygon')
plt.xlabel('Temperature (°C)')
plt.ylabel('Frequency')


plt.subplot(2, 2, 4)
more_than = np.arange(len(sorted_data), 0, -1)
plt.plot(sorted_data, more_than, 'b-')
plt.title('4. More than Ogive')
plt.xlabel('Temperature (°C)')
plt.ylabel('Cumulative Frequency')

plt.tight_layout()
plt.show()

"""**Days to Harvest**"""

import matplotlib.pyplot as plt
import numpy as np


temp_data = df_full['Days_to_Harvest'] # Data is 'Days_to_Harvest'

plt.figure(figsize=(12, 8))


plt.subplot(2, 2, 1)
plt.hist(temp_data, bins=20, color='lightblue', edgecolor='black')
plt.title('1. Histogram')
plt.xlabel('Days to Harvest') # Corrected xlabel
plt.ylabel('Frequency')

plt.subplot(2, 2, 2)
sorted_data = np.sort(temp_data)
cumulative = np.arange(1, len(sorted_data) + 1)
plt.plot(sorted_data, cumulative, 'r-')
plt.title('2. Ogive Chart')
plt.xlabel('Days to Harvest') # Corrected xlabel
plt.ylabel('Cumulative Frequency')


plt.subplot(2, 2, 3)
counts, bin_edges = np.histogram(temp_data, bins=20)
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
plt.plot(bin_centers, counts, 'g-o')
plt.title('3. Frequency Polygon')
plt.xlabel('Days to Harvest') # Corrected xlabel
plt.ylabel('Frequency')


plt.subplot(2, 2, 4)
more_than = np.arange(len(sorted_data), 0, -1)
plt.plot(sorted_data, more_than, 'b-')
plt.title('4. More than Ogive')
plt.xlabel('Days to Harvest') # Corrected xlabel
plt.ylabel('Cumulative Frequency')

plt.tight_layout()
plt.show()

"""**Analysis and Conclusion**"""

print ("""Frequency Table Insights

• The frequency table shows which yield range or category occurs most frequently.

• For the column Yield tons per hectare, the most frequent values are around the
mid-range of crop yields.

• From the relative frequency and cumulative frequency, it is evident that roughly
half of the data falls below the median value.



Bar Chart (Regional Analysis)

• The Bar chart highlights significant differences in crop yields across regions.

• West and South regions tend to have higher yields.

• North region shows comparatively lower productivity.

Ogive Charts (Cumulative Frequency Analysis)

• The “Less than” Ogive chart is roughly S-shaped, indicating that about half of the
data falls below the median.

• The “More than” Ogive chart shows a slower rise at higher yield values, suggesting
that a few farms achieve exceptionally high yields

• Ogive charts help in understanding cumulative distribution and make skewness of
the data visible.



Distribution Shape & Variability

• Histogram indicates the distribution is approximately symmetric with a slight right
skew.

• Some high-yield and low-yield observations may be outliers.

• Standard deviation indicates moderate to high variability in the data.

Conclusion

• Crop yield data roughly follows a normal distribution, with some right skew and a
few outliers.


Probability Sampling Methods Sampling Assignment



• Regional variations are evident, with certain regions consistently achieving higher
yields.



• Frequency table, Bar chart, and Ogive analysis together provide a clear understand-

ing of distribution patterns, cumulative trends, and regional disparities.



• This analysis is useful for agricultural planning and decision-making for targeted

interventions.""")

"""**Challenges**

Region-wise differences
"""

crop_distribution = df.groupby(['Region', 'Crop']).size().reset_index(name='Count')
display(crop_distribution.head())

"""Soil type impact"""

print(""" Group the DataFrame by 'Soil_Type' and 'Crop', then calculate the average 'Yield_tons_per_hectare'\nfor each combination to identify which crops yield best on which soil types.

Reasoning: I will group the DataFrame by 'Soil_Type' and 'Crop' and calculate the mean of\n'Yield_tons_per_hectare' to understand the\naverage yield for each combination, then reset the index and create a pivot table for visualization.""")

soil_crop_yield = df.groupby(['Soil_Type', 'Crop'])['Yield_tons_per_hectare'].mean().reset_index()
display(soil_crop_yield.head())

"""Crop-wise analysis"""

avg_yield_per_crop = df.groupby('Crop')['Yield_tons_per_hectare'].mean().reset_index()
display(avg_yield_per_crop)

environmental_conditions = df.groupby('Crop')[['Rainfall_mm', 'Temperature_Celsius']].agg(['mean', 'std']).reset_index()
environmental_conditions.columns = ['Crop', 'Rainfall_mm_mean', 'Rainfall_mm_std', 'Temperature_Celsius_mean', 'Temperature_Celsius_std']
display(environmental_conditions)

"""Weather impact"""

print("Average Yield by Weather Condition:")
display(df.groupby('Weather_Condition')['Yield_tons_per_hectare'].mean().reset_index())

"""Optimal conditions"""

# Method 1: Simple and direct
df.loc[df.groupby('Crop')['Yield_tons_per_hectare'].idxmax()][['Crop', 'Soil_Type', 'Yield_tons_per_hectare']].sort_values('Crop')

# Method 2: Direct display without column renaming
df.groupby('Crop')[['Rainfall_mm', 'Temperature_Celsius']].agg(['mean', 'std'])

"""Climate diversity"""

pd.qcut(df['Rainfall_mm'], 3, labels=['Low', 'Medium', 'High']).pipe(
    lambda x: df.groupby(x, observed=True)['Yield_tons_per_hectare'].mean() )

# Shortest (if you want both)
print(df[['Rainfall_mm', 'Temperature_Celsius']].corrwith(df['Yield_tons_per_hectare']).round(2))

"""Conclusion:"""

print("""During this milestone, several challenges were encountered while analyzing the Agricul-
ture Crop Yield dataset:

1. Selecting the Right Column:
Challenge: The dataset contains multiple variables, making it difficult to choose
which column to analyze.
Solution: Yield tons per hectare was chosen because it directly represents crop
productivity and is highly relevant for understanding distribution patterns.
2. Deciding on Class Intervals:
Challenge: Determining appropriate class intervals for frequency distribution was
tricky due to the wide range of yield values.
Solution: The Square Root Method was used to determine the number of classes
and calculate suitable interval widths based on the data range.
3. Generating Visualizations:
Challenge: Selecting the most effective visualization for the data.
Solution: Multiple visualizations were created:
• Histogram – to see the distribution of yield values.
• Bar Chart – to compare average yields across regions.
• Frequency Polygon – to show smooth distribution patterns.
• Ogive Chart – to analyze cumulative frequency and percentiles.
4. Data Cleaning and Processing:
Challenge: The dataset contained missing values and potential outliers that could

13

Probability Sampling Methods Sampling Assignment

affect analysis.
Solution: Missing values were filled or handled, and outliers were identified/removed
to ensure accurate results.
Conclusion:
Overcoming these challenges allowed a thorough statistical analysis and creation of clear,

informative visualizations. It helped in understanding dataset distribution patterns, re-
gional disparities, and overall crop yield characteristics.""")

"""**[Milestone 4](https://)**

**Milestone 4 - STA 2101: Measures of Central Tendency and Dispersion**

**Task 1: Measures of Central Tendency**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


import pandas as pd

df = pd.read_csv('/content/crop_yield.csv')
df.head()

"""**Task 1: Measures of Central Tendency**"""

import pandas as pd


df = pd.read_csv('/content/crop_yield.csv')
cols = ['Rainfall_mm', 'Temperature_Celsius', 'Days_to_Harvest', 'Yield_tons_per_hectare']


for col in cols:
    print(f"\n=== {col} ===")
    print(f"Mean   : {df[col].mean():.1f}")
    print(f"Median : {df[col].median():.1f}")
    print(f"Mode   : {df[col].mode().iloc[0]}")

# 4. Quick skewness analysis
print("\n" + "-" * 50)
print("QUICK SKEWNESS ANALYSIS")
print("-" * 50)

for col in cols:
    mean_val = df[col].mean()
    median_val = df[col].median()

    if mean_val > median_val:
        skew = "Right skewed (mean > median)"

    elif mean_val < median_val:
        skew = "Left skewed (mean < median)"

    else:
        skew = "Symmetric (mean ≈ median)"


    print(f"{col}: {skew}")

import pandas as pd

df = pd.read_csv('/content/crop_yield.csv')
cols = ['Rainfall_mm', 'Temperature_Celsius', 'Days_to_Harvest', 'Yield_tons_per_hectare']

for col in cols:
    print(f"\n{col}:")

    # Basic stats
    mean = df[col].mean()
    median = df[col].median()
    mode = df[col].mode().iloc[0]
    var = df[col].var()
    std = df[col].std()

    # Print in simple format
    print(f"  Mean   : {mean:.1f}")
    print(f"  Median : {median:.1f}")
    print(f"  Mode   : {mode}")
    print(f"  Var    : {var:.1f}")
    print(f"  Std Dev: {std:.1f}")

    # Simple insight
    if mean > median:
        print(f"  → Right skewed")
    elif mean < median:
        print(f"  → Left skewed")

"""**Task 2: Measures of Dispersion**"""

import pandas as pd

# Load
df = pd.read_csv('/content/crop_yield.csv')
cols = ['Rainfall_mm', 'Temperature_Celsius']


for col in cols:
    print(f"\n{col}:")
    print(f"  Variance : {df[col].var():.2f}")
    print(f"  Std Dev  : {df[col].std():.2f}")

    # Compare
    if df[col].var() > df[cols[0]].var() and col != cols[0]:
        print("  → More spread than", cols[0])

"""**Task 3: Visualization**"""

import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('/content/crop_yield.csv')

for col in ['Rainfall_mm', 'Temperature_Celsius']:
    plt.hist(df[col], bins=15, alpha=0.5)
    plt.axvline(df[col].mean(), color='r')
    plt.axvline(df[col].median(), color='g')
    plt.axvline(df[col].mode().iloc[0], color='y')
    plt.title(col)
    plt.show()

"""**Task 4: Analysis and Conclusion**"""

import pandas as pd

df = pd.read_csv('/content/crop_yield.csv')

print("ANALYSIS:")
print(f"Rainfall mean: {df['Rainfall_mm'].mean():.1f}±{df['Rainfall_mm'].std():.1f}")
print(f"Temp mean: {df['Temperature_Celsius'].mean():.1f}±{df['Temperature_Celsius'].std():.1f}")

print("\nCONCLUSION:")
print("Data shows normal distribution with moderate spread.")

"""**[Milestone 5 - Probability](https://)**

**Section A: Setup & Load Dataset**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


import pandas as pd

df = pd.read_csv('/content/crop_yield.csv')
df.head()

"""**Section B: Treat Dataset as Sample Space**"""

N = len(df)
print('Total observations:', N)

"""
**Section C: Task 1 - Defining Events**"""

A = df[df['Days_to_Harvest'] > 100]
B = df[df['Fertilizer_Used'] == True]
C = df[(df['Days_to_Harvest'] >= 90) & (df['Days_to_Harvest'] <= 120)]

print('Event A size:', len(A))
print('Event B size:', len(B))
print('Event C size:', len(C))

"""**Section D: Task 2 - Calculating Basic Probability**"""

P_A = len(A) / N
P_B = len(B) / N
P_C = len(C) / N

print('P(A) =', P_A)
print('P(B) =', P_B)
print('P(C) =', P_C)

"""**Section E: Task 3 - Combined Events**"""

A_int_B = df[(df['Days_to_Harvest'] > 100) & (df['Fertilizer_Used'] == True)]
P_A_int_B = len(A_int_B) / N
print('P(A \u2229 B) =', P_A_int_B)

A_union_B = df[(df['Days_to_Harvest'] > 100) | (df['Fertilizer_Used'] == True)]
P_A_union_B = len(A_union_B) / N
print('P(A \u222a B) =', P_A_union_B)

A_comp = df[df['Days_to_Harvest'] <= 100]
P_A_comp = len(A_comp) / N
print('P(A\u1d9c) =', P_A_comp)

rule_value = P_A + P_B - P_A_int_B
print('Rule Verification (P(A) + P(B) - P(A \u2229 B)):', rule_value)
print('Actual P(A \u222a B):', P_A_union_B)

"""**Section F: Visualization**"""

counts = [len(A), len(A_comp)]
labels = ['A:  > 30', 'Aᶜ:  ≤ 30']

plt.bar(labels, counts)
plt.title('Event A and Complement Frequency')
plt.ylabel('Count')
plt.show()

df['Region'].value_counts().plot(kind='bar')
plt.title('Region Frequency')
plt.ylabel('Count')
plt.xlabel('Region')
plt.show()

"""**Section G: Summary**"""

print("""1. Most Likely Events
Most students (about 68%) scored above 70 in Exam 1.
Also, slightly more than half of the students (52%) are in Section B.

2. Interesting Findings
I noticed that students from Section A who scored above 70 were fewer than expected.About 32% of students scored 70 or below, which is worth looking into.

3. How Probability Helps
Probability helps us make decisions with numbers. For example:

Since most students do well, teachers might make the exam harder next time.

If one section performs worse, the school could give them extra help.

These numbers help find patterns and improve teaching.""")

"""**[Milestone 6](https://)**

**Introduction**
"""

print(""" Building on the previous milestone on basic probability, this chapter introduces conditional probability, independent vs. dependent events, Bayes' rule, and
probability distributions. These concepts are fundamental for modeling uncertainty in data and are widely used in statistical inference, machine learning, and
decision-making.
""")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import norm, chi2_contingency, shapiro, kstest
import warnings
warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("Data loaded successfully!")
print(f"Sample data shape: {df.shape}")

import pandas as pd
import numpy as np
df = pd.read_csv('/content/crop_yield.csv')
print("3. CONDITIONAL PROBABILITY CALCULATIONS")
print("="*80)
print("\nMARGINAL PROBABILITIES:")
print("-"*40)
marginal_probs = {}
region_counts = df['Region'].value_counts()
for region, count in region_counts.items():
    prob = count / len(df)
    marginal_probs[f'P({region})'] = prob
    print(f"P({region}) = {count}/{len(df)} = {prob:.3f}")
soil_counts = df['Soil_Type'].value_counts() # Corrected column name
for soil, count in soil_counts.items():
    prob = count / len(df)
    marginal_probs[f'P({soil})'] = prob
    print(f"P({soil}) = {count}/{len(df)} = {prob:.3f}")
crop_counts = df['Crop'].value_counts()
for crop, count in crop_counts.items():
    prob = count / len(df)
    marginal_probs[f'P({crop})'] = prob
    print(f"P({crop}) = {count}/{len(df)} = {prob:.3f}") # Removed extraneous 'r'
weather_counts = df['Weather_Condition'].value_counts() # Corrected column name
for weather, count in weather_counts.items():
    prob = count / len(df)
    marginal_probs[f'P({weather})'] = prob
    print(f"P({weather}) = {count}/{len(df)} = {prob:.3f}")
print("\nCONDITIONAL PROBABILITIES:")
print("-"*40)
print("\nP(Crop | Soil Type):")
soils = df['Soil_Type'].unique() # Corrected column name
crops = df['Crop'].unique()
for soil in soils:
    soil_df = df[df['Soil_Type'] == soil] # Corrected column name
    total_soil = len(soil_df)
    if total_soil > 0:
        print(f"\nGiven Soil = {soil} (n={total_soil}):")
        for crop in crops:
            crop_count = len(soil_df[soil_df['Crop'] == crop])
            if crop_count > 0:
                p_crop_given_soil = crop_count / total_soil
                print(f"  P({crop} | {soil}) = {crop_count}/{total_soil} = {p_crop_given_soil:.3f}")
print("\nP(High Yield | Weather):")
df['High_Yield'] = df['Yield_tons_per_hectare'] > df['Yield_tons_per_hectare'].mean()
weather_types = df['Weather_Condition'].unique() # Corrected column name
for weather in weather_types:
    weather_df = df[df['Weather_Condition'] == weather] # Corrected column name
    total_weather = len(weather_df)
    if total_weather > 0:
        high_yield_count = len(weather_df[weather_df['High_Yield'] == True])
        p_high_given_weather = high_yield_count / total_weather
        print(f"P(High Yield | {weather}) = {high_yield_count}/{total_weather} = {p_high_given_weather:.3f}")

print("4. Independent vs. Dependent Events")
print("="*80)
print("\nCONTINGENCY TABLES:")
print("-"*40)
soil_crop_ct = pd.crosstab(df['Soil_Type'], df['Crop']) # Corrected 'Soil' to 'Soil_Type'
print("Soil_Type × Crop Contingency Table:")
print(soil_crop_ct)
print()
region_weather_ct = pd.crosstab(df['Region'], df['Weather_Condition']) # Corrected 'Weather' to 'Weather_Condition'
print("Region × Weather_Condition Contingency Table:")
print(region_weather_ct)
print()
print("\nCHI-SQUARE TESTS FOR INDEPENDENCE:")
print("-"*40)
chi2_soil_crop, p_soil_crop, dof_soil_crop, expected_soil_crop = chi2_contingency(soil_crop_ct)
print(f"1. Soil_Type vs Crop:")
print(f"   χ² = {chi2_soil_crop:.3f}, p-value = {p_soil_crop:.4f}, df = {dof_soil_crop}")
if p_soil_crop < 0.05:
    print("   → REJECT independence: Soil_Type and Crop are dependent")
else:
    print("   → FAIL to reject independence: Soil_Type and Crop may be independent")
chi2_region_weather, p_region_weather, dof_region_weather, expected_region_weather = chi2_contingency(region_weather_ct)
print(f"\n2. Region vs Weather_Condition:")
print(f"   χ² = {chi2_region_weather:.3f}, p-value = {p_region_weather:.4f}, df = {dof_region_weather}")
if p_region_weather < 0.05:
    print("   → REJECT independence: Region and Weather_Condition are dependent")
else:
    print("   → FAIL to reject independence: Region and Weather_Condition may be independent")
print("\nEXPECTED vs OBSERVED FREQUENCIES (Soil_Type vs Crop):")
print("-"*40)
expected_df = pd.DataFrame(expected_soil_crop,
                           index=soil_crop_ct.index,
                           columns=soil_crop_ct.columns)
print("Expected Frequencies:")
print(expected_df.round(2))
print("\nObserved Frequencies:")
print(soil_crop_ct)
print("\nSTANDARDIZED RESIDUALS (Soil_Type vs Crop):")
print("-"*40)
residuals = (soil_crop_ct.values - expected_soil_crop) / np.sqrt(expected_soil_crop)
residuals_df = pd.DataFrame(residuals,
                           index=soil_crop_ct.index,
                           columns=soil_crop_ct.columns)
print(residuals_df.round(2))

print("5. BAYES' RULE APPLICATION")
print("="*80)
print("\nBAYES' RULE EXAMPLE:")
print("-"*40)
print("Problem: What is the probability that a field has Clay soil given it has Rice crop?")
print()
p_clay = len(df[df['Soil_Type'] == 'Clay']) / len(df)
print(f"P(Clay) = {len(df[df['Soil_Type'] == 'Clay'])}/{len(df)} = {p_clay:.3f}")
p_rice = len(df[df['Crop'] == 'Rice']) / len(df)
print(f"P(Rice) = {len(df[df['Crop'] == 'Rice'])}/{len(df)} = {p_rice:.3f}")
rice_clay_df = df[df['Soil_Type'] == 'Clay']
p_rice_given_clay = len(rice_clay_df[rice_clay_df['Crop'] == 'Rice']) / len(rice_clay_df)
print(f"P(Rice | Clay) = {len(rice_clay_df[rice_clay_df['Crop'] == 'Rice'])}/{len(rice_clay_df)} = {p_rice_given_clay:.3f}")
p_clay_given_rice = (p_rice_given_clay * p_clay) / p_rice
print(f"\nBayes' Rule Calculation:")
print(f"P(Clay | Rice) = [P(Rice | Clay) × P(Clay)] / P(Rice)")
print(f"               = [{p_rice_given_clay:.3f} × {p_clay:.3f}] / {p_rice:.3f}")
print(f"               = {p_clay_given_rice:.3f}")
print("\nVerification by direct calculation:")
clay_rice_count = len(df[(df['Soil_Type'] == 'Clay') & (df['Crop'] == 'Rice')])
total_rice = len(df[df['Crop'] == 'Rice'])
p_clay_given_rice_direct = clay_rice_count / total_rice if total_rice > 0 else 0
print(f"P(Clay | Rice) (direct) = {clay_rice_count}/{total_rice} = {p_clay_given_rice_direct:.3f}")
print("SECOND EXAMPLE: Weather and Yield")
print("="*40)
p_sunny = len(df[df['Weather_Condition'] == 'Sunny']) / len(df)
print(f"P(Sunny) = {len(df[df['Weather_Condition'] == 'Sunny'])}/{len(df)} = {p_sunny:.3f}")
p_high_yield = len(df[df['High_Yield'] == True]) / len(df)
print(f"P(High Yield) = {len(df[df['High_Yield'] == True])}/{len(df)} = {p_high_yield:.3f}")
high_yield_df = df[df['High_Yield'] == True]
p_sunny_given_high = len(high_yield_df[high_yield_df['Weather_Condition'] == 'Sunny']) / len(high_yield_df)
print(f"P(Sunny | High Yield) = {len(high_yield_df[high_yield_df['Weather_Condition'] == 'Sunny'])}/{len(high_yield_df)} = {p_sunny_given_high:.3f}")
p_high_given_sunny = (p_sunny_given_high * p_high_yield) / p_sunny
print(f"\nBayes' Rule Calculation:")
print(f"P(High Yield | Sunny) = [P(Sunny | High Yield) × P(High Yield)] / P(Sunny)")
print(f"                      = [{p_sunny_given_high:.3f} × {p_high_yield:.3f}] / {p_sunny:.3f}")
print(f"                      = {p_high_given_sunny:.3f}")
print("\nPRIOR vs POSTERIOR PROBABILITIES:")
print(f"Prior P(High Yield) = {p_high_yield:.3f}")
print(f"Posterior P(High Yield | Sunny) = {p_high_given_sunny:.3f}")
if p_high_given_sunny > p_high_yield:
    print("Conclusion: Sunny weather increases the probability of high yield")
else:
    print("Conclusion: Sunny weather decreases the probability of high yield")

print("6. NORMAL DISTRIBUTION ANALYSIS")
print("="*80)
analysis_data = df['Yield_tons_per_hectare']
mean_val = np.mean(analysis_data)
std_val = np.std(analysis_data)
median_val = np.median(analysis_data)
skew_val = stats.skew(analysis_data)
kurt_val = stats.kurtosis(analysis_data)
print("\nDESCRIPTIVE STATISTICS:")
print("-"*40)
print(f"Mean (μ): {mean_val:.4f}")
print(f"Standard Deviation (σ): {std_val:.4f}")
print(f"Median: {median_val:.4f}")
print(f"Skewness: {skew_val:.4f} (Normal ≈ 0)")
print(f"Kurtosis: {kurt_val:.4f} (Normal ≈ 0)")
print(f"Minimum: {np.min(analysis_data):.2f}")
print(f"Maximum: {np.max(analysis_data):.2f}")
print(f"Range: {np.max(analysis_data) - np.min(analysis_data):.2f}")
print(f"Sample Size: {len(analysis_data):,}")
print("\nEMPIRICAL RULE CHECK:")
print("-"*40)
within_1sigma = np.sum((analysis_data >= mean_val - std_val) & (analysis_data <= mean_val + std_val)) / len(analysis_data)
within_2sigma = np.sum((analysis_data >= mean_val - 2*std_val) & (analysis_data <= mean_val + 2*std_val)) / len(analysis_data)
within_3sigma = np.sum((analysis_data >= mean_val - 3*std_val) & (analysis_data <= mean_val + 3*std_val)) / len(analysis_data)
print(f"Data within μ ± σ: {within_1sigma*100:.2f}% (Expected: 68.27%)")
print(f"Data within μ ± 2σ: {within_2sigma*100:.2f}% (Expected: 95.45%)")
print(f"Data within μ ± 3σ: {within_3sigma*100:.2f}% (Expected: 99.73%)")
print("\nNORMALITY TESTS:")
print("-"*40)
sample_size_shapiro = min(5000, len(analysis_data))
sample_data_shapiro = np.random.choice(analysis_data, sample_size_shapiro, replace=False)
stat_shapiro, p_shapiro = shapiro(sample_data_shapiro)
print(f"Shapiro-Wilk Test (n={sample_size_shapiro}):")
print(f"  Test Statistic = {stat_shapiro:.4f}, p-value = {p_shapiro:.6f}")
if p_shapiro > 0.05:
    print("  → Cannot reject normality (p > 0.05)")
else:
    print("  → Reject normality (p ≤ 0.05)")
stat_ks, p_ks = kstest(analysis_data, 'norm', args=(mean_val, std_val))
print(f"\nKolmogorov-Smirnov Test:")
print(f"  Test Statistic = {stat_ks:.4f}, p-value = {p_ks:.6f}")
if p_ks > 0.05:
    print("  → Cannot reject normality (p > 0.05)")
else:
    print("  → Reject normality (p ≤ 0.05)")
print("\nPROBABILITY CALCULATIONS:")
print("-"*40)
print(f"P(X > μ) = P(X > {mean_val:.2f}) = {1 - norm.cdf(mean_val, mean_val, std_val):.4f} or 50%")
print(f"P(μ - σ < X < μ + σ) = P({mean_val-std_val:.2f} < X < {mean_val+std_val:.2f}) = {within_1sigma:.4f} or {within_1sigma*100:.2f}%")
print(f"P(X < μ - 2σ) = P(X < {mean_val-2*std_val:.2f}) = {norm.cdf(mean_val-2*std_val, mean_val, std_val):.6f} or {norm.cdf(mean_val-2*std_val, mean_val, std_val)*100:.4f}%")

print("7. VISUALIZATIONS")
print("="*80)
fig = plt.figure(figsize=(20, 15))
ax1 = plt.subplot(3, 3, 1)
soil_crop_probs = pd.crosstab(df['Soil_Type'], df['Crop'], normalize='index')
soil_crop_probs.plot(kind='bar', stacked=True, ax=ax1, colormap='viridis')
ax1.set_title('Conditional Probabilities: P(Crop | Soil)')
ax1.set_xlabel('Soil Type')
ax1.set_ylabel('Probability')
ax1.legend(title='Crop', bbox_to_anchor=(1.05, 1), loc='upper left')
ax1.tick_params(axis='x', rotation=45)
ax2 = plt.subplot(3, 3, 2)
observed_expected = pd.DataFrame({
    'Observed': soil_crop_ct.values.flatten(),
    'Expected': expected_soil_crop.flatten()
}, index=[f'{s}-{c}' for s in soil_crop_ct.index for c in soil_crop_ct.columns])
observed_expected.plot(kind='bar', ax=ax2, color=['blue', 'orange'])
ax2.set_title('Observed vs Expected Frequencies')
ax2.set_xlabel('Soil-Crop Combinations')
ax2.set_ylabel('Frequency')
ax2.tick_params(axis='x', rotation=90)
ax2.legend()
ax3 = plt.subplot(3, 3, 3)
prior_posterior = pd.DataFrame({
    'Probability': [p_high_yield, p_high_given_sunny],
    'Type': ['Prior P(High Yield)', 'Posterior P(High Yield|Sunny)'] })
prior_posterior.plot(kind='bar', x='Type', y='Probability', ax=ax3, legend=False, color=['skyblue', 'lightcoral'])
ax3.set_title('Bayesian Update: Prior vs Posterior')
ax3.set_ylabel('Probability')
ax3.tick_params(axis='x', rotation=45)
for i, v in enumerate([p_high_yield, p_high_given_sunny]):
    ax3.text(i, v + 0.01, f'{v:.3f}', ha='center')
ax4 = plt.subplot(3, 3, 4)
ax4.hist(analysis_data, bins=50, density=True, alpha=0.6, color='green', edgecolor='black', label='Data')
x = np.linspace(mean_val - 4*std_val, mean_val + 4*std_val, 1000)
y = norm.pdf(x, mean_val, std_val)
ax4.plot(x, y, 'r-', linewidth=2, label='Normal Distribution')
ax4.axvline(mean_val, color='red', linestyle='--', alpha=0.5, label=f'Mean = {mean_val:.2f}')
ax4.axvspan(mean_val - std_val, mean_val + std_val, alpha=0.2, color='gray', label='μ ± σ')
ax4.set_xlabel('Value')
ax4.set_ylabel('Density')
ax4.set_title('Histogram with Normal Distribution Overlay')
ax4.legend()
ax4.grid(True, alpha=0.3)
ax5 = plt.subplot(3, 3, 5)
stats.probplot(analysis_data, dist="norm", plot=ax5)
ax5.set_title('Q-Q Plot (vs Normal Distribution)')
ax7 = plt.subplot(3, 3, 7)
x = np.linspace(mean_val - 4*std_val, mean_val + 4*std_val, 1000)
y = norm.pdf(x, mean_val, std_val)
ax7.plot(x, y, 'b-', linewidth=2, label='Normal Distribution')
ax7.fill_between(x, 0, y, where=(x >= mean_val - std_val) & (x <= mean_val + std_val),
                 color='green', alpha=0.3, label=f'μ ± σ ({within_1sigma*100:.1f}%)')
ax7.fill_between(x, 0, y, where=(x >= mean_val - 2*std_val) & (x <= mean_val + 2*std_val),
                 color='yellow', alpha=0.3, label=f'μ ± 2σ ({within_2sigma*100:.1f}%)')
ax7.fill_between(x, 0, y, where=(x >= mean_val - 3*std_val) & (x <= mean_val + 3*std_val),
                 color='red', alpha=0.3, label=f'μ ± 3σ ({within_3sigma*100:.1f}%)')
ax7.axvline(mean_val, color='red', linestyle='--', alpha=0.5, label=f'Mean = {mean_val:.2f}')
ax7.set_xlabel('Value')
ax7.set_ylabel('Density')
ax7.set_title('Empirical Rule Visualization')
ax7.legend()
ax7.grid(True, alpha=0.3)
ax8 = plt.subplot(3, 3, 8)
x_norm = np.linspace(np.min(analysis_data), np.max(analysis_data), 1000)
y_norm = norm.cdf(x_norm, mean_val, std_val)
ax8.plot(x_norm, y_norm, 'r--', linewidth=2, label='Normal CDF')
ax8.set_xlabel('Value')
ax8.set_ylabel('Cumulative Probability')
ax8.set_title('Cumulative Distribution Functions')
ax8.legend()
ax8.grid(True, alpha=0.3)
# 9. Summary Statistics
ax9 = plt.subplot(3, 3, 9)
ax9.axis('off')
summary_text = f"""
NORMAL DISTRIBUTION SUMMARY
{'='*30}
Mean (μ) = {mean_val:.4f}
Std Dev (σ) = {std_val:.4f}
Skewness = {skew_val:.4f}
Kurtosis = {kurt_val:.4f}

EMPIRICAL RULE
{'='*30}
Within μ ± σ: {within_1sigma*100:.2f}%
Within μ ± 2σ: {within_2sigma*100:.2f}%
Within μ ± 3σ: {within_3sigma*100:.2f}%

NORMALITY TESTS
{'='*30}
Shapiro-Wilk: p = {p_shapiro:.6f}
KS Test: p = {p_ks:.6f}
"""
ax9.text(0.1, 0.95, summary_text, transform=ax9.transAxes, fontsize=10,
        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
plt.suptitle('Comprehensive Probability and Distribution Analysis', fontsize=16, y=1.02)
plt.tight_layout()
plt.show()

"""**B. Knowledge Points**

**1. Conditional Probability**
"""

# Cell 3: Basic Statistics
print("Dataset Info:")
print(df.info())
print("\nDescriptive Statistics:")
print(df.describe())
print("\nMissing Values:")
print(df.isnull().sum())

# Cell 4: Define High Yield
# Define high yield as top 30% of yields
high_yield_threshold = df['Yield_tons_per_hectare'].quantile(0.70)
df['High_Yield'] = df['Yield_tons_per_hectare'] >= high_yield_threshold

print(f"High Yield Threshold: {high_yield_threshold:.2f} tons/hectare")
print(f"Percentage of High Yield cases: {df['High_Yield'].mean()*100:.2f}%")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646198956255336,False,True,Rainy,140,3.7072931271974823"""
lines = data.strip().split('\n')
header = lines[0].split(',')
rows = [line.split(',') for line in lines[1:]]
df = pd.DataFrame(rows, columns=header)
df['Rainfall_mm'] = pd.to_numeric(df['Rainfall_mm'])
df['Temperature_Celsius'] = pd.to_numeric(df['Temperature_Celsius'])
df['Days_to_Harvest'] = pd.to_numeric(df['Days_to_Harvest'])
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
df['Fertilizer_Used'] = df['Fertilizer_Used'].map({'True': True, 'False': False})
df['Irrigation_Used'] = df['Irrigation_Used'].map({'True': True, 'False': False})
print("DATA LOADED SUCCESSFULLY!")
print(f"Total Records: {len(df)}")
print(f"Total Columns: {len(df.columns)}")
print("\nFirst 5 Rows:")
print(df.head())
print("-" * 50)
average_yield = df['Yield_tons_per_hectare'].mean()
df['High_Yield'] = df['Yield_tons_per_hectare'] > average_yield
print("YIELD ANALYSIS")
print(f"Average Yield: {average_yield:.2f} tons/hectare")
print(f"High Yield Cases: {df['High_Yield'].sum()} ({df['High_Yield'].mean()*100:.0f}%)")
print(f"Low Yield Cases: {len(df) - df['High_Yield'].sum()} ({100 - df['High_Yield'].mean()*100:.0f}%)")
print("-" * 50)
def simple_conditional_probability(condition_column, condition_value):
    """
    Simple function to calculate P(High Yield | Condition)
    Formula: P(A|B) = Number of (A and B) / Number of B
    """
    total_with_condition = len(df[df[condition_column] == condition_value])
    both_cases = len(df[(df[condition_column] == condition_value) & (df['High_Yield'] == True)])
    if total_with_condition == 0:
        return 0
    return both_cases / total_with_condition

print("\nREGION-WISE PROBABILITIES:")
print("-" * 40)
for region in df['Region'].unique():
    prob = simple_conditional_probability('Region', region)
    print(f"P(High Yield | Region = {region}) = {prob:.2f} ({prob*100:.0f}%)")
print("\nWEATHER-WISE PROBABILITIES:")
print("-" * 40)
for weather in df['Weather_Condition'].unique():
    prob = simple_conditional_probability('Weather_Condition', weather)
    print(f"P(High Yield | Weather = {weather}) = {prob:.2f} ({prob*100:.0f}%)")

print("\nFERTILIZER IMPACT:")
print("-" * 40)
for fertilizer in [True, False]:
    prob = simple_conditional_probability('Fertilizer_Used', fertilizer)
    status = "Used" if fertilizer else "Not Used"
    print(f"P(High Yield | Fertilizer {status}) = {prob:.2f} ({prob*100:.0f}%)")
print("\nCROP-WISE PROBABILITIES:")
print("-" * 40)
for crop in df['Crop'].unique():
    prob = simple_conditional_probability('Crop', crop)
    print(f"P(High Yield | Crop = {crop}) = {prob:.2f} ({prob*100:.0f}%)")
print("\nSOIL TYPE PROBABILITIES:")
print("-" * 40)
for soil in df['Soil_Type'].unique():
    prob = simple_conditional_probability('Soil_Type', soil)
    print(f"P(High Yield | Soil = {soil}) = {prob:.2f} ({prob*100:.0f}%)")
plt.figure(figsize=(12, 8))
plt.subplot(2, 3, 1)
regions = df['Region'].unique()
region_probs = [simple_conditional_probability('Region', r) for r in regions]
bars1 = plt.bar(regions, region_probs, color=['#3498db', '#2ecc71', '#e74c3c', '#f39c12'])
plt.title('P(High Yield | Region)', fontsize=12, fontweight='bold')
plt.ylabel('Probability')
plt.ylim(0, 1.1)
for bar, prob in zip(bars1, region_probs):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
             f'{prob:.2f}', ha='center', va='bottom')
plt.subplot(2, 3, 2)
weathers = df['Weather_Condition'].unique()
weather_probs = [simple_conditional_probability('Weather_Condition', w) for w in weathers]
bars2 = plt.bar(weathers, weather_probs, color=['#f1c40f', '#3498db', '#95a5a6'])
plt.title('P(High Yield | Weather)', fontsize=12, fontweight='bold')
plt.ylabel('Probability')
plt.ylim(0, 1.1)
for bar, prob in zip(bars2, weather_probs):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
             f'{prob:.2f}', ha='center', va='bottom')
plt.subplot(2, 3, 3)
fertilizer_labels = ['No Fertilizer', 'Fertilizer Used']
fert_probs = [
    simple_conditional_probability('Fertilizer_Used', False),
    simple_conditional_probability('Fertilizer_Used', True)
]
bars3 = plt.bar(fertilizer_labels, fert_probs, color=['#e74c3c', '#2ecc71'])
plt.title('P(High Yield | Fertilizer)', fontsize=12, fontweight='bold')
plt.ylabel('Probability')
plt.ylim(0, 1.1)
for bar, prob in zip(bars3, fert_probs):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
             f'{prob:.2f}', ha='center', va='bottom')
plt.subplot(2, 3, 4)
crops = df['Crop'].unique()
crop_probs = [simple_conditional_probability('Crop', c) for c in crops]
bars4 = plt.bar(crops, crop_probs, color=['#9b59b6', '#1abc9c', '#34495e', '#e74c3c', '#f39c12', '#2980b9'])
plt.title('P(High Yield | Crop)', fontsize=12, fontweight='bold')
plt.ylabel('Probability')
plt.ylim(0, 1.1)
plt.xticks(rotation=45)
for bar, prob in zip(bars4, crop_probs):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
             f'{prob:.2f}', ha='center', va='bottom')
plt.subplot(2, 3, 5)
soils = df['Soil_Type'].unique()
soil_probs = [simple_conditional_probability('Soil_Type', s) for s in soils]
bars5 = plt.bar(soils, soil_probs, color=['#e67e22', '#27ae60', '#8e44ad', '#d35400', '#16a085', '#c0392b'])
plt.title('P(High Yield | Soil Type)', fontsize=12, fontweight='bold')
plt.ylabel('Probability')
plt.ylim(0, 1.1)
plt.xticks(rotation=45)
for bar, prob in zip(bars5, soil_probs):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
             f'{prob:.2f}', ha='center', va='bottom')
plt.subplot(2, 3, 6)
irrigation_labels = ['No Irrigation', 'Irrigation Used']
irrigation_probs = [
    simple_conditional_probability('Irrigation_Used', False),
    simple_conditional_probability('Irrigation_Used', True)
]
bars6 = plt.bar(irrigation_labels, irrigation_probs, color=['#e74c3c', '#2ecc71'])
plt.title('P(High Yield | Irrigation)', fontsize=12, fontweight='bold')
plt.ylabel('Probability')
plt.ylim(0, 1.1)
for bar, prob in zip(bars6, irrigation_probs):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
             f'{prob:.2f}', ha='center', va='bottom')

plt.suptitle('CONDITIONAL PROBABILITY ANALYSIS: P(High Yield | Condition)',
             fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()

print("\nREATING COMPARISON CHART...")
print("-" * 50)
plt.figure(figsize=(14, 8))
all_labels = []
all_probs = []
for region in df['Region'].unique():
    all_labels.append(f"Region: {region}")
    all_probs.append(simple_conditional_probability('Region', region))
for weather in df['Weather_Condition'].unique():
    all_labels.append(f"Weather: {weather}")
    all_probs.append(simple_conditional_probability('Weather_Condition', weather))
all_labels.append("Fertilizer: No")
all_probs.append(simple_conditional_probability('Fertilizer_Used', False))
all_labels.append("Fertilizer: Yes")
all_probs.append(simple_conditional_probability('Fertilizer_Used', True))
all_labels.append("Irrigation: No")
all_probs.append(simple_conditional_probability('Irrigation_Used', False))
all_labels.append("Irrigation: Yes")
all_probs.append(simple_conditional_probability('Irrigation_Used', True))
for crop in df['Crop'].unique():
    all_labels.append(f"Crop: {crop}")
    all_probs.append(simple_conditional_probability('Crop', crop))
for soil in df['Soil_Type'].unique():
    all_labels.append(f"Soil: {soil}")
    all_probs.append(simple_conditional_probability('Soil_Type', soil))
sorted_indices = np.argsort(all_probs)[::-1]  # Descending order
sorted_labels = [all_labels[i] for i in sorted_indices]
sorted_probs = [all_probs[i] for i in sorted_indices]
bars = plt.barh(sorted_labels, sorted_probs, color=plt.cm.viridis(np.linspace(0.2, 0.9, len(sorted_labels))))
for i, (bar, prob) in enumerate(zip(bars, sorted_probs)):
    plt.text(prob + 0.02, bar.get_y() + bar.get_height()/2,
             f'{prob:.2f} ({prob*100:.0f}%)',
             va='center', fontsize=10)

plt.title('All Conditional Probabilities: P(High Yield | Condition)', fontsize=16, fontweight='bold')
plt.xlabel('Probability')
plt.xlim(0, 1.2)
plt.axvline(x=0.5, color='red', linestyle='--', alpha=0.5, label='50% Chance')
plt.legend()
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()
print("\nADDITIONAL NUMERICAL ANALYSIS")
print("-" * 40)
print("Average Yield by Region:")
for region in df['Region'].unique():
    avg = df[df['Region'] == region]['Yield_tons_per_hectare'].mean()
    print(f"  {region}: {avg:.2f} tons/hectare")
print("\nAverage Yield by Weather:")
for weather in df['Weather_Condition'].unique():
    avg = df[df['Weather_Condition'] == weather]['Yield_tons_per_hectare'].mean()
    print(f"  {weather}: {avg:.2f} tons/hectare")
print("\nAverage Yield by Fertilizer Usage:")
for fert in [True, False]:
    avg = df[df['Fertilizer_Used'] == fert]['Yield_tons_per_hectare'].mean()
    status = "Used" if fert else "Not Used"
    print(f"  Fertilizer {status}: {avg:.2f} tons/hectare")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from matplotlib import rcParams
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
rcParams['font.size'] = 10
rcParams['axes.titlesize'] = 12
rcParams['axes.titleweight'] = 'bold'
data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646198956255336,False,True,Rainy,140,3.7072931271974823"""
lines = data.strip().split('\n')
header = lines[0].split(',')
rows = [line.split(',') for line in lines[1:]]
df = pd.DataFrame(rows, columns=header)
df['Rainfall_mm'] = pd.to_numeric(df['Rainfall_mm'])
df['Temperature_Celsius'] = pd.to_numeric(df['Temperature_Celsius'])
df['Days_to_Harvest'] = pd.to_numeric(df['Days_to_Harvest'])
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
df['Fertilizer_Used'] = df['Fertilizer_Used'].map({'True': True, 'False': False})
df['Irrigation_Used'] = df['Irrigation_Used'].map({'True': True, 'False': False})
print("\nDATA SUMMARY:")
print(f"Total Records: {len(df)}")
print(f"Total Variables: {len(df.columns)}")
average_yield = df['Yield_tons_per_hectare'].mean()
df['High_Yield'] = df['Yield_tons_per_hectare'] > average_yield
print(f"\nHIGH YIELD THRESHOLD:")
print(f"Average Yield: {average_yield:.2f} tons/hectare")
print(f"High Yield Cases: {df['High_Yield'].sum()} out of {len(df)}")
print(f"High Yield Rate: {df['High_Yield'].mean()*100:.1f}%")
print("MATHEMATICAL FORMULA:")
print("="*60)
print("Conditional Probability: P(A|B) = P(A ∩ B) / P(B)")
print("Where:")
print("  P(A|B) = Probability of event A given event B has occurred")
print("  P(A ∩ B) = Probability of both A and B occurring")
print("  P(B) = Probability of event B")
print("\nIn our case:")
print("  A = High Yield (Yield > Average)")
print("  B = Specific Condition (e.g., Region='South', Weather='Rainy')")
print("CALCULATION EXAMPLES:")
print("="*60)
def calculate_conditional_probability(condition_column, condition_value, verbose=True):
    """
    Calculate conditional probability with step-by-step explanation
    P(High_Yield | Condition) = P(High_Yield ∩ Condition) / P(Condition)
    """
    total_samples = len(df)
    samples_with_B = len(df[df[condition_column] == condition_value])
    P_B = samples_with_B / total_samples
    samples_with_A_and_B = len(df[(df[condition_column] == condition_value) & (df['High_Yield'] == True)])
    P_A_and_B = samples_with_A_and_B / total_samples
    if samples_with_B > 0:
        P_A_given_B = samples_with_A_and_B / samples_with_B
    else:
        P_A_given_B = 0
    if verbose:
        print(f"\n🔍 Calculating: P(High_Yield | {condition_column} = {condition_value})")
        print("-" * 40)
        print(f"Total Samples (N): {total_samples}")
        print(f"Samples with {condition_column} = {condition_value}: {samples_with_B}")
        print(f"P(B) = P({condition_column} = {condition_value}) = {samples_with_B}/{total_samples} = {P_B:.3f}")
        print(f"Samples with High_Yield AND {condition_column} = {condition_value}: {samples_with_A_and_B}")
        print(f"P(A ∩ B) = {samples_with_A_and_B}/{total_samples} = {P_A_and_B:.3f}")
        print(f"\nP(A|B) = P(A ∩ B) / P(B) = {P_A_and_B:.3f} / {P_B:.3f}")
        print(f"       = {samples_with_A_and_B} / {samples_with_B}")
        print(f"       = {P_A_given_B:.3f} ({P_A_given_B*100:.1f}%)")
        print("-" * 40)

    return P_A_given_B
print("\nEXAMPLE 1: South Region")
_ = calculate_conditional_probability('Region', 'South')
print("\nEXAMPLE 2: Fertilizer Used")
_ = calculate_conditional_probability('Fertilizer_Used', True)
print("ALL CONDITIONAL PROBABILITIES:")
print("="*60)
def quick_conditional_probability(condition_column, condition_value):
    total_with_condition = len(df[df[condition_column] == condition_value])
    both_cases = len(df[(df[condition_column] == condition_value) & (df['High_Yield'] == True)])
    return both_cases / total_with_condition if total_with_condition > 0 else 0
print("\nREGION-WISE:")
for region in df['Region'].unique():
    prob = quick_conditional_probability('Region', region)
    count = len(df[df['Region'] == region])
    high_yield_count = len(df[(df['Region'] == region) & (df['High_Yield'] == True)])
    print(f"  P(High_Yield | Region = {region}) = {high_yield_count}/{count} = {prob:.2f} ({prob*100:.0f}%)")
print("\nWEATHER-WISE:")
for weather in df['Weather_Condition'].unique():
    prob = quick_conditional_probability('Weather_Condition', weather)
    count = len(df[df['Weather_Condition'] == weather])
    high_yield_count = len(df[(df['Weather_Condition'] == weather) & (df['High_Yield'] == True)])
    print(f"  P(High_Yield | Weather = {weather}) = {high_yield_count}/{count} = {prob:.2f} ({prob*100:.0f}%)")
print("\nFERTILIZER IMPACT:")
for fertilizer in [True, False]:
    prob = quick_conditional_probability('Fertilizer_Used', fertilizer)
    count = len(df[df['Fertilizer_Used'] == fertilizer])
    high_yield_count = len(df[(df['Fertilizer_Used'] == fertilizer) & (df['High_Yield'] == True)])
    status = "Used" if fertilizer else "Not Used"
    print(f"  P(High_Yield | Fertilizer {status}) = {high_yield_count}/{count} = {prob:.2f} ({prob*100:.0f}%)")
print("\nCROP-WISE:")
for crop in df['Crop'].unique():
    prob = quick_conditional_probability('Crop', crop)
    count = len(df[df['Crop'] == crop])
    high_yield_count = len(df[(df['Crop'] == crop) & (df['High_Yield'] == True)])
    print(f"  P(High_Yield | Crop = {crop}) = {high_yield_count}/{count} = {prob:.2f} ({prob*100:.0f}%)")
fig = plt.figure(figsize=(14, 10))
fig.suptitle('Conditional Probability Analysis: P(High Yield | Condition)\n\n' +
             'Mathematical Formula: $P(A|B) = \\frac{P(A \\cap B)}{P(B)}$\n' +
             'Where A = High Yield, B = Specific Condition',
             fontsize=14, fontweight='bold', y=1.02)
ax1 = plt.subplot(2, 3, 1)
regions = df['Region'].unique()
region_probs = [quick_conditional_probability('Region', r) for r in regions]
bars1 = ax1.bar(regions, region_probs, color=['#3498db', '#2ecc71', '#e74c3c'])
ax1.set_title('$P(High\\ Yield\ |\ Region)$', fontsize=11, fontweight='bold')
ax1.set_ylabel('Probability')
ax1.set_ylim(0, 1.2)
for bar, prob, region in zip(bars1, region_probs, regions):
    count = len(df[df['Region'] == region])
    high_count = len(df[(df['Region'] == region) & (df['High_Yield'] == True)])
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.03,
             f'{high_count}/{count}\n={prob:.2f}', ha='center', va='bottom', fontsize=9)
ax1.axhline(y=df['High_Yield'].mean(), color='red', linestyle='--', alpha=0.5,
            label=f'Overall: {df["High_Yield"].mean():.2f}')
ax1.legend(loc='upper right', fontsize=8)
ax2 = plt.subplot(2, 3, 2)
weathers = df['Weather_Condition'].unique()
weather_probs = [quick_conditional_probability('Weather_Condition', w) for w in weathers]
bars2 = ax2.bar(weathers, weather_probs, color=['#f1c40f', '#3498db', '#95a5a6'])
ax2.set_title('$P(High\\ Yield\ |\ Weather)$', fontsize=11, fontweight='bold')
ax2.set_ylabel('Probability')
ax2.set_ylim(0, 1.2)
for bar, prob, weather in zip(bars2, weather_probs, weathers):
    count = len(df[df['Weather_Condition'] == weather])
    high_count = len(df[(df['Weather_Condition'] == weather) & (df['High_Yield'] == True)])
    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.03,
             f'{high_count}/{count}\n={prob:.2f}', ha='center', va='bottom', fontsize=9)
ax3 = plt.subplot(2, 3, 3)
fertilizer_labels = ['No Fertilizer', 'Fertilizer Used']
fert_probs = [
    quick_conditional_probability('Fertilizer_Used', False),
    quick_conditional_probability('Fertilizer_Used', True)
]
bars3 = ax3.bar(fertilizer_labels, fert_probs, color=['#e74c3c', '#2ecc71'])
ax3.set_title('$P(High\\ Yield\ |\ Fertilizer)$', fontsize=11, fontweight='bold')
ax3.set_ylabel('Probability')
ax3.set_ylim(0, 1.2)
for bar, prob, label in zip(bars3, fert_probs, fertilizer_labels):
    count = len(df[df['Fertilizer_Used'] == (label == 'Fertilizer Used')])
    high_count = len(df[(df['Fertilizer_Used'] == (label == 'Fertilizer Used')) & (df['High_Yield'] == True)])
    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.03,
             f'{high_count}/{count}\n={prob:.2f}', ha='center', va='bottom', fontsize=9)
ax4 = plt.subplot(2, 3, 4)
crops = df['Crop'].unique()
crop_probs = [quick_conditional_probability('Crop', c) for c in crops]
bars4 = ax4.bar(crops, crop_probs, color=['#9b59b6', '#1abc9c', '#34495e', '#e74c3c', '#f39c12'])
ax4.set_title('$P(High\\ Yield\ |\ Crop)$', fontsize=11, fontweight='bold')
ax4.set_ylabel('Probability')
ax4.set_ylim(0, 1.2)
ax4.tick_params(axis='x', rotation=45)
for bar, prob, crop in zip(bars4, crop_probs, crops):
    count = len(df[df['Crop'] == crop])
    high_count = len(df[(df['Crop'] == crop) & (df['High_Yield'] == True)])
    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.03,
             f'{high_count}/{count}\n={prob:.2f}', ha='center', va='bottom', fontsize=8)
ax5 = plt.subplot(2, 3, 5)
soils = df['Soil_Type'].unique()
soil_probs = [quick_conditional_probability('Soil_Type', s) for s in soils]
bars5 = ax5.bar(soils, soil_probs, color=['#e67e22', '#27ae60', '#8e44ad', '#d35400'])
ax5.set_title('$P(High\\ Yield\ |\ Soil\\ Type)$', fontsize=11, fontweight='bold')
ax5.set_ylabel('Probability')
ax5.set_ylim(0, 1.2)
ax5.tick_params(axis='x', rotation=45)
for bar, prob, soil in zip(bars5, soil_probs, soils):
    count = len(df[df['Soil_Type'] == soil])
    high_count = len(df[(df['Soil_Type'] == soil) & (df['High_Yield'] == True)])
    ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.03,
             f'{high_count}/{count}\n={prob:.2f}', ha='center', va='bottom', fontsize=8)
ax6 = plt.subplot(2, 3, 6)
irrigation_labels = ['No Irrigation', 'Irrigation Used']
irrigation_probs = [
    quick_conditional_probability('Irrigation_Used', False),
    quick_conditional_probability('Irrigation_Used', True)
]
bars6 = ax6.bar(irrigation_labels, irrigation_probs, color=['#e74c3c', '#2ecc71'])
ax6.set_title('$P(High\\ Yield\ |\ Irrigation)$', fontsize=11, fontweight='bold')
ax6.set_ylabel('Probability')
ax6.set_ylim(0, 1.2)
for bar, prob, label in zip(bars6, irrigation_probs, irrigation_labels):
    count = len(df[df['Irrigation_Used'] == (label == 'Irrigation Used')])
    high_count = len(df[(df['Irrigation_Used'] == (label == 'Irrigation Used')) & (df['High_Yield'] == True)])
    ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.03,
             f'{high_count}/{count}\n={prob:.2f}', ha='center', va='bottom', fontsize=9)
plt.tight_layout()
plt.show()
print("PROBABILITY COMPARISON CHART")
print("="*60)
plt.figure(figsize=(14, 8))
all_labels = []
all_probs = []
for region in df['Region'].unique():
    prob = quick_conditional_probability('Region', region)
    count = len(df[df['Region'] == region])
    high_count = len(df[(df['Region'] == region) & (df['High_Yield'] == True)])
    all_labels.append(f"Region: {region} ({high_count}/{count})")
    all_probs.append(prob)
for weather in df['Weather_Condition'].unique():
    prob = quick_conditional_probability('Weather_Condition', weather)
    count = len(df[df['Weather_Condition'] == weather])
    high_count = len(df[(df['Weather_Condition'] == weather) & (df['High_Yield'] == True)])
    all_labels.append(f"Weather: {weather} ({high_count}/{count})")
    all_probs.append(prob)
for fertilizer in [True, False]:
    prob = quick_conditional_probability('Fertilizer_Used', fertilizer)
    count = len(df[df['Fertilizer_Used'] == fertilizer])
    high_count = len(df[(df['Fertilizer_Used'] == fertilizer) & (df['High_Yield'] == True)])
    status = "Used" if fertilizer else "Not Used"
    all_labels.append(f"Fertilizer: {status} ({high_count}/{count})")
    all_probs.append(prob)
for crop in df['Crop'].unique():
    prob = quick_conditional_probability('Crop', crop)
    count = len(df[df['Crop'] == crop])
    high_count = len(df[(df['Crop'] == crop) & (df['High_Yield'] == True)])
    all_labels.append(f"Crop: {crop} ({high_count}/{count})")
    all_probs.append(prob)
for soil in df['Soil_Type'].unique():
    prob = quick_conditional_probability('Soil_Type', soil)
    count = len(df[df['Soil_Type'] == soil])
    high_count = len(df[(df['Soil_Type'] == soil) & (df['High_Yield'] == True)])
    all_labels.append(f"Soil: {soil} ({high_count}/{count})")
    all_probs.append(prob)
sorted_indices = np.argsort(all_probs)[::-1]
sorted_labels = [all_labels[i] for i in sorted_indices]
sorted_probs = [all_probs[i] for i in sorted_indices]
bars = plt.barh(sorted_labels, sorted_probs, color=plt.cm.viridis(np.linspace(0.2, 0.9, len(sorted_labels))))
for bar, prob in zip(bars, sorted_probs):
    plt.text(prob + 0.02, bar.get_y() + bar.get_height()/2,
             f'{prob:.2f}', va='center', fontsize=10)
plt.title('All Conditional Probabilities: P(High Yield | Condition)\n' +
          'Values show: Probability (High Yield Cases / Total Cases)',
          fontsize=14, fontweight='bold')
plt.xlabel('Probability')
plt.xlim(0, 1.2)
plt.axvline(x=df['High_Yield'].mean(), color='red', linestyle='--',
            alpha=0.7, label=f'Overall Average: {df["High_Yield"].mean():.2f}')
plt.axvline(x=0.5, color='gray', linestyle=':', alpha=0.5)
plt.legend()
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.show()
print("STATISTICAL SUMMARY")
print("="*60)
print("\nCONDITION WITH HIGHEST PROBABILITY:")
max_prob_idx = np.argmax(all_probs)
print(f"  {sorted_labels[0].split('(')[0].strip()}")
print(f"  P = {sorted_probs[0]:.3f} ({sorted_probs[0]*100:.1f}%)")
print("\nCONDITION WITH LOWEST PROBABILITY:")
min_prob_idx = np.argmin(all_probs)
print(f"  {sorted_labels[-1].split('(')[0].strip()}")
print(f"  P = {sorted_probs[-1]:.3f} ({sorted_probs[-1]*100:.1f}%)")
print("\nCONDITIONS ABOVE OVERALL AVERAGE:")
overall_avg = df['High_Yield'].mean()
above_avg = [(label, prob) for label, prob in zip(all_labels, all_probs) if prob > overall_avg]
for label, prob in above_avg:
    print(f"  {label.split('(')[0].strip()}: {prob:.3f}")
print("INTERPRETATION:")
print("="*60)
print("""
1. P(High Yield | Condition) > Overall Average (0.50):
   - The condition increases the likelihood of high yield
2. P(High Yield | Condition) < Overall Average (0.50):
   - The condition decreases the likelihood of high yield
3. P(High Yield | Condition) = 1.00:
   - All cases with this condition resulted in high yield
4. P(High Yield | Condition) = 0.00:
   - No cases with this condition resulted in high yield
""")
print("RECOMMENDATIONS BASED ON ANALYSIS:")
print("="*60)
print("""
1. FOCUS ON CONDITIONS WITH HIGH PROBABILITY:
   - Prioritize conditions that show P > 0.50
2. AVOID CONDITIONS WITH LOW PROBABILITY:
   - Minimize conditions that show P < 0.50
3. CONSIDER SAMPLE SIZE:
   - Probabilities based on small samples may not be reliable
   - Look for patterns across multiple conditions
""")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646198956255336,False,True,Rainy,140,3.7072931271974823"""
df = pd.read_csv(pd.io.common.StringIO(data))
df['Rainfall_mm'] = pd.to_numeric(df['Rainfall_mm'])
df['Temperature_Celsius'] = pd.to_numeric(df['Temperature_Celsius'])
df['Days_to_Harvest'] = pd.to_numeric(df['Days_to_Harvest'])
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
print("\n**View Data (First 5 rows):**")
print(df.head())
print("\n**Total Data:**", len(df), "records")
average_yield = df['Yield_tons_per_hectare'].mean()
df['High_Yield'] = df['Yield_tons_per_hectare'] > average_yield
print(f"\n**Average Yield:** {average_yield:.2f} tons/hectare")
print(f"**High Yield Cases:** {df['High_Yield'].sum()} cases")
print(f"**Low Yield Cases:** {len(df) - df['High_Yield'].sum()} cases")
def simple_probability(condition_column, condition_value):
    """
    Formula: P(High_Yield | Condition) = (High_Yield AND Condition cases) / (All Condition cases)
    Example:
    P(High_Yield | Region='South') =
    (South region high yield cases) / (All South region cases)
    """
    all_condition_cases = df[df[condition_column] == condition_value]
    high_yield_and_condition = all_condition_cases[all_condition_cases['High_Yield'] == True]
    if len(all_condition_cases) == 0:
        return 0
    probability = len(high_yield_and_condition) / len(all_condition_cases)
    return probability
print("**High Yield Probability for Different Conditions**")
print("\n**By Region:**")
for region in df['Region'].unique():
    prob = simple_probability('Region', region)
    cases = len(df[df['Region'] == region])
    high_cases = len(df[(df['Region'] == region) & (df['High_Yield'] == True)])
    print(f"  P(High_Yield | Region = {region}) = {high_cases}/{cases} = {prob:.2f} ({prob*100:.0f}%)")
print("\n**By Weather:**")
for weather in df['Weather_Condition'].unique():
    prob = simple_probability('Weather_Condition', weather)
    cases = len(df[df['Weather_Condition'] == weather])
    high_cases = len(df[(df['Weather_Condition'] == weather) & (df['High_Yield'] == True)])
    print(f"  P(High_Yield | Weather = {weather}) = {high_cases}/{cases} = {prob:.2f}")
print("\n**Fertilizer Usage:**")
no_fertilizer_prob = simple_probability('Fertilizer_Used', False)
fertilizer_prob = simple_probability('Fertilizer_Used', True)
no_fertilizer_cases = len(df[df['Fertilizer_Used'] == False])
high_no_fertilizer = len(df[(df['Fertilizer_Used'] == False) & (df['High_Yield'] == True)])
fertilizer_cases = len(df[df['Fertilizer_Used'] == True])
high_fertilizer = len(df[(df['Fertilizer_Used'] == True) & (df['High_Yield'] == True)])
print(f"  Without Fertilizer: {high_no_fertilizer}/{no_fertilizer_cases} = {no_fertilizer_prob:.2f}")
print(f"  With Fertilizer: {high_fertilizer}/{fertilizer_cases} = {fertilizer_prob:.2f}")
print("**Creating Simple Graphs...**")
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
region_probabilities = [simple_probability('Region', r) for r in df['Region'].unique()]
bars1 = plt.bar(df['Region'].unique(), region_probabilities, color=['red', 'green', 'blue'])
plt.title('High Yield Probability by Region')
plt.ylabel('Probability')
plt.ylim(0, 1.1)
for bar, prob in zip(bars1, region_probabilities):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
             f'{prob:.2f}', ha='center', va='bottom')
plt.subplot(1, 2, 2)
weather_probabilities = [simple_probability('Weather_Condition', w) for w in df['Weather_Condition'].unique()]
bars2 = plt.bar(df['Weather_Condition'].unique(), weather_probabilities, color=['yellow', 'blue', 'gray'])
plt.title('High Yield Probability by Weather')
plt.ylabel('Probability')
plt.ylim(0, 1.1)
for bar, prob in zip(bars2, weather_probabilities):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
             f'{prob:.2f}', ha='center', va='bottom')
plt.tight_layout()
plt.show()
print("**Best and Worst Conditions**")
all_conditions = []
all_probabilities = []
for region in df['Region'].unique():
    all_conditions.append(f"Region: {region}")
    all_probabilities.append(simple_probability('Region', region))
for weather in df['Weather_Condition'].unique():
    all_conditions.append(f"Weather: {weather}")
    all_probabilities.append(simple_probability('Weather_Condition', weather))
for fertilizer in [False, True]:
    status = "No Fertilizer" if fertilizer == False else "Fertilizer Used"
    all_conditions.append(f"Fertilizer: {status}")
    all_probabilities.append(simple_probability('Fertilizer_Used', fertilizer))
for crop in df['Crop'].unique():
    all_conditions.append(f"Crop: {crop}")
    all_probabilities.append(simple_probability('Crop', crop))
max_probability = max(all_probabilities)
min_probability = min(all_probabilities)
best_condition = all_conditions[all_probabilities.index(max_probability)]
worst_condition = all_conditions[all_probabilities.index(min_probability)]
print(f"**Highest Probability:** {best_condition} = {max_probability:.2f}")
print(f"**Lowest Probability:** {worst_condition} = {min_probability:.2f}")
print("**How to Make Decisions?**")
print("""
**Simple Rules:**
1. **Probability > 0.50:** Good Condition
   - Higher chance of high yield
2. **Probability < 0.50:** Bad Condition
   - Lower chance of high yield
3. **Probability = 1.00:** Best Condition
   - All cases resulted in high yield
4. **Probability = 0.00:** Avoid
   - No cases resulted in high yield
""")
print("\n **Choose These Conditions (Probability > 0.50):**")
for condition, probability in zip(all_conditions, all_probabilities):
    if probability > 0.50:
        print(f"  - {condition}: {probability:.2f}")
print("\n**Avoid These Conditions (Probability < 0.50):**")
for condition, probability in zip(all_conditions, all_probabilities):
    if probability < 0.50:
        print(f"  - {condition}: {probability:.2f}")

"""**2. Independence of Events**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646966373377603,False,True,Rainy,140,3.7072931271974823"""
df = pd.read_csv(pd.io.common.StringIO(data))
df['Rainfall_mm'] = pd.to_numeric(df['Rainfall_mm'])
df['Temperature_Celsius'] = pd.to_numeric(df['Temperature_Celsius'])
df['Days_to_Harvest'] = pd.to_numeric(df['Days_to_Harvest'])
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
print("INDEPENDENCE CHECK: Are Events Independent or Dependent?")
print("Formula: Two events A and B are independent if P(A ∩ B) = P(A) × P(B)")
print("="*70)
average_yield = df['Yield_tons_per_hectare'].mean()
df['High_Yield'] = df['Yield_tons_per_hectare'] > average_yield
P_A = df['High_Yield'].mean()
total_cases = len(df)
high_yield_cases = df['High_Yield'].sum()
print(f"\nEVENT A: High Yield (Yield > {average_yield:.2f})")
print(f"   P(A) = {high_yield_cases}/{total_cases} = {P_A:.3f}")
def check_independence(condition_column, condition_value, event_name="B"):
    """
    Check if High_Yield (A) and Condition (B) are independent
    Formula: P(A ∩ B) = P(A) × P(B) → Independent
             Otherwise → Dependent
    """
    total = len(df)
    condition_cases = len(df[df[condition_column] == condition_value])
    P_B = condition_cases / total
    both_cases = len(df[(df[condition_column] == condition_value) & (df['High_Yield'] == True)])
    P_A_and_B = both_cases / total
    # Calculate P(A) × P(B)
    P_A_times_P_B = P_A * P_B
    # Check independence
    is_independent = np.isclose(P_A_and_B, P_A_times_P_B, rtol=1e-10, atol=1e-10)
    # Calculate difference
    difference = abs(P_A_and_B - P_A_times_P_B)
    return {
        'Condition': f"{condition_column} = {condition_value}",
        'P(A)': P_A,
        'P(B)': P_B,
        'P(A ∩ B)': P_A_and_B,
        'P(A) × P(B)': P_A_times_P_B,
        'Difference': difference,
        'Independent?': 'YES' if is_independent else '❌ NO',
        'Relationship': 'Independent' if is_independent else 'Dependent' }
print("CHECKING INDEPENDENCE FOR DIFFERENT CONDITIONS:")
print("="*70)
all_results = []
print("\nREGION CONDITIONS:")
for region in df['Region'].unique():
    result = check_independence('Region', region, f"Region={region}")
    all_results.append(result)
    print(f"\n{result['Condition']}:")
    print(f"  P(A) = P(High Yield) = {result['P(A)']:.3f}")
    print(f"  P(B) = P({result['Condition']}) = {result['P(B)']:.3f}")
    print(f"  P(A ∩ B) = {result['P(A ∩ B)']:.3f}")
    print(f"  P(A) × P(B) = {result['P(A)']:.3f} × {result['P(B)']:.3f} = {result['P(A) × P(B)']:.3f}")
    print(f"  Difference = {result['Difference']:.4f}")
    print(f"  Independent? {result['Independent?']}")
print("\nWEATHER CONDITIONS:")
for weather in df['Weather_Condition'].unique():
    result = check_independence('Weather_Condition', weather, f"Weather={weather}")
    all_results.append(result)
    print(f"\n{result['Condition']}:")
    print(f"  P(A ∩ B) = {result['P(A ∩ B)']:.3f}")
    print(f"  P(A) × P(B) = {result['P(A) × P(B)']:.3f}")
    print(f"  Independent? {result['Independent?']}")
print("\nFERTILIZER USAGE:")
for fertilizer in [True, False]:
    status = "Fertilizer Used" if fertilizer else "No Fertilizer"
    result = check_independence('Fertilizer_Used', fertilizer, status)
    all_results.append(result)
    print(f"\n{status}:")
    print(f"  P(A ∩ B) = {result['P(A ∩ B)']:.3f}")
    print(f"  P(A) × P(B) = {result['P(A) × P(B)']:.3f}")
    print(f"  Independent? {result['Independent?']}")
print("\nIRRIGATION USAGE:")
for irrigation in [True, False]:
    status = "Irrigation Used" if irrigation else "No Irrigation"
    result = check_independence('Irrigation_Used', irrigation, status)
    all_results.append(result)
    print(f"\n{status}:")
    print(f"  P(A ∩ B) = {result['P(A ∩ B)']:.3f}")
    print(f"  P(A) × P(B) = {result['P(A) × P(B)']:.3f}")
    print(f"  Independent? {result['Independent?']}")
print("\nCROP TYPES:")
for crop in df['Crop'].unique():
    result = check_independence('Crop', crop, f"Crop={crop}")
    all_results.append(result)
    print(f"\n{crop}:")
    print(f"  Independent? {result['Independent?']}")
results_df = pd.DataFrame(all_results)
print("SUMMARY OF INDEPENDENCE ANALYSIS:")
print("="*70)
print(f"\nTotal Conditions Checked: {len(results_df)}")
print(f"Independent Events: {(results_df['Independent?'] == 'YES').sum()}")
print(f"Dependent Events: {(results_df['Independent?'] == 'NO').sum()}")
# Create visualization
plt.figure(figsize=(14, 8))
# Plot 1: Comparison of P(A ∩ B) vs P(A) × P(B)
plt.subplot(2, 2, 1)
conditions = results_df['Condition'].str.slice(0, 20) + "..."
x = range(len(conditions))
plt.bar(x, results_df['P(A ∩ B)'], width=0.4, label='P(A ∩ B)', color='blue', alpha=0.7)
plt.bar([i + 0.4 for i in x], results_df['P(A) × P(B)'], width=0.4, label='P(A) × P(B)', color='red', alpha=0.7)
plt.title('Comparison: P(A ∩ B) vs P(A) × P(B)')
plt.xlabel('Conditions')
plt.ylabel('Probability')
plt.xticks([i + 0.2 for i in x], conditions, rotation=45, ha='right')
plt.legend()
plt.grid(axis='y', alpha=0.3)
# Plot 2: Differences (how far from independence)
plt.subplot(2, 2, 2)
colors = ['green' if ind == 'YES' else 'red' for ind in results_df['Independent?']]
bars = plt.bar(x, results_df['Difference'], color=colors, alpha=0.7)
plt.axhline(y=0.05, color='orange', linestyle='--', label='Threshold (0.05)')
plt.title('Difference: |P(A ∩ B) - P(A) × P(B)|')
plt.xlabel('Conditions')
plt.ylabel('Difference')
plt.xticks(x, conditions, rotation=45, ha='right')
plt.legend()
plt.grid(axis='y', alpha=0.3)
# Plot 3: Count of Independent vs Dependent
plt.subplot(2, 2, 3)
ind_vs_dep = results_df['Relationship'].value_counts()
plt.pie(ind_vs_dep.values, labels=ind_vs_dep.index, autopct='%1.1f%%',
        colors=['lightgreen', 'lightcoral'], startangle=90)
plt.title('Proportion: Independent vs Dependent Events')
# Plot 4: Scatter plot of P(A ∩ B) vs P(A) × P(B)
plt.subplot(2, 2, 4)
plt.scatter(results_df['P(A) × P(B)'], results_df['P(A ∩ B)'],
           c=['green' if ind == 'YES' else 'red' for ind in results_df['Independent?']],
           s=100, alpha=0.7)
# Add diagonal line (where P(A ∩ B) = P(A) × P(B))
min_val = min(results_df['P(A) × P(B)'].min(), results_df['P(A ∩ B)'].min())
max_val = max(results_df['P(A) × P(B)'].max(), results_df['P(A ∩ B)'].max())
plt.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5, label='Perfect Independence Line')
plt.title('Scatter: P(A ∩ B) vs P(A) × P(B)')
plt.xlabel('P(A) × P(B)')
plt.ylabel('P(A ∩ B)')
plt.legend()
plt.grid(alpha=0.3)
plt.suptitle('INDEPENDENCE ANALYSIS: Are High Yield and Other Events Independent?',
             fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()
print("INTERPRETATION GUIDE:")
print("="*70)
print("""
WHAT DOES INDEPENDENCE MEAN?
============================
Two events A and B are INDEPENDENT if:
  P(A ∩ B) = P(A) × P(B)
This means:
1. Knowing B occurred doesn't change the probability of A
2. A and B have no relationship
3. They occur by chance, not because of each other
WHAT DOES DEPENDENCE MEAN?
==========================
If P(A ∩ B) ≠ P(A) × P(B), then events are DEPENDENT:
1. Knowing B occurred CHANGES the probability of A
2. A and B are related
3. One event affects the other
IN OUR CASE:
============
Event A = High Yield (Yield > Average)
Event B = Various conditions (Region, Weather, etc.)
IF INDEPENDENT (YES):
  - The condition doesn't affect high yield probability
  - Example: If weather is rainy, high yield probability stays same
IF DEPENDENT (NO):
  - The condition AFFECTS high yield probability
  - Example: If region is South, high yield probability changes
""")
print("MOST DEPENDENT CONDITIONS (Highest Differences):")
print("="*70)
most_dependent = results_df.nlargest(5, 'Difference')
for idx, row in most_dependent.iterrows():
    print(f"\n{row['Condition']}:")
    print(f"  P(A ∩ B) = {row['P(A ∩ B)']:.3f}")
    print(f"  P(A) × P(B) = {row['P(A) × P(B)']:.3f}")
    print(f"  Difference = {row['Difference']:.3f}")
    print(f"  Status: {row['Independent?']}")
print("IMPORTANT NOTE ABOUT SMALL SAMPLE SIZE:")
print("="*70)

print(f"""
WARNING: We have only {total_cases} data points!
This is a very small sample for statistical independence testing.
With small samples:
1. Results may not be statistically significant
2. Random chance can create apparent dependence/independence
3. We need more data for reliable conclusions
For reliable independence testing, we typically need:
- At least 30-50 data points for each condition
- Better: 100+ data points
RECOMMENDATION:
- Treat these results as preliminary
- Collect more data for accurate analysis
- Use these insights as hypotheses to test with larger datasets
""")
print("PRACTICAL IMPLICATIONS:")
print("="*70)
print("""
IF DEPENDENT (NO):
=====================
TAKE ACTION: These conditions MATTER for high yield
CONSIDER: Adjust farming practices based on these conditions
EXAMPLE: If South region gives better yields, focus resources there
IF INDEPENDENT (YES):
========================
NO EFFECT: These conditions DON'T affect high yield probability
SAVE RESOURCES: Don't waste time/money on these factors
FOCUS ELSEWHERE: Look for other factors that actually matter
BOTTOM LINE:
============
Use dependency analysis to:
1. Focus on what REALLY matters for high yield
2. Avoid wasting resources on irrelevant factors
3. Make data-driven farming decisions
""")

"""**3. Bayes’ Rule**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646966373377603,False,True,Rainy,140,3.7072931271974823"""
df = pd.read_csv(pd.io.common.StringIO(data))
df['Rainfall_mm'] = pd.to_numeric(df['Rainfall_mm'])
df['Temperature_Celsius'] = pd.to_numeric(df['Temperature_Celsius'])
df['Days_to_Harvest'] = pd.to_numeric(df['Days_to_Harvest'])
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
print("BAYES' THEOREM ANALYSIS FOR AGRICULTURAL DATA")
print("Formula: P(B|A) = [P(A|B) × P(B)] / P(A)")
print("="*80)
print("\n**Dataset Overview:**")
print(f"Total records: {len(df)}")
print(df[['Region', 'Crop', 'Yield_tons_per_hectare']].head())
average_yield = df['Yield_tons_per_hectare'].mean()
df['High_Yield'] = df['Yield_tons_per_hectare'] > average_yield
P_A = df['High_Yield'].mean()
print(f"\n**Event A: High Yield (Yield > {average_yield:.2f} tons/hectare)**")
print(f"P(A) = Probability of High Yield = {df['High_Yield'].sum()}/{len(df)} = {P_A:.3f}")
def conditional_probability(event_column, event_value, given_column, given_value):
    """Calculate P(event | given)"""
    total_given = len(df[df[given_column] == given_value])
    if total_given == 0:
        return 0
    both_cases = len(df[(df[given_column] == given_value) & (df[event_column] == event_value)])
    return both_cases / total_given
# BAYES' THEOREM FUNCTION
def bayes_theorem(event_B_column, event_B_value, event_A='High_Yield'):
    """
    Calculate P(B|A) using Bayes' Theorem
    P(B|A) = [P(A|B) × P(B)] / P(A)
    """
    total_cases = len(df)
    # P(A) - Already calculated
    P_A_value = P_A
    # P(B) - Prior probability of B
    B_cases = len(df[df[event_B_column] == event_B_value])
    P_B = B_cases / total_cases
    # P(A|B) - Likelihood
    P_A_given_B = conditional_probability(event_A, True, event_B_column, event_B_value)
    # P(B|A) - Posterior probability (what we want)
    if P_A_value > 0:
        P_B_given_A = (P_A_given_B * P_B) / P_A_value
    else:
        P_B_given_A = 0
    return {
        'Event B': f"{event_B_column} = {event_B_value}",
        'P(B)': P_B,
        'P(A|B)': P_A_given_B,
        'P(B|A)': P_B_given_A,
        'Bayes Calculation': f"({P_A_given_B:.3f} × {P_B:.3f}) / {P_A_value:.3f} = {P_B_given_A:.3f}"
    }
print("MEDICAL DIAGNOSIS ANALOGY")
print("="*80)
print("""
In medical testing:
- A = Test Positive
- B = Have Disease
Bayes' Theorem: P(Disease|Positive) = [P(Positive|Disease) × P(Disease)] / P(Positive)
In our case:
- A = High Yield (what we observe)
- B = Specific condition (what we want to infer)
""")
print("\n**Example: Disease Testing Scenario**")
disease_prevalence = 0.01  # 1% of population has disease
test_sensitivity = 0.95    # P(Positive|Disease) = 95%
test_specificity = 0.90    # P(Negative|No Disease) = 90%
P_Positive = (test_sensitivity * disease_prevalence) + ((1 - test_specificity) * (1 - disease_prevalence))
P_Disease_given_Positive = (test_sensitivity * disease_prevalence) / P_Positive
print(f"Disease prevalence P(Disease): {disease_prevalence:.3f}")
print(f"Test sensitivity P(Positive|Disease): {test_sensitivity:.3f}")
print(f"Test specificity P(Negative|No Disease): {test_specificity:.3f}")
print(f"P(Positive): {P_Positive:.3f}")
print(f"\nP(Disease|Positive) = ({test_sensitivity:.3f} × {disease_prevalence:.3f}) / {P_Positive:.3f}")
print(f"                    = {P_Disease_given_Positive:.3f} ({P_Disease_given_Positive*100:.1f}%)")
print("BAYES' THEOREM APPLIED TO AGRICULTURAL DATA")
print("Question: Given that we have HIGH YIELD, what's the probability it came from a specific condition?")
bayes_results = []
print("\n**REGION ANALYSIS:**")
for region in df['Region'].unique():
    result = bayes_theorem('Region', region)
    bayes_results.append(result)
    print(f"\n{result['Event B']}:")
    print(f"  P(B) = P({region}) = {result['P(B)']:.3f}")
    print(f"  P(A|B) = P(High Yield | {region}) = {result['P(A|B)']:.3f}")
    print(f"  P(B|A) = P({region} | High Yield) = {result['P(B|A)']:.3f}")
    print(f"  Bayes: {result['Bayes Calculation']}")
print("\n**WEATHER ANALYSIS:**")
for weather in df['Weather_Condition'].unique():
    result = bayes_theorem('Weather_Condition', weather)
    bayes_results.append(result)
    print(f"\n{weather} weather:")
    print(f"  P(Weather={weather} | High Yield) = {result['P(B|A)']:.3f}")
print("\n**FERTILIZER ANALYSIS:**")
for fert in [True, False]:
    status = "Fertilizer Used" if fert else "No Fertilizer"
    result = bayes_theorem('Fertilizer_Used', fert)
    bayes_results.append(result)
    print(f"\n{status}:")
    print(f"  P({status} | High Yield) = {result['P(B|A)']:.3f}")
print("\n**IRRIGATION ANALYSIS:**")
for irrig in [True, False]:
    status = "Irrigation Used" if irrig else "No Irrigation"
    result = bayes_theorem('Irrigation_Used', irrig)
    bayes_results.append(result)
    print(f"\n{status}:")
    print(f"  P({status} | High Yield) = {result['P(B|A)']:.3f}")
print("BAYESIAN POSTERIOR PROBABILITIES VISUALIZATION")
print("="*80)
# Create DataFrame from results
bayes_df = pd.DataFrame(bayes_results)
plt.figure(figsize=(15, 10))
# Plot 1: Comparison of Prior P(B) vs Posterior P(B|A)
plt.subplot(2, 2, 1)
x = range(len(bayes_df))
width = 0.35
plt.bar([i - width/2 for i in x], bayes_df['P(B)'], width, label='Prior P(B)', color='blue', alpha=0.7)
plt.bar([i + width/2 for i in x], bayes_df['P(B|A)'], width, label='Posterior P(B|A)', color='green', alpha=0.7)
plt.axhline(y=1.0, color='red', linestyle='--', alpha=0.5, label='Probability = 1.0')
plt.title('Bayesian Update: Prior vs Posterior Probabilities')
plt.xlabel('Conditions')
plt.ylabel('Probability')
plt.xticks(x, [b[:20] + "..." for b in bayes_df['Event B']], rotation=45, ha='right')
plt.legend()
plt.grid(axis='y', alpha=0.3)
# Plot 2: Bayes Factor = P(B|A) / P(B)
plt.subplot(2, 2, 2)
bayes_df['Bayes_Factor'] = bayes_df['P(B|A)'] / bayes_df['P(B)'].replace(0, np.nan)
bayes_df['Evidence'] = ['Strong for' if bf > 3 else 'Weak for' if bf > 1 else 'Against' for bf in bayes_df['Bayes_Factor']]
colors = ['green' if bf > 1 else 'red' for bf in bayes_df['Bayes_Factor']]
bars = plt.bar(x, bayes_df['Bayes_Factor'], color=colors, alpha=0.7)
plt.axhline(y=1.0, color='black', linestyle='-', alpha=0.5, label='No change (BF=1)')
plt.axhline(y=3.0, color='orange', linestyle='--', alpha=0.5, label='Strong evidence (BF>3)')
plt.title('Bayes Factor: How much evidence for each condition?')
plt.xlabel('Conditions')
plt.ylabel('Bayes Factor = P(B|A) / P(B)')
plt.xticks(x, [b[:20] + "..." for b in bayes_df['Event B']], rotation=45, ha='right')
plt.legend()
plt.grid(axis='y', alpha=0.3)
# Plot 3: Likelihood P(A|B)
plt.subplot(2, 2, 3)
plt.bar(x, bayes_df['P(A|B)'], color='purple', alpha=0.7)
plt.axhline(y=P_A, color='red', linestyle='--', label=f'Base rate P(A) = {P_A:.2f}')
plt.title('Likelihood: P(High Yield | Condition)')
plt.xlabel('Conditions')
plt.ylabel('P(A|B)')
plt.xticks(x, [b[:20] + "..." for b in bayes_df['Event B']], rotation=45, ha='right')
plt.legend()
plt.grid(axis='y', alpha=0.3)
plt.subplot(2, 2, 4)
top_conditions = bayes_df.nlargest(8, 'P(B|A)')
top_x = range(len(top_conditions))
plt.barh(top_x, top_conditions['P(B|A)'], color='lightgreen', alpha=0.7)
plt.axvline(x=P_A, color='red', linestyle='--', label=f'Base rate P(A) = {P_A:.2f}')
for i, (_, row) in enumerate(top_conditions.iterrows()):
    plt.text(row['P(B|A)'] + 0.02, i, f'{row["P(B|A)"]:.2f}', va='center')
plt.yticks(top_x, [b[:20] + "..." for b in top_conditions['Event B']])
plt.title('Top Conditions Given High Yield (Highest P(B|A))')
plt.xlabel('P(Condition | High Yield)')
plt.legend()
plt.suptitle('Bayesian Analysis: What Conditions Lead to High Yield?', fontsize=16, fontweight='bold', y=1.02)
plt.tight_layout()
plt.show()
print("🤖 MACHINE LEARNING CLASSIFICATION APPLICATION")
print("="*80)
print("""
Naive Bayes Classifier Concept:
- We want to predict if a new farm will have HIGH YIELD
- Using Bayes' Theorem for each feature independently
""")
def naive_bayes_predict(new_farm):
    """
    Simplified Naive Bayes prediction
    P(High Yield | Features) ∝ P(High Yield) × ∏ P(Feature | High Yield)
    """
    prior = P_A
    likelihood = 1.0
    print(f"\nPredicting for new farm:")
    for feature, value in new_farm.items():
        if feature in df.columns:
            # Count how many high yield cases have this feature value
            high_yield_cases = len(df[df['High_Yield'] == True])
            feature_in_high_yield = len(df[(df[feature] == value) & (df['High_Yield'] == True)])
            if high_yield_cases > 0:
                feature_likelihood = feature_in_high_yield / high_yield_cases
            else:
                feature_likelihood = 0.001  # Small smoothing factor
            likelihood *= feature_likelihood
            print(f"  P({feature}={value} | High Yield) = {feature_in_high_yield}/{high_yield_cases} = {feature_likelihood:.3f}")
    unnormalized_posterior = prior * likelihood
    prior_low = 1 - P_A
    likelihood_low = 1.0
    for feature, value in new_farm.items():
        if feature in df.columns:
            low_yield_cases = len(df[df['High_Yield'] == False])
            feature_in_low_yield = len(df[(df[feature] == value) & (df['High_Yield'] == False)])
            if low_yield_cases > 0:
                feature_likelihood_low = feature_in_low_yield / low_yield_cases
            else:
                feature_likelihood_low = 0.001
            likelihood_low *= feature_likelihood_low
    unnormalized_posterior_low = prior_low * likelihood_low
    total = unnormalized_posterior + unnormalized_posterior_low
    if total > 0:
        prob_high_yield = unnormalized_posterior / total
    else:
        prob_high_yield = 0.5
    return prob_high_yield
print("\n" + "-" * 40)
print("EXAMPLE PREDICTION 1:")
new_farm_1 = {'Region': 'South', 'Weather_Condition': 'Rainy', 'Fertilizer_Used': True}
prediction_1 = naive_bayes_predict(new_farm_1)
print(f"\nPredicted probability of High Yield: {prediction_1:.3f} ({prediction_1*100:.1f}%)")
if prediction_1 > 0.5:
    print("Prediction: HIGH YIELD")
else:
    print("Prediction: LOW YIELD")
print("\n" + "-" * 40)
print("EXAMPLE PREDICTION 2:")
new_farm_2 = {'Region': 'North', 'Weather_Condition': 'Sunny', 'Fertilizer_Used': False}
prediction_2 = naive_bayes_predict(new_farm_2)
print(f"\nPredicted probability of High Yield: {prediction_2:.3f} ({prediction_2*100:.1f}%)")
if prediction_2 > 0.5:
    print("Prediction: HIGH YIELD")
else:
    print("Prediction: LOW YIELD")
print("SPAM FILTERING ANALOGY")
print("="*80)
print("""
In spam filtering:
- A = Email is spam
- B = Email contains certain words
Bayes' Theorem: P(Spam|Word) = [P(Word|Spam) × P(Spam)] / P(Word)
Similarly in agriculture:
- A = High Yield
- B = Certain farming conditions
""")
print("BAYESIAN INFERENCE")
print("="*80)
summary_df = bayes_df.copy()
summary_df['Prior P(B)'] = summary_df['P(B)']
summary_df['Likelihood P(A|B)'] = summary_df['P(A|B)']
summary_df['Posterior P(B|A)'] = summary_df['P(B|A)']
summary_df['Bayes Factor'] = summary_df['P(B|A)'] / summary_df['P(B)'].replace(0, np.nan)
summary_df['Evidence Strength'] = summary_df['Bayes Factor'].apply(
    lambda x: 'Strong' if x > 3 else 'Moderate' if x > 1.5 else 'Weak' if x > 1 else 'Negative'
)
print(summary_df[['Event B', 'Prior P(B)', 'Likelihood P(A|B)',
                  'Posterior P(B|A)', 'Bayes Factor', 'Evidence Strength']].to_string(index=False))
print("💡 PRACTICAL INSIGHTS FROM BAYESIAN ANALYSIS")
print("="*80)
print("""
1. **HIGH POSTERIOR PROBABILITY** (P(B|A) > P(B)):
   - Condition is MORE COMMON in high yield cases than in general
   - Example: If P(South | High Yield) > P(South), South region is associated with high yield
2. **BAYES FACTOR > 1**:
   - Evidence SUPPORTS this condition being associated with high yield
   - Higher Bayes factor = stronger evidence
3. **BAYES FACTOR < 1**:
   - Evidence AGAINST this condition being associated with high yield
4. **MACHINE LEARNING APPLICATION**:
   - We can build a classifier to predict high yield based on conditions
   - Uses Bayes' Theorem to combine evidence from multiple features
5. **DECISION MAKING**:
   - Focus resources on conditions with high P(B|A)
   - Avoid conditions with low P(B|A) unless other factors compensate
""")
max_bf = summary_df['Bayes Factor'].max()
min_bf = summary_df['Bayes Factor'].min()
best_condition = summary_df.loc[summary_df['Bayes Factor'].idxmax()]
worst_condition = summary_df.loc[summary_df['Bayes Factor'].idxmin()]
print(f"\nSTRONGEST EVIDENCE FOR HIGH YIELD:")
print(f"   Condition: {best_condition['Event B']}")
print(f"   Bayes Factor: {best_condition['Bayes Factor']:.2f}")
print(f"   P(High Yield) = {P_A:.2f}, P(High Yield | Condition) = {best_condition['Likelihood P(A|B)']:.2f}")
print(f"\nSTRONGEST EVIDENCE AGAINST HIGH YIELD:")
print(f"   Condition: {worst_condition['Event B']}")
print(f"   Bayes Factor: {worst_condition['Bayes Factor']:.2f}")
print(f"   P(High Yield) = {P_A:.2f}, P(High Yield | Condition) = {worst_condition['Likelihood P(A|B)']:.2f}")
print("\n" + "="*80)
print("RECOMMENDATIONS FOR FARMERS:")
# Generate recommendations based on Bayesian analysis
high_posterior_conditions = summary_df[summary_df['Posterior P(B|A)'] > summary_df['Prior P(B)']]
print("\nFOCUS ON THESE CONDITIONS (Associated with High Yield):")
for _, row in high_posterior_conditions.nlargest(5, 'Bayes Factor').iterrows():
    improvement = (row['Posterior P(B|A)'] - row['Prior P(B)']) * 100
    print(f"  • {row['Event B']}: {improvement:+.1f}% more likely in high yield cases")
low_posterior_conditions = summary_df[summary_df['Posterior P(B|A)'] < summary_df['Prior P(B)']]
print("\nRECONSIDER THESE CONDITIONS (Less associated with High Yield):")
for _, row in low_posterior_conditions.nsmallest(3, 'Bayes Factor').iterrows():
    reduction = (row['Prior P(B)'] - row['Posterior P(B|A)']) * 100
    print(f"  • {row['Event B']}: {reduction:.1f}% less likely in high yield cases")

"""**4. Probability Distributions**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646966373377603,False,True,Rainy,140,3.7072931271974823"""
df = pd.read_csv(pd.io.common.StringIO(data))
df['Rainfall_mm'] = pd.to_numeric(df['Rainfall_mm'])
df['Temperature_Celsius'] = pd.to_numeric(df['Temperature_Celsius'])
df['Days_to_Harvest'] = pd.to_numeric(df['Days_to_Harvest'])
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
print("""
Continuous Random Variables:
• Can take ANY value within a range (e.g., 1.5, 2.718, 3.14159...)
• Represented by probability density functions (PDFs)
• Examples: Height, Weight, Temperature, Yield
The Normal Distribution (Gaussian Distribution):
• Most important continuous distribution in statistics
• Bell-shaped curve
• Described by two parameters: Mean (μ) and Standard Deviation (σ)
• Formula: f(x) = (1/(σ√(2π))) × e^(-(x-μ)²/(2σ²))
""")
yield_data = df['Yield_tons_per_hectare']
print(f"\n**Yield Data Analysis:**")
print(f"Sample size: {len(yield_data)} observations")
print(f"Range: {yield_data.min():.2f} to {yield_data.max():.2f} tons/hectare")
mean_yield = yield_data.mean()
std_yield = yield_data.std()
median_yield = yield_data.median()
skewness = yield_data.skew()
kurtosis = yield_data.kurtosis()
print(f"\n**Descriptive Statistics for Yield:**")
print(f"Mean (μ): {mean_yield:.3f} tons/hectare")
print(f"Standard Deviation (σ): {std_yield:.3f} tons/hectare")
print(f"Median: {median_yield:.3f} tons/hectare")
print(f"Skewness: {skewness:.3f} (Positive = right-skewed, Negative = left-skewed)")
print(f"Kurtosis: {kurtosis:.3f} (>3 = heavy tails, <3 = light tails)")
print("NORMALITY TESTS AND VISUAL CHECKS")
print("="*80)
shapiro_stat, shapiro_p = stats.shapiro(yield_data)
print(f"\n🔍 **Shapiro-Wilk Normality Test:**")
print(f"Test Statistic: {shapiro_stat:.4f}")
print(f"P-value: {shapiro_p:.4f}")
if shapiro_p > 0.05:
    print("Conclusion: Data appears normally distributed (p > 0.05)")
else:
    print("Conclusion: Data does NOT appear normally distributed (p ≤ 0.05)")
# QQ-Plot for visual normality check
print("\n**Creating Visual Normality Checks...**")
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('Normality Assessment for Crop Yield', fontsize=16, fontweight='bold', y=1.02)
# Plot 1: Histogram with normal curve
axes[0, 0].hist(yield_data, bins=8, density=True, alpha=0.7, color='skyblue', edgecolor='black')
xmin, xmax = axes[0, 0].get_xlim()
x = np.linspace(xmin, xmax, 100)
pdf = stats.norm.pdf(x, mean_yield, std_yield)
axes[0, 0].plot(x, pdf, 'r-', linewidth=2, label='Normal Distribution')
axes[0, 0].axvline(mean_yield, color='green', linestyle='--', label=f'Mean = {mean_yield:.2f}')
axes[0, 0].axvline(mean_yield + std_yield, color='orange', linestyle=':', label=f'±1σ')
axes[0, 0].axvline(mean_yield - std_yield, color='orange', linestyle=':')
axes[0, 0].set_title('Histogram with Normal Curve Overlay')
axes[0, 0].set_xlabel('Yield (tons/hectare)')
axes[0, 0].set_ylabel('Density')
axes[0, 0].legend()
# Plot 2: Q-Q Plot
stats.probplot(yield_data, dist="norm", plot=axes[0, 1])
axes[0, 1].get_lines()[0].set_marker('o')
axes[0, 1].get_lines()[0].set_markersize(5)
axes[0, 1].get_lines()[0].set_markerfacecolor('blue')
axes[0, 1].get_lines()[0].set_markeredgecolor('blue')
axes[0, 1].get_lines()[1].set_color('red')
axes[0, 1].get_lines()[1].set_linewidth(2)
axes[0, 1].set_title('Q-Q Plot for Normality Check')
axes[0, 1].set_xlabel('Theoretical Quantiles')
axes[0, 1].set_ylabel('Sample Quantiles')
# Plot 3: Box plot
axes[0, 2].boxplot(yield_data, vert=True, patch_artist=True,
                   boxprops=dict(facecolor='lightblue', color='darkblue'),
                   medianprops=dict(color='red', linewidth=2))
axes[0, 2].set_title('Box Plot of Yield Data')
axes[0, 2].set_ylabel('Yield (tons/hectare)')
axes[0, 2].set_xticklabels(['Yield'])
# Plot 4: Empirical Rule Check
axes[1, 0].hist(yield_data, bins=8, density=True, alpha=0.7, color='lightgreen', edgecolor='black')
# Shade areas for empirical rule
x_fill = np.linspace(mean_yield - std_yield, mean_yield + std_yield, 100)
axes[1, 0].fill_between(x_fill, stats.norm.pdf(x_fill, mean_yield, std_yield),
                        color='green', alpha=0.3, label='68% within 1σ')
x_fill2 = np.linspace(mean_yield - 2*std_yield, mean_yield + 2*std_yield, 100)
axes[1, 0].fill_between(x_fill2, stats.norm.pdf(x_fill2, mean_yield, std_yield),
                        color='yellow', alpha=0.2, label='95% within 2σ')
axes[1, 0].plot(x, pdf, 'r-', linewidth=2)
axes[1, 0].axvline(mean_yield, color='green', linestyle='--')
axes[1, 0].set_title('Empirical Rule (68-95-99.7 Rule)')
axes[1, 0].set_xlabel('Yield (tons/hectare)')
axes[1, 0].set_ylabel('Density')
axes[1, 0].legend()
# Plot 5: Density plot
sns.kdeplot(yield_data, ax=axes[1, 1], color='blue', linewidth=2, label='Actual Data')
axes[1, 1].plot(x, pdf, 'r-', linewidth=2, label='Normal Distribution')
axes[1, 1].fill_between(x, pdf, alpha=0.3, color='red')
axes[1, 1].set_title('Kernel Density Estimation vs Normal')
axes[1, 1].set_xlabel('Yield (tons/hectare)')
axes[1, 1].set_ylabel('Density')
axes[1, 1].legend()
# Plot 6: Cumulative Distribution Function (CDF)
sorted_yield = np.sort(yield_data)
cdf_actual = np.arange(1, len(sorted_yield) + 1) / len(sorted_yield)
axes[1, 2].plot(sorted_yield, cdf_actual, 'bo-', linewidth=2, markersize=5, label='Actual CDF')
# Theoretical normal CDF
cdf_theoretical = stats.norm.cdf(sorted_yield, mean_yield, std_yield)
axes[1, 2].plot(sorted_yield, cdf_theoretical, 'r-', linewidth=2, label='Theoretical Normal CDF')
axes[1, 2].set_title('Cumulative Distribution Function (CDF)')
axes[1, 2].set_xlabel('Yield (tons/hectare)')
axes[1, 2].set_ylabel('Cumulative Probability')
axes[1, 2].legend()
axes[1, 2].grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
print("EMPIRICAL RULE (68-95-99.7 RULE) DEMONSTRATION")
print("="*80)
print(f"""
The Empirical Rule for Normal Distributions:
• About 68% of data falls within 1 standard deviation of the mean
• About 95% of data falls within 2 standard deviations of the mean
• About 99.7% of data falls within 3 standard deviations of the mean
Mean (μ) = {mean_yield:.3f}
Standard Deviation (σ) = {std_yield:.3f}
""")
within_1_std = np.sum((yield_data >= mean_yield - std_yield) & (yield_data <= mean_yield + std_yield))
within_2_std = np.sum((yield_data >= mean_yield - 2*std_yield) & (yield_data <= mean_yield + 2*std_yield))
within_3_std = np.sum((yield_data >= mean_yield - 3*std_yield) & (yield_data <= mean_yield + 3*std_yield))
pct_1_std = within_1_std / len(yield_data) * 100
pct_2_std = within_2_std / len(yield_data) * 100
pct_3_std = within_3_std / len(yield_data) * 100
print(f"\n**Empirical Rule Check for Sample Data:**")
print(f"Data within μ ± 1σ ({mean_yield - std_yield:.2f} to {mean_yield + std_yield:.2f}):")
print(f"  Expected: ~68%, Actual: {within_1_std}/{len(yield_data)} = {pct_1_std:.1f}%")
print(f"\nData within μ ± 2σ ({mean_yield - 2*std_yield:.2f} to {mean_yield + 2*std_yield:.2f}):")
print(f"  Expected: ~95%, Actual: {within_2_std}/{len(yield_data)} = {pct_2_std:.1f}%")
print(f"\nData within μ ± 3σ ({mean_yield - 3*std_yield:.2f} to {mean_yield + 3*std_yield:.2f}):")
print(f"  Expected: ~99.7%, Actual: {within_3_std}/{len(yield_data)} = {pct_3_std:.1f}%")
print("Z-SCORES (STANDARD SCORES)")
print("="*80)
print("""
Z-Score Formula: z = (x - μ) / σ
Z-scores tell us how many standard deviations a value is from the mean:
• z = 0: Exactly at the mean
• z > 0: Above the mean
• z < 0: Below the mean
• |z| > 2: Unusual (in the tails of the distribution)
""")
df['Yield_z_score'] = (df['Yield_tons_per_hectare'] - mean_yield) / std_yield
print("\n**Z-Scores for Each Observation:**")
for idx, row in df.iterrows():
    yield_value = row['Yield_tons_per_hectare']
    z_score = row['Yield_z_score']
    interpretation = ""
    if abs(z_score) < 1:
        interpretation = "(Within 1 SD of mean)"
    elif abs(z_score) < 2:
        interpretation = f"({abs(z_score):.1f} SD from mean - somewhat unusual)"
    else:
        interpretation = f"({abs(z_score):.1f} SD from mean - very unusual)"
    print(f"Row {idx}: Yield = {yield_value:.2f}, Z-score = {z_score:.2f} {interpretation}")
print("PROBABILITY CALCULATIONS USING NORMAL DISTRIBUTION")
print("="*80)
print("""
We can calculate probabilities using the normal distribution:
1. P(Yield < X): Probability yield is less than X
2. P(Yield > X): Probability yield is greater than X
3. P(a < Yield < b): Probability yield is between a and b
""")
threshold_high = 7.0  # tons/hectare
threshold_low = 3.0   # tons/hectare
prob_less_than_7 = stats.norm.cdf(threshold_high, mean_yield, std_yield)
prob_greater_than_7 = 1 - prob_less_than_7
prob_between_3_and_7 = stats.norm.cdf(threshold_high, mean_yield, std_yield) - stats.norm.cdf(threshold_low, mean_yield, std_yield)
print(f"\n**Probability Calculations:**")
print(f"P(Yield < {threshold_high}) = {prob_less_than_7:.3f} or {prob_less_than_7*100:.1f}%")
print(f"P(Yield > {threshold_high}) = {prob_greater_than_7:.3f} or {prob_greater_than_7*100:.1f}%")
print(f"P({threshold_low} < Yield < {threshold_high}) = {prob_between_3_and_7:.3f} or {prob_between_3_and_7*100:.1f}%")
print("PERCENTILES AND QUANTILES")
print("="*80)
print("""
Percentiles divide the data into 100 equal parts.
Quantiles divide the data into equal-sized groups.
""")
percentiles = [10, 25, 50, 75, 90, 95]
percentile_values = np.percentile(yield_data, percentiles)
print(f"\n**Percentiles of Yield Data:**")
for p, val in zip(percentiles, percentile_values):
    print(f"  {p}th percentile: {val:.3f} tons/hectare")
q1 = np.percentile(yield_data, 25)
q2 = np.percentile(yield_data, 50)  # Median
q3 = np.percentile(yield_data, 75)
iqr = q3 - q1
print(f"\n**Quartiles and IQR:**")
print(f"  Q1 (25th percentile): {q1:.3f}")
print(f"  Q2 (50th percentile, Median): {q2:.3f}")
print(f"  Q3 (75th percentile): {q3:.3f}")
print(f"  IQR (Q3 - Q1): {iqr:.3f}")
print("SIMULATING NORMALLY DISTRIBUTED DATA")
print("="*80)
print(f"""
Simulating new yield data based on our sample statistics:
Using μ = {mean_yield:.3f}, σ = {std_yield:.3f}
""")
# Generate simulated data
np.random.seed(42)  # For reproducibility
simulated_yield = np.random.normal(mean_yield, std_yield, 1000)
# Create visualization comparing actual and simulated data
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
fig.suptitle('Actual vs Simulated Normally Distributed Yield Data', fontsize=14, fontweight='bold')
# Plot 1: Histogram comparison
axes[0].hist(yield_data, bins=8, density=True, alpha=0.7, label='Actual Data', color='blue', edgecolor='black')
axes[0].hist(simulated_yield, bins=30, density=True, alpha=0.5, label='Simulated Data', color='red', edgecolor='black')
axes[0].set_title('Histogram Comparison')
axes[0].set_xlabel('Yield (tons/hectare)')
axes[0].set_ylabel('Density')
axes[0].legend()
axes[0].grid(True, alpha=0.3)
box_data = [yield_data, simulated_yield]
axes[1].boxplot(box_data, labels=['Actual (n=10)', 'Simulated (n=1000)'],
                patch_artist=True,
                boxprops=dict(facecolor='lightblue', color='darkblue'),
                medianprops=dict(color='red', linewidth=2))
axes[1].set_title('Box Plot Comparison')
axes[1].set_ylabel('Yield (tons/hectare)')
axes[1].grid(True, alpha=0.3)
stats.probplot(simulated_yield, dist="norm", plot=axes[2])
axes[2].get_lines()[0].set_marker('o')
axes[2].get_lines()[0].set_markersize(3)
axes[2].get_lines()[0].set_markerfacecolor('red')
axes[2].get_lines()[0].set_markeredgecolor('red')
axes[2].get_lines()[1].set_color('green')
axes[2].get_lines()[1].set_linewidth(2)
axes[2].set_title('Q-Q Plot of Simulated Data')
axes[2].set_xlabel('Theoretical Quantiles')
axes[2].set_ylabel('Sample Quantiles')
plt.tight_layout()
plt.show()
print("CENTRAL LIMIT THEOREM (CLT) DEMONSTRATION")
print("="*80)
print("""
Central Limit Theorem:
• For large enough sample sizes, the distribution of sample means
  approaches a normal distribution, regardless of the population distribution
• Even if individual yields aren't normal, average yields from multiple
  farms will be approximately normal
""")
# Simulate CLT
np.random.seed(123)
sample_means = []
sample_sizes = [5, 10, 30]  # Different sample sizes
for n in sample_sizes:
    means = []
    for _ in range(1000):
        sample = np.random.choice(yield_data, n, replace=True)
        means.append(np.mean(sample))
    sample_means.append(means)
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
fig.suptitle('Central Limit Theorem: Distribution of Sample Means', fontsize=14, fontweight='bold')
for i, (means, n) in enumerate(zip(sample_means, sample_sizes)):
    axes[i].hist(means, bins=30, density=True, alpha=0.7, color='green', edgecolor='black')
    mean_of_means = np.mean(means)
    std_of_means = np.std(means)
    x = np.linspace(min(means), max(means), 100)
    pdf = stats.norm.pdf(x, mean_of_means, std_of_means)
    axes[i].plot(x, pdf, 'r-', linewidth=2)
    axes[i].set_title(f'Sample Size n = {n}')
    axes[i].set_xlabel(f'Mean Yield (n={n})')
    axes[i].set_ylabel('Density')
    axes[i].grid(True, alpha=0.3)
    clt_mean = mean_yield
    clt_std = std_yield / np.sqrt(n)
    axes[i].text(0.05, 0.95, f'CLT Prediction:\nμ = {clt_mean:.2f}\nσ/√n = {clt_std:.2f}',
                transform=axes[i].transAxes, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
plt.tight_layout()
plt.show()

"""**4.1 Normal Distribution**"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings
warnings.filterwarnings('ignore')
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646966373377603,False,True,Rainy,140,3.7072931271974823"""
df = pd.read_csv(pd.io.common.StringIO(data))
df['Rainfall_mm'] = pd.to_numeric(df['Rainfall_mm'])
df['Temperature_Celsius'] = pd.to_numeric(df['Temperature_Celsius'])
df['Days_to_Harvest'] = pd.to_numeric(df['Days_to_Harvest'])
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
print("NORMAL DISTRIBUTION ANALYSIS: X ∼ N(μ, σ²)")
print("="*80)
# Select Yield as our continuous variable
yield_data = df['Yield_tons_per_hectare']
# Calculate μ (mean) and σ (standard deviation)
μ = yield_data.mean()
σ = yield_data.std()
σ_squared = σ ** 2  # variance
print(f"\n**Normal Distribution Parameters:**")
print(f"Mean (μ) = {μ:.4f}")
print(f"Standard Deviation (σ) = {σ:.4f}")
print(f"Variance (σ²) = {σ_squared:.4f}")
print(f"\nOur Yield Distribution: X ∼ N({μ:.2f}, {σ_squared:.2f})")
print("68-95-99.7 RULE (Empirical Rule)")
print("="*80)
range_1σ = (μ - σ, μ + σ)
range_2σ = (μ - 2*σ, μ + 2*σ)
range_3σ = (μ - 3*σ, μ + 3*σ)
print(f"\n**1 Standard Deviation (68% of data):**")
print(f"   μ ± 1σ = {μ:.2f} ± {σ:.2f}")
print(f"   Range: [{μ-σ:.2f}, {μ+σ:.2f}]")
print(f"\n**2 Standard Deviations (95% of data):**")
print(f"   μ ± 2σ = {μ:.2f} ± {2*σ:.2f}")
print(f"   Range: [{μ-2*σ:.2f}, {μ+2*σ:.2f}]")
print(f"\n**3 Standard Deviations (99.7% of data):**")
print(f"   μ ± 3σ = {μ:.2f} ± {3*σ:.2f}")
print(f"   Range: [{μ-3*σ:.2f}, {μ+3*σ:.2f}]")
within_1σ = sum((yield_data >= μ-σ) & (yield_data <= μ+σ))
within_2σ = sum((yield_data >= μ-2*σ) & (yield_data <= μ+2*σ))
within_3σ = sum((yield_data >= μ-3*σ) & (yield_data <= μ+3*σ))
total = len(yield_data)
print(f"\n**Actual Data Check (n={total}):**")
print(f"   Within 1σ: {within_1σ}/{total} = {within_1σ/total*100:.1f}% (Expected: ~68%)")
print(f"   Within 2σ: {within_2σ}/{total} = {within_2σ/total*100:.1f}% (Expected: ~95%)")
print(f"   Within 3σ: {within_3σ}/{total} = {within_3σ/total*100:.1f}% (Expected: ~99.7%)")
# Create visualization of Normal Distribution
print("\n" + "="*80)
print("VISUALIZING NORMAL DISTRIBUTION")
print("="*80)
# Create figure with multiple subplots
fig, axes = plt.subplots(2, 2, figsize=(12, 10))
fig.suptitle('Normal Distribution Analysis: X ∼ N(μ, σ²)', fontsize=16, fontweight='bold')
# 1. Histogram with normal curve
axes[0, 0].hist(yield_data, bins=8, density=True, alpha=0.7, color='skyblue', edgecolor='black', label='Actual Data')
# Generate normal curve
x = np.linspace(μ - 4*σ, μ + 4*σ, 1000)
pdf = (1/(σ * np.sqrt(2*np.pi))) * np.exp(-0.5 * ((x - μ)/σ)**2)
axes[0, 0].plot(x, pdf, 'r-', linewidth=2, label=f'N({μ:.2f}, {σ_squared:.2f})')
axes[0, 0].set_title('Histogram with Normal Curve')
axes[0, 0].set_xlabel('Yield (tons/hectare)')
axes[0, 0].set_ylabel('Density')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)
# 2. Empirical Rule Visualization
axes[0, 1].plot(x, pdf, 'k-', linewidth=2, label='Normal PDF')
# 1σ (68%)
x_1σ = np.linspace(μ-σ, μ+σ, 100)
pdf_1σ = (1/(σ * np.sqrt(2*np.pi))) * np.exp(-0.5 * ((x_1σ - μ)/σ)**2)
axes[0, 1].fill_between(x_1σ, pdf_1σ, color='green', alpha=0.3, label='68% within 1σ')
# 2σ (95%)
x_2σ = np.linspace(μ-2*σ, μ+2*σ, 100)
pdf_2σ = (1/(σ * np.sqrt(2*np.pi))) * np.exp(-0.5 * ((x_2σ - μ)/σ)**2)
axes[0, 1].fill_between(x_2σ, pdf_2σ, color='yellow', alpha=0.2, label='95% within 2σ')
# 3σ (99.7%)
x_3σ = np.linspace(μ-3*σ, μ+3*σ, 100)
pdf_3σ = (1/(σ * np.sqrt(2*np.pi))) * np.exp(-0.5 * ((x_3σ - μ)/σ)**2)
axes[0, 1].fill_between(x_3σ, pdf_3σ, color='red', alpha=0.1, label='99.7% within 3σ')
axes[0, 1].set_title('68-95-99.7 Empirical Rule')
axes[0, 1].set_xlabel('Yield (tons/hectare)')
axes[0, 1].set_ylabel('Density')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)
# 3. Z-scores calculation and plot
z_scores = (yield_data - μ) / σ
df['Z_Score'] = z_scores
axes[1, 0].scatter(range(len(yield_data)), z_scores, color='blue', s=100, alpha=0.7)
axes[1, 0].axhline(y=0, color='red', linestyle='--', label='Mean (z=0)')
axes[1, 0].axhline(y=1, color='green', linestyle=':', label='+1σ')
axes[1, 0].axhline(y=-1, color='green', linestyle=':', label='-1σ')
axes[1, 0].axhline(y=2, color='orange', linestyle=':', label='+2σ')
axes[1, 0].axhline(y=-2, color='orange', linestyle=':', label='-2σ')
for i, (yield_val, z) in enumerate(zip(yield_data, z_scores)):
    axes[1, 0].text(i, z + 0.1, f'{yield_val:.1f}', ha='center', fontsize=8)
axes[1, 0].set_title('Z-Scores for Each Yield Observation')
axes[1, 0].set_xlabel('Observation Number')
axes[1, 0].set_ylabel('Z-Score')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)
stats.probplot(yield_data, dist="norm", plot=axes[1, 1])
axes[1, 1].get_lines()[0].set_marker('o')
axes[1, 1].get_lines()[0].set_markersize(6)
axes[1, 1].get_lines()[0].set_markerfacecolor('blue')
axes[1, 1].get_lines()[0].set_markeredgecolor('blue')
axes[1, 1].get_lines()[1].set_color('red')
axes[1, 1].get_lines()[1].set_linewidth(2)
axes[1, 1].set_title('Q-Q Plot: Checking for Normality')
axes[1, 1].set_xlabel('Theoretical Quantiles')
axes[1, 1].set_ylabel('Sample Quantiles')
axes[1, 1].grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
print("PROBABILITY CALCULATIONS USING NORMAL DISTRIBUTION")
print("="*80)
print(f"\n**Probability that a random farm has:**")

# P(Yield < threshold)
thresholds = [3.0, 5.0, 7.0, μ]
for threshold in thresholds:
    prob = stats.norm.cdf(threshold, μ, σ)
    print(f"   Yield < {threshold}: {prob:.3f} or {prob*100:.1f}%")
print(f"\n**Probability that a random farm has:**")

# P(Yield > threshold)
for threshold in thresholds:
    prob = 1 - stats.norm.cdf(threshold, μ, σ)
    print(f"   Yield > {threshold}: {prob:.3f} or {prob*100:.1f}%")
# Calculate probability between two values
print(f"\n**Probability between ranges:**")
ranges = [(3, 5), (5, 7), (μ-σ, μ+σ)]
for low, high in ranges:
    prob = stats.norm.cdf(high, μ, σ) - stats.norm.cdf(low, μ, σ)
    print(f"   {low} < Yield < {high}: {prob:.3f} or {prob*100:.1f}%")
# Find percentiles
print(f"\n**Percentiles of Yield Distribution:**")
percentiles = [5, 25, 50, 75, 95]
for p in percentiles:
    value = stats.norm.ppf(p/100, μ, σ)
    print(f"   {p}th percentile: {value:.3f} tons/hectare")
# Check for outliers using 3σ rule
print("\n" + "="*80)
print("OUTLIER DETECTION USING 3σ RULE")
print("="*80)
lower_bound = μ - 3*σ
upper_bound = μ + 3*σ
outliers = yield_data[(yield_data < lower_bound) | (yield_data > upper_bound)]
print(f"\nOutlier bounds (3σ rule):")
print(f"   Lower bound (μ - 3σ): {lower_bound:.3f}")
print(f"   Upper bound (μ + 3σ): {upper_bound:.3f}")
if len(outliers) > 0:
    print(f"\n**Outliers detected:**")
    for outlier in outliers:
        z = (outlier - μ) / σ
        print(f"   Yield = {outlier:.3f} (Z-score = {z:.2f})")
else:
    print(f"\n**No outliers detected** using 3σ rule")

"""**C. Task 1: Define Events**"""

import pandas as pd
import numpy as np
data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646966373377603,False,True,Rainy,140,3.7072931271974823"""
df = pd.read_csv(pd.io.common.StringIO(data))
df['Rainfall_mm'] = pd.to_numeric(df['Rainfall_mm'])
df['Temperature_Celsius'] = pd.to_numeric(df['Temperature_Celsius'])
df['Days_to_Harvest'] = pd.to_numeric(df['Days_to_Harvest'])
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
print("\n**SELECTED VARIABLES:**")
print("1. Yield_tons_per_hectare (Continuous Variable)")
print("2. Region (Categorical Variable: North, South, West)")
print("DEFINING EVENTS")
print("="*80)
mean_yield = df['Yield_tons_per_hectare'].mean()
std_yield = df['Yield_tons_per_hectare'].std()
median_yield = df['Yield_tons_per_hectare'].median()
print(f"\n**Yield Statistics:**")
print(f"  Mean (μ): {mean_yield:.3f} tons/hectare")
print(f"  Standard Deviation (σ): {std_yield:.3f} tons/hectare")
print(f"  Median: {median_yield:.3f} tons/hectare")
print(f"  Minimum: {df['Yield_tons_per_hectare'].min():.3f} tons/hectare")
print(f"  Maximum: {df['Yield_tons_per_hectare'].max():.3f} tons/hectare")
# Event 1: High Yield
high_yield_threshold = mean_yield  # Above average yield
df['High_Yield'] = df['Yield_tons_per_hectare'] > high_yield_threshold
print("\n**EVENT 1: High Yield (A)**")
print("   Let A be the event that yield is above average")
print(f"   Mathematical Notation: A = {{Y > {high_yield_threshold:.3f}}}")
print(f"   Where Y = Yield_tons_per_hectare")
print(f"   Threshold: Above average yield = {high_yield_threshold:.3f} tons/hectare")
print(f"   Number of cases: {df['High_Yield'].sum()} out of {len(df)}")
# Event 2: Southern Region
print("\n**EVENT 2: Southern Region (B)**")
print("   Let B be the event that farm is in Southern region")
print("   Mathematical Notation: B = {Region = 'South'}")
print(f"   Number of cases: {(df['Region'] == 'South').sum()} out of {len(df)}")
# Event 3: Moderate Temperature Range
print("\n**EVENT 3: Moderate Temperature (C)**")
print("   Let C be the event that temperature is moderate (20-30°C)")
print("   Mathematical Notation: C = {20 ≤ Temperature ≤ 30}")
print(f"   Temperature range: 20°C to 30°C")
print(f"   Number of cases: {((df['Temperature_Celsius'] >= 20) & (df['Temperature_Celsius'] <= 30)).sum()} out of {len(df)}")
# Additional Event 4: Using Fertilizer
print("\n**EVENT 4: Fertilizer Used (D)**")
print("   Let D be the event that fertilizer is used")
print("   Mathematical Notation: D = {Fertilizer_Used = True}")
print(f"   Number of cases: {df['Fertilizer_Used'].sum()} out of {len(df)}")
print("MATHEMATICAL REPRESENTATION OF EVENTS")
print("="*80)
print("""
Let's define our sample space and events more formally:
SAMPLE SPACE (Ω):
  All possible outcomes of our agricultural observations
  Ω = {All 10 observations in our dataset}
RANDOM VARIABLES:
  1. Y = Yield_tons_per_hectare (continuous)
  2. R = Region (categorical: {North, South, West})
EVENTS:
  1. Event A: High Yield
     A = {ω ∈ Ω : Y(ω) > 5.08}
     Where Y(ω) is the yield for observation ω
     P(A) = Number of observations with Y > 5.08 / 10
  2. Event B: Southern Region
     B = {ω ∈ Ω : R(ω) = 'South'}
     Where R(ω) is the region for observation ω
     P(B) = Number of observations with Region = 'South' / 10
  3. Event C: Moderate Temperature
     C = {ω ∈ Ω : 20 ≤ T(ω) ≤ 30}
     Where T(ω) is the temperature for observation ω
     P(C) = Number of observations with 20 ≤ T ≤ 30 / 10
  4. Event D: Fertilizer Used
     D = {ω ∈ Ω : F(ω) = True}
     Where F(ω) is the fertilizer usage for observation ω
     P(D) = Number of observations with Fertilizer_Used = True / 10 """)
print("PROBABILITY CALCULATIONS")
print("="*80)
# Calculate individual probabilities
P_A = df['High_Yield'].mean()
P_B = (df['Region'] == 'South').mean()
P_C = ((df['Temperature_Celsius'] >= 20) & (df['Temperature_Celsius'] <= 30)).mean()
P_D = df['Fertilizer_Used'].mean()
print(f"\n**Individual Probabilities:**")
print(f"  P(A) = P(High Yield) = {P_A:.3f} ({P_A*100:.1f}%)")
print(f"  P(B) = P(Southern Region) = {P_B:.3f} ({P_B*100:.1f}%)")
print(f"  P(C) = P(Moderate Temperature) = {P_C:.3f} ({P_C*100:.1f}%)")
print(f"  P(D) = P(Fertilizer Used) = {P_D:.3f} ({P_D*100:.1f}%)")
print("\n**Compound Probabilities (Intersections):**")
# P(A ∩ B): High Yield AND Southern Region
A_and_B = ((df['High_Yield'] == True) & (df['Region'] == 'South')).sum()
P_A_and_B = A_and_B / len(df)
print(f"  P(A ∩ B) = P(High Yield AND Southern Region) = {A_and_B}/10 = {P_A_and_B:.3f}")
# P(A ∩ D): High Yield AND Fertilizer Used
A_and_D = ((df['High_Yield'] == True) & (df['Fertilizer_Used'] == True)).sum()
P_A_and_D = A_and_D / len(df)
print(f"  P(A ∩ D) = P(High Yield AND Fertilizer Used) = {A_and_D}/10 = {P_A_and_D:.3f}")
# P(B ∩ C): Southern Region AND Moderate Temperature
B_and_C = ((df['Region'] == 'South') & ((df['Temperature_Celsius'] >= 20) & (df['Temperature_Celsius'] <= 30))).sum()
P_B_and_C = B_and_C / len(df)
print(f"  P(B ∩ C) = P(Southern Region AND Moderate Temperature) = {B_and_C}/10 = {P_B_and_C:.3f}")
# Calculate conditional probabilities
print("\n**Conditional Probabilities:**")
# P(A|B): Probability of High Yield given Southern Region
P_A_given_B = A_and_B / (df['Region'] == 'South').sum() if (df['Region'] == 'South').sum() > 0 else 0
print(f"  P(A|B) = P(High Yield | Southern Region) = {A_and_B}/{(df['Region'] == 'South').sum()} = {P_A_given_B:.3f}")
# P(B|A): Probability of Southern Region given High Yield
P_B_given_A = A_and_B / df['High_Yield'].sum() if df['High_Yield'].sum() > 0 else 0
print(f"  P(B|A) = P(Southern Region | High Yield) = {A_and_B}/{df['High_Yield'].sum()} = {P_B_given_A:.3f}")
# P(A|D): Probability of High Yield given Fertilizer Used
P_A_given_D = A_and_D / df['Fertilizer_Used'].sum() if df['Fertilizer_Used'].sum() > 0 else 0
print(f"  P(A|D) = P(High Yield | Fertilizer Used) = {A_and_D}/{df['Fertilizer_Used'].sum()} = {P_A_given_D:.3f}")
print("\n**Union Probabilities:**")
# P(A ∪ B): High Yield OR Southern Region
A_or_B = ((df['High_Yield'] == True) | (df['Region'] == 'South')).sum()
P_A_or_B = A_or_B / len(df)
print(f"  P(A ∪ B) = P(High Yield OR Southern Region) = {A_or_B}/10 = {P_A_or_B:.3f}")
# Verify using addition rule: P(A ∪ B) = P(A) + P(B) - P(A ∩ B)
P_A_or_B_calculated = P_A + P_B - P_A_and_B
print(f"    Verification: P(A) + P(B) - P(A ∩ B) = {P_A:.3f} + {P_B:.3f} - {P_A_and_B:.3f} = {P_A_or_B_calculated:.3f}")
print("DATASET WITH EVENT INDICATORS")
print("="*80)
# Create a display DataFrame
display_df = df.copy()
display_df['Event A (High Yield)'] = display_df['High_Yield']
display_df['Event B (South Region)'] = display_df['Region'] == 'South'
display_df['Event C (Mod Temp 20-30°C)'] = (display_df['Temperature_Celsius'] >= 20) & (display_df['Temperature_Celsius'] <= 30)
display_df['Event D (Fertilizer)'] = display_df['Fertilizer_Used']
print(display_df[['Region', 'Yield_tons_per_hectare', 'Temperature_Celsius', 'Fertilizer_Used',
                  'Event A (High Yield)', 'Event B (South Region)',
                  'Event C (Mod Temp 20-30°C)', 'Event D (Fertilizer)']].to_string(index=False))
print("VENN DIAGRAM REPRESENTATION")
print("="*80)
count_A = df['High_Yield'].sum()
count_B = (df['Region'] == 'South').sum()
count_C = ((df['Temperature_Celsius'] >= 20) & (df['Temperature_Celsius'] <= 30)).sum()
count_D = df['Fertilizer_Used'].sum()
count_A_and_B = A_and_B
count_A_and_D = A_and_D
count_B_and_C = B_and_C
print(f"\n**Event Counts:**")
print(f"  Event A (High Yield): {count_A} observations")
print(f"  Event B (South Region): {count_B} observations")
print(f"  Event C (Mod Temp 20-30°C): {count_C} observations")
print(f"  Event D (Fertilizer Used): {count_D} observations")
print(f"\n**Intersection Counts:**")
print(f"  A ∩ B: {count_A_and_B} observations")
print(f"  A ∩ D: {count_A_and_D} observations")
print(f"  B ∩ C: {count_B_and_C} observations")
print("INDEPENDENCE CHECKING")
print("="*80)
print("""Two events X and Y are independent if: P(X ∩ Y) = P(X) × P(Y)""")
expected_A_and_B = P_A * P_B
print(f"\nChecking independence of A and B:")
print(f"  P(A ∩ B) = {P_A_and_B:.3f}")
print(f"  P(A) × P(B) = {P_A:.3f} × {P_B:.3f} = {expected_A_and_B:.3f}")
if abs(P_A_and_B - expected_A_and_B) < 0.01:
    print(f"  Conclusion: A and B are approximately independent")
else:
    print(f"  Conclusion: A and B are DEPENDENT (difference = {abs(P_A_and_B - expected_A_and_B):.3f})")
expected_A_and_D = P_A * P_D
print(f"\nChecking independence of A and D:")
print(f"  P(A ∩ D) = {P_A_and_D:.3f}")
print(f"  P(A) × P(D) = {P_A:.3f} × {P_D:.3f} = {expected_A_and_D:.3f}")
if abs(P_A_and_D - expected_A_and_D) < 0.01:
    print(f"  Conclusion: A and D are approximately independent")
else:
    print(f"  Conclusion: A and D are DEPENDENT (difference = {abs(P_A_and_D - expected_A_and_D):.3f})")
print("REAL-WORLD INTERPRETATION")
print("="*80)
print(f"""
Based on our analysis:
1. **High Yield Farms (Event A):**
   - {count_A} out of 10 farms have above-average yield
   - Probability: {P_A:.2f} or {P_A*100:.0f}%
   - This is our target outcome for farmers
2. **Southern Region Farms (Event B):**
   - {count_B} out of 10 farms are in Southern region
   - Probability: {P_B:.2f} or {P_B*100:.0f}%
   - Regional factors might affect yield
3. **Relationship between Region and Yield:**
   - P(High Yield | Southern Region) = {P_A_given_B:.2f}
   - P(Southern Region | High Yield) = {P_B_given_A:.2f}
   - This suggests that Southern farms are {P_A_given_B/P_A:.1f}x more likely to have high yield than average
4. **Fertilizer Impact:**
   - P(High Yield | Fertilizer Used) = {P_A_given_D:.2f}
   - Farms using fertilizer are {P_A_given_D/P_A:.1f}x more likely to have high yield
PRACTICAL IMPLICATIONS:
• Southern regions show promise for high yield
• Fertilizer use is associated with better yields
• Temperature range 20-30°C occurs in {count_C} farms """)
# Summary table of all probabilities
print("\n" + "="*80)
print("SUMMARY OF ALL PROBABILITIES")
print("="*80)

summary_data = {
    'Event': ['A: High Yield', 'B: South Region', 'C: Mod Temp (20-30°C)', 'D: Fertilizer Used',
              'A ∩ B', 'A ∩ D', 'B ∩ C', 'A ∪ B'],
    'Description': ['Yield > average', 'Region = South', '20 ≤ Temp ≤ 30', 'Fertilizer = True',
                   'High Yield AND South', 'High Yield AND Fertilizer', 'South AND Mod Temp', 'High Yield OR South'],
    'Count': [count_A, count_B, count_C, count_D, count_A_and_B, count_A_and_D, count_B_and_C, A_or_B],
    'Probability': [P_A, P_B, P_C, P_D, P_A_and_B, P_A_and_D, P_B_and_C, P_A_or_B],
    'Percentage': [f"{P_A*100:.1f}%", f"{P_B*100:.1f}%", f"{P_C*100:.1f}%", f"{P_D*100:.1f}%",
                   f"{P_A_and_B*100:.1f}%", f"{P_A_and_D*100:.1f}%", f"{P_B_and_C*100:.1f}%", f"{P_A_or_B*100:.1f}%"]
}

summary_df = pd.DataFrame(summary_data)
print(summary_df.to_string(index=False))
print("MATHEMATICAL NOTATION SUMMARY")
print("="*80)

print("""
FINAL MATHEMATICAL NOTATION:

Sample Space: Ω = {ω₁, ω₂, ..., ω₁₀} where each ωᵢ is an observation

Random Variables:
  Y: Ω → ℝ, Y(ω) = Yield_tons_per_hectare of observation ω
  R: Ω → {North, South, West}, R(ω) = Region of observation ω
  T: Ω → ℝ, T(ω) = Temperature_Celsius of observation ω
  F: Ω → {True, False}, F(ω) = Fertilizer_Used status of observation ω

Events:
  A = {ω ∈ Ω : Y(ω) > 5.08}
  B = {ω ∈ Ω : R(ω) = 'South'}
  C = {ω ∈ Ω : 20 ≤ T(ω) ≤ 30}
  D = {ω ∈ Ω : F(ω) = True}

Probabilities (Empirical):
  P(A) = 6/10 = 0.6
  P(B) = 4/10 = 0.4
  P(C) = 5/10 = 0.5
  P(D) = 5/10 = 0.5

Conditional Probabilities:
  P(A|B) = P(A ∩ B)/P(B) = 4/4 = 1.0
  P(B|A) = P(A ∩ B)/P(A) = 4/6 = 0.667
  P(A|D) = P(A ∩ D)/P(D) = 3/5 = 0.6
""")

"""**D. Task 2: Conditional Probability**"""

import pandas as pd
import numpy as np
import itertools
data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646966373377603,False,True,Rainy,140,3.7072931271974823"""
df = pd.read_csv(pd.io.common.StringIO(data))
df['Rainfall_mm'] = pd.to_numeric(df['Rainfall_mm'])
df['Temperature_Celsius'] = pd.to_numeric(df['Temperature_Celsius'])
df['Days_to_Harvest'] = pd.to_numeric(df['Days_to_Harvest'])
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
average_yield = df['Yield_tons_per_hectare'].mean()
df['Event_A'] = df['Yield_tons_per_hectare'] > average_yield  # A = High Yield
df['Event_B'] = df['Region'] == 'South'  # B = South Region
df['Event_C'] = (df['Temperature_Celsius'] >= 20) & (df['Temperature_Celsius'] <= 30)
df['Event_D'] = df['Fertilizer_Used']  # D = Fertilizer Used
P_A = df['Event_A'].mean()
P_B = df['Event_B'].mean()
P_C = df['Event_C'].mean()
P_D = df['Event_D'].mean()
print(f"\n**INDIVIDUAL EVENT PROBABILITIES:**")
print(f"  P(A) = P(High Yield) = {P_A:.3f} ({P_A*100:.1f}%)")
print(f"  P(B) = P(South Region) = {P_B:.3f} ({P_B*100:.1f}%)")
print(f"  P(C) = P(Moderate Temperature) = {P_C:.3f} ({P_C*100:.1f}%)")
print(f"  P(D) = P(Fertilizer Used) = {P_D:.3f} ({P_D*100:.1f}%)")
def conditional_probability(event_A_col, event_B_col):
    both_cases = ((df[event_A_col] == True) & (df[event_B_col] == True)).sum()
    b_cases = (df[event_B_col] == True).sum()
    if b_cases == 0:
        return 0
    return both_cases / b_cases
event_pairs = [
    ('A', 'B', 'Event_A', 'Event_B', 'High Yield', 'South Region'),
    ('A', 'C', 'Event_A', 'Event_C', 'High Yield', 'Moderate Temperature'),
    ('A', 'D', 'Event_A', 'Event_D', 'High Yield', 'Fertilizer Used'),
    ('B', 'A', 'Event_B', 'Event_A', 'South Region', 'High Yield'),
    ('B', 'C', 'Event_B', 'Event_C', 'South Region', 'Moderate Temperature'),
    ('B', 'D', 'Event_B', 'Event_D', 'South Region', 'Fertilizer Used'),
    ('C', 'A', 'Event_C', 'Event_A', 'Moderate Temperature', 'High Yield'),
    ('C', 'B', 'Event_C', 'Event_B', 'Moderate Temperature', 'South Region'),
    ('C', 'D', 'Event_C', 'Event_D', 'Moderate Temperature', 'Fertilizer Used'),
    ('D', 'A', 'Event_D', 'Event_A', 'Fertilizer Used', 'High Yield'),
    ('D', 'B', 'Event_D', 'Event_B', 'Fertilizer Used', 'South Region'),
    ('D', 'C', 'Event_D', 'Event_C', 'Fertilizer Used', 'Moderate Temperature'),
]
print("ANALYSIS OF ALL PAIRS: P(A|B) = P(A ∩ B) / P(B)")
print("="*80)
summary_data = []
for event1, event2, col1, col2, desc1, desc2 in event_pairs:
    # Get probabilities
    if event1 == 'A':
        P_event1 = P_A
    elif event1 == 'B':
        P_event1 = P_B
    elif event1 == 'C':
        P_event1 = P_C
    else:  # D
        P_event1 = P_D
    if event2 == 'A':
        P_event2 = P_A
    elif event2 == 'B':
        P_event2 = P_B
    elif event2 == 'C':
        P_event2 = P_C
    else:  # D
        P_event2 = P_D
    # Calculate conditional probability
    P_A_given_B = conditional_probability(col1, col2)
    # Count cases for the formula display
    both_count = ((df[col1] == True) & (df[col2] == True)).sum()
    b_count = (df[col2] == True).sum()
    total = len(df)
    print(f"\n**Pair: P({event1} | {event2}) = P({desc1} | {desc2})**")
    print("-" * 60)
    # Mathematical formula display
    print(f"  Formula: P({event1}|{event2}) = P({event1} ∩ {event2}) / P({event2})")
    print(f"  Calculation: {both_count}/{b_count} = {P_A_given_B:.3f}")
    print(f"  P({event1}) = {P_event1:.3f}, P({event2}) = {P_event2:.3f}")
    print(f"  P({event1} ∩ {event2}) = {both_count}/{total} = {both_count/total:.3f}")
    # Interpretation
    print(f"\n  *Interpretation:**")
    if event1 == 'A' and event2 == 'B':
        print("     Given that a farm is in the SOUTH REGION,")
        print(f"     the probability of having HIGH YIELD is {P_A_given_B:.1%}")
        if P_A_given_B > P_event1:
            print("     → South region farms are MORE LIKELY to have high yield")
        elif P_A_given_B < P_event1:
            print("     → South region farms are LESS LIKELY to have high yield")
        else:
            print("     → South region doesn't affect high yield probability")
    elif event1 == 'A' and event2 == 'C':
        print("     Given MODERATE TEMPERATURE (20-30°C),")
        print(f"     the probability of HIGH YIELD is {P_A_given_B:.1%}")
    elif event1 == 'A' and event2 == 'D':
        print("     Given that FERTILIZER IS USED,")
        print(f"     the probability of HIGH YIELD is {P_A_given_B:.1%}")
    elif event1 == 'B' and event2 == 'A':
        print("     Given that a farm has HIGH YIELD,")
        print(f"     the probability it's in SOUTH REGION is {P_A_given_B:.1%}")
    elif event1 == 'B' and event2 == 'C':
        print("     Given MODERATE TEMPERATURE,")
        print(f"     the probability of SOUTH REGION is {P_A_given_B:.1%}")
    elif event1 == 'B' and event2 == 'D':
        print("     Given that FERTILIZER IS USED,")
        print(f"     the probability of SOUTH REGION is {P_A_given_B:.1%}")
    elif event1 == 'C' and event2 == 'A':
        print("     Given HIGH YIELD,")
        print(f"     the probability of MODERATE TEMPERATURE is {P_A_given_B:.1%}")
    elif event1 == 'C' and event2 == 'B':
        print("     Given SOUTH REGION,")
        print(f"     the probability of MODERATE TEMPERATURE is {P_A_given_B:.1%}")
    elif event1 == 'C' and event2 == 'D':
        print("     Given that FERTILIZER IS USED,")
        print(f"     the probability of MODERATE TEMPERATURE is {P_A_given_B:.1%}")
    elif event1 == 'D' and event2 == 'A':
        print("     Given HIGH YIELD,")
        print(f"     the probability that FERTILIZER WAS USED is {P_A_given_B:.1%}")
    elif event1 == 'D' and event2 == 'B':
        print("     Given SOUTH REGION,")
        print(f"     the probability that FERTILIZER WAS USED is {P_A_given_B:.1%}")
    elif event1 == 'D' and event2 == 'C':
        print("     Given MODERATE TEMPERATURE,")
        print(f"     the probability that FERTILIZER WAS USED is {P_A_given_B:.1%}")
    summary_data.append({
        'P(A|B)': f'P({event1}|{event2})',
        'Description': f'P({desc1} | {desc2})',
        'Value': P_A_given_B,
        'P(A)': P_event1,
        'P(B)': P_event2,
        'Comparison': 'Higher' if P_A_given_B > P_event1 else 'Lower' if P_A_given_B < P_event1 else 'Same'
    })
print("COMPARISON MATRIX: P(A|B) vs P(A)")
print("="*80)
print("""
This shows how knowing B changes the probability of A:
                  P(A|B)   vs   P(A)     →   Effect of knowing B
----------------------------------------------------------------
P(High Yield | South)     = 1.000  vs  0.600  →  INCREASES probability
P(High Yield | Mod Temp)  = 0.800  vs  0.600  →  INCREASES probability
P(High Yield | Fertilizer)= 0.600  vs  0.600  →  NO CHANGE
P(South | High Yield)     = 0.667  vs  0.400  →  INCREASES probability
P(Mod Temp | High Yield)  = 0.667  vs  0.500  →  INCREASES probability
P(Fertilizer | High Yield)= 0.500  vs  0.500  →  NO CHANGE""")
print("BAYES' THEOREM APPLICATION")
print("="*80)
print("""
Let's verify P(B|A) using Bayes' Theorem:
P(B|A) = [P(A|B) × P(B)] / P(A)""")
# For P(South | High Yield) = P(B|A)
P_A_given_B = conditional_probability('Event_A', 'Event_B')  # P(A|B) = 1.000
P_B = P_B  # 0.400
P_A = P_A  # 0.600
P_B_given_A_calculated = (P_A_given_B * P_B) / P_A
print(f"\nP(B|A) = P(South | High Yield) = [P(A|B) × P(B)] / P(A)")
print(f"       = [{P_A_given_B:.3f} × {P_B:.3f}] / {P_A:.3f}")
print(f"       = {P_B_given_A_calculated:.3f}")
# Compare with direct calculation
P_B_given_A_direct = conditional_probability('Event_B', 'Event_A')
print(f"\nDirect calculation: P(B|A) = {P_B_given_A_direct:.3f}")
print(f"Bayes' Theorem gives the same result: {P_B_given_A_calculated:.3f}")

"""**E. Task 3: Independence Check**"""

import pandas as pd
import numpy as np
print("\n**Events Definition:**")
print("Event A: High Yield (Yield > Average)")
print("Event B: South Region (Region = 'South')")
data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646198956255336,False,True,Rainy,140,3.7072931271974823"""
df = pd.read_csv(pd.io.common.StringIO(data))
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
# Event A: High Yield
average_yield = df['Yield_tons_per_hectare'].mean()
df['Event_A'] = df['Yield_tons_per_hectare'] > average_yield
# Event B: South Region
df['Event_B'] = df['Region'] == 'South'
print(f"\n**Data Summary (10 records):**")
print(f"Average Yield: {average_yield:.3f} tons/hectare")
print(df[['Region', 'Yield_tons_per_hectare', 'Event_A', 'Event_B']].to_string(index=False))
print("1. COMPUTING PROBABILITIES")
print("="*80)
# P(A) - Probability of High Yield
P_A = df['Event_A'].mean()
print(f"\n**P(A) = P(High Yield):**")
print(f"   Number of High Yield cases: {df['Event_A'].sum()}")
print(f"   Total cases: {len(df)}")
print(f"   P(A) = {df['Event_A'].sum()}/{len(df)} = {P_A:.3f}")
# P(B) - Probability of South Region
P_B = df['Event_B'].mean()
print(f"\n**P(B) = P(South Region):**")
print(f"   Number of South Region cases: {df['Event_B'].sum()}")
print(f"   Total cases: {len(df)}")
print(f"   P(B) = {df['Event_B'].sum()}/{len(df)} = {P_B:.3f}")
# P(A ∩ B) - Probability of both A and B
A_and_B = ((df['Event_A'] == True) & (df['Event_B'] == True)).sum()
P_A_and_B = A_and_B / len(df)
print(f"\n**P(A ∩ B) = P(High Yield AND South Region):**")
print(f"   Number of cases with both High Yield AND South Region: {A_and_B}")
print(f"   Total cases: {len(df)}")
print(f"   P(A ∩ B) = {A_and_B}/{len(df)} = {P_A_and_B:.3f}")
print("2. COMPARING P(A ∩ B) WITH P(A)P(B)")
print("="*80)
# Calculate P(A)P(B)
P_A_times_P_B = P_A * P_B
print(f"\n**Calculation:**")
print(f"   P(A) × P(B) = {P_A:.3f} × {P_B:.3f}")
print(f"               = {P_A_times_P_B:.3f}")
print(f"\n**Comparison:**")
print(f"   P(A ∩ B) = {P_A_and_B:.3f}")
print(f"   P(A) × P(B) = {P_A_times_P_B:.3f}")
print(f"   Difference = {abs(P_A_and_B - P_A_times_P_B):.3f}")
print("3. CHECKING INDEPENDENCE")
print("="*80)
print("\n**Independence Rule:**")
print("Two events A and B are independent if: P(A ∩ B) = P(A) × P(B)")
if abs(P_A_and_B - P_A_times_P_B) < 0.01:
    print(f"\n**RESULT: INDEPENDENT**")
    print(f"   Because P(A ∩ B) ≈ P(A) × P(B)")
    print(f"   {P_A_and_B:.3f} ≈ {P_A_times_P_B:.3f}")
else:
    print(f"\n**RESULT: DEPENDENT (NOT INDEPENDENT)**")
    print(f"   Because P(A ∩ B) ≠ P(A) × P(B)")
    print(f"   {P_A_and_B:.3f} ≠ {P_A_times_P_B:.3f}")
print("MATHEMATICAL VERIFICATION")
print("="*80)
print(f"\n**Step-by-step calculation:**")
print(f"1. P(A) = {df['Event_A'].sum()}/{len(df)} = {P_A:.3f}")
print(f"2. P(B) = {df['Event_B'].sum()}/{len(df)} = {P_B:.3f}")
print(f"3. P(A) × P(B) = {P_A:.3f} × {P_B:.3f} = {P_A_times_P_B:.3f}")
print(f"4. P(A ∩ B) = {A_and_B}/{len(df)} = {P_A_and_B:.3f}")
print(f"\n**Conclusion: {'INDEPENDENT' if abs(P_A_and_B - P_A_times_P_B) < 0.01 else 'DEPENDENT'}**")
print(f"   Because {P_A_and_B:.3f} {'=' if abs(P_A_and_B - P_A_times_P_B) < 0.01 else '≠'} {P_A_times_P_B:.3f}")
print("ADDITIONAL ANALYSIS WITH EXACT VALUES")
print("="*80)
print(f"\n**Exact values from data:**")
print(f"Total farms: {len(df)}")
print(f"High Yield farms: {df['Event_A'].sum()}")
print(f"South Region farms: {df['Event_B'].sum()}")
print(f"Both High Yield AND South Region: {A_and_B}")
print(f"\n**Exact probabilities:**")
print(f"P(A) = {df['Event_A'].sum()}/{len(df)} = {df['Event_A'].sum()/len(df):.4f}")
print(f"P(B) = {df['Event_B'].sum()}/{len(df)} = {df['Event_B'].sum()/len(df):.4f}")
print(f"P(A) × P(B) = {P_A_times_P_B:.4f}")
print(f"P(A ∩ B) = {A_and_B}/{len(df)} = {P_A_and_B:.4f}")

"""**F. Task 4: Bayes’ Rule**"""

import pandas as pd
import numpy as np
print("BAYES' THEOREM ANALYSIS: P(B|A) = [P(A|B) × P(B)] / P(A)")
data = """Region,Soil_Type,Crop,Rainfall_mm,Temperature_Celsius,Fertilizer_Used,Irrigation_Used,Weather_Condition,Days_to_Harvest,Yield_tons_per_hectare
West,Sandy,Cotton,897.0772391101236,27.676966373377603,False,True,Cloudy,122,6.555816258223593
South,Clay,Rice,992.6732816189208,18.02614225436302,True,True,Rainy,140,8.5273409063236
North,Loam,Barley,147.9980252926104,29.79404241557257,False,False,Sunny,106,1.127443335982929
North,Sandy,Soybean,986.8663313367325,16.64419019137728,False,True,Rainy,146,6.517572507555278
South,Silt,Wheat,730.379174445627,31.620687370805797,True,True,Cloudy,110,7.248251218445701
South,Silt,Soybean,797.4711823962564,37.70497446941277,False,True,Rainy,74,5.898416311841461
West,Clay,Wheat,357.90235724297685,31.59343138976995,False,False,Rainy,90,2.652391664619867
South,Sandy,Rice,441.13115357285005,30.88710699523619,True,True,Sunny,61,5.8295423488104605
North,Silt,Wheat,181.5878606243205,26.752728580811905,True,False,Sunny,127,2.9437164569313867
West,Sandy,Wheat,395.0489682684721,17.646198956255336,False,True,Rainy,140,3.7072931271974823"""
df = pd.read_csv(pd.io.common.StringIO(data))
df['Yield_tons_per_hectare'] = pd.to_numeric(df['Yield_tons_per_hectare'])
print("\n**Dataset (10 records):**")
print(df[['Region', 'Yield_tons_per_hectare']].to_string(index=False))
print("1. DEFINE EVENTS")
print("="*80)
print("""
Event A: High Yield (Yield > Average)
Event B: South Region (Region = 'South')""")
# Calculate average yield
average_yield = df['Yield_tons_per_hectare'].mean()
# Create event columns
df['Event_A'] = df['Yield_tons_per_hectare'] > average_yield  # High Yield
df['Event_B'] = df['Region'] == 'South'  # South Region
print(f"Average Yield: {average_yield:.3f} tons/hectare")
print("\n**Dataset with Events:**")
print(df[['Region', 'Yield_tons_per_hectare', 'Event_A', 'Event_B']].to_string(index=False))
print("2. COMPUTE P(B) - PROBABILITY OF SOUTH REGION")
print("="*80)
# P(B) = Probability of South Region
south_count = df['Event_B'].sum()
total_count = len(df)
P_B = south_count / total_count
print(f"Number of South Region farms: {south_count}")
print(f"Total farms: {total_count}")
print(f"\nP(B) = P(South Region) = {south_count}/{total_count}")
print(f"P(B) = {P_B:.3f} ({P_B*100:.1f}%)")
print("3. COMPUTE P(A|B) - CONDITIONAL PROBABILITY")
print("="*80)
print("""
P(A|B) = Probability of High Yield GIVEN South Region
       = P(High Yield AND South Region) / P(South Region)
       = Count(High Yield AND South) / Count(South)""")
# Count of South Region farms
south_farms = df[df['Event_B'] == True]
south_count = len(south_farms)
# Count of High Yield AND South Region
high_yield_south = south_farms[south_farms['Event_A'] == True]
high_yield_south_count = len(high_yield_south)
# Calculate P(A|B)
P_A_given_B = high_yield_south_count / south_count if south_count > 0 else 0
print(f"South Region farms: {south_count}")
print(f"High Yield AND South Region farms: {high_yield_south_count}")
print(f"\nP(A|B) = {high_yield_south_count}/{south_count}")
print(f"P(A|B) = {P_A_given_B:.3f} ({P_A_given_B*100:.1f}%)")
print("\n**Interpretation:**")
print(f"Given that a farm is in South Region, the probability of High Yield is {P_A_given_B:.1%}")
print("4. BAYES' THEOREM: COMPUTE P(B|A)")
print("="*80)
print("""
Bayes' Theorem Formula:
P(B|A) = [P(A|B) × P(B)] / P(A)
Where:
• P(B|A) = Probability of South Region GIVEN High Yield
• P(A|B) = Probability of High Yield GIVEN South Region (calculated above)
• P(B) = Probability of South Region (calculated above)
• P(A) = Probability of High Yield (need to calculate)""")
# Calculate P(A) - Probability of High Yield
high_yield_count = df['Event_A'].sum()
P_A = high_yield_count / total_count
print(f"\n**Calculate P(A):**")
print(f"High Yield farms: {high_yield_count}")
print(f"Total farms: {total_count}")
print(f"P(A) = P(High Yield) = {high_yield_count}/{total_count} = {P_A:.3f}")
print(f"\n**Apply Bayes' Theorem:**")
print(f"P(A|B) = {P_A_given_B:.3f}")
print(f"P(B) = {P_B:.3f}")
print(f"P(A) = {P_A:.3f}")
P_B_given_A_calculated = (P_A_given_B * P_B) / P_A
print(f"\n**Calculation:**")
print(f"P(B|A) = [{P_A_given_B:.3f} * {P_B:.3f}] / {P_A:.3f}")
print(f"       = {P_A_given_B * P_B:.3f} / {P_A:.3f}")
print(f"       = {P_B_given_A_calculated:.3f} ({P_B_given_A_calculated*100:.1f}%)")
print("5. EMPIRICAL VALUE: DIRECT CALCULATION OF P(B|A)")
print("="*80)
print("""
Direct calculation from data:
P(B|A) = P(South Region | High Yield)
       = Count(South Region AND High Yield) / Count(High Yield)""")
high_yield_farms = df[df['Event_A'] == True]
high_yield_count = len(high_yield_farms)
south_and_high_yield = high_yield_farms[high_yield_farms['Event_B'] == True]
south_and_high_yield_count = len(south_and_high_yield)
P_B_given_A_empirical = south_and_high_yield_count / high_yield_count if high_yield_count > 0 else 0
print(f"High Yield farms: {high_yield_count}")
print(f"South Region AND High Yield farms: {south_and_high_yield_count}")
print(f"\nP(B|A) (Empirical) = {south_and_high_yield_count}/{high_yield_count}")
print(f"                    = {P_B_given_A_empirical:.3f} ({P_B_given_A_empirical*100:.1f}%)")
print("6. COMPARISON: BAYES' THEOREM VS EMPIRICAL VALUE")
print("="*80)
print("\n**Results Comparison:**")
print(f"Bayes' Theorem calculation: P(B|A) = {P_B_given_A_calculated:.3f}")
print(f"Empirical (direct) value:   P(B|A) = {P_B_given_A_empirical:.3f}")
difference = abs(P_B_given_A_calculated - P_B_given_A_empirical)
if difference < 0.001:
    print(f"\n**PERFECT MATCH!**")
    print(f"   Bayes' Theorem gives the exact same result as empirical calculation.")
    print(f"   Difference: {difference:.6f}")
else:
    print(f"\n**Difference found:** {difference:.6f}")
    print(f"   This could be due to rounding or calculation precision.")
print("7. VERIFICATION WITH ACTUAL DATA")
print("="*80)
print("\n**Verification Table:**")
verification_df = df[['Region', 'Yield_tons_per_hectare', 'Event_A', 'Event_B']].copy()
verification_df['High Yield?'] = verification_df['Event_A'].apply(lambda x: 'Yes' if x else 'No')
verification_df['South Region?'] = verification_df['Event_B'].apply(lambda x: 'Yes' if x else 'No')
print(verification_df.to_string(index=False))
print(f"\n**Count Summary:**")
print(f"Total farms: {total_count}")
print(f"High Yield farms (A): {high_yield_count}")
print(f"South Region farms (B): {south_count}")
print(f"Both A and B: {south_and_high_yield_count}")
print("\n**Probability Summary:**")
print(f"P(A) = P(High Yield) = {high_yield_count}/{total_count} = {P_A:.3f}")
print(f"P(B) = P(South Region) = {south_count}/{total_count} = {P_B:.3f}")
print(f"P(A ∩ B) = {south_and_high_yield_count}/{total_count} = {south_and_high_yield_count/total_count:.3f}")
print(f"P(A|B) = {high_yield_south_count}/{south_count} = {P_A_given_B:.3f}")
print(f"P(B|A) = {south_and_high_yield_count}/{high_yield_count} = {P_B_given_A_empirical:.3f}")
print("8. STEP-BY-STEP BAYES' THEOREM VERIFICATION")
print("="*80)
print("""
Let's verify each step of Bayes' Theorem:
Step 1: Calculate all components""")
print(f"1. P(A) = P(High Yield) = {high_yield_count}/{total_count} = {P_A:.4f}")
print(f"2. P(B) = P(South Region) = {south_count}/{total_count} = {P_B:.4f}")
print(f"3. P(A|B) = P(High Yield | South) = {high_yield_south_count}/{south_count} = {P_A_given_B:.4f}")
print(f"\nStep 2: Apply Bayes' Theorem")
print(f"P(B|A) = [P(A|B) * P(B)] / P(A)")
print(f"       = [{P_A_given_B:.4f} * {P_B:.4f}] / {P_A:.4f}")
print(f"       = {P_A_given_B * P_B:.4f} / {P_A:.4f}")
print(f"       = {P_B_given_A_calculated:.4f}")
print(f"\nStep 3: Compare with empirical value")
print(f"Empirical P(B|A) = {south_and_high_yield_count}/{high_yield_count} = {P_B_given_A_empirical:.4f}")
print(f"\n**Conclusion: Bayes' Theorem is verified!**")
print(f"   Calculated: {P_B_given_A_calculated:.4f}")
print(f"   Empirical:  {P_B_given_A_empirical:.4f}")

"""**G. Task 5: Probability Distribution (Normal Only)**

**G1. Explore a Numerical Variable**
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
labels = ["100.00 - 118.00", "118.00 - 136.00", "136.00 - 154.00", "154.00 - 172.00",
          "172.00 - 190.00", "190.00 - 208.00", "208.00 - 226.00", "226.00 - 244.00",
          "244.00 - 262.00", "262.00 - 280.00", "280.00 - 298.00", "298.00 - 316.00",
          "316.00 - 334.00", "334.00 - 352.00", "352.00 - 370.00", "370.00 - 388.00",
          "388.00 - 406.00", "406.00 - 424.00", "424.00 - 442.00", "442.00 - 460.00",
          "460.00 - 478.00", "478.00 - 496.00", "496.00 - 514.00", "514.00 - 532.00",
          "532.00 - 550.00", "550.00 - 568.00", "568.00 - 586.00", "586.00 - 604.00",
          "604.00 - 622.00", "622.00 - 640.00", "640.00 - 658.00", "658.00 - 676.00",
          "676.00 - 694.00", "694.00 - 712.00", "712.00 - 730.00", "730.00 - 748.00",
          "748.00 - 766.00", "766.00 - 784.00", "784.00 - 802.00", "802.00 - 820.00",
          "820.00 - 838.00", "838.00 - 856.00", "856.00 - 874.00", "874.00 - 892.00",
          "892.00 - 910.00", "910.00 - 928.00", "928.00 - 946.00", "946.00 - 964.00",
          "964.00 - 982.00", "982.00 - 1000.00"]
counts = [20220, 20109, 20008, 19900, 19867, 19830, 20237, 20063, 20050, 20174,
          19953, 19667, 20148, 19693, 19959, 19954, 20004, 20195, 19920, 19987,
          19947, 20006, 19927, 19917, 20115, 19844, 19971, 20117, 20051, 19889,
          20087, 19788, 20177, 20139, 20270, 20124, 20009, 19972, 19860, 19833,
          20035, 19917, 20226, 20085, 19894, 19784, 19891, 20012, 20329, 19846]
midpoints = []
for label in labels:
    start, end = label.split(" - ")
    start = float(start)
    end = float(end.replace("00", "")) if "00" in end else float(end)
    midpoints.append((start + end) / 2)
data = []
for midpoint, count in zip(midpoints, counts):
    data.extend([midpoint] * count)
data = np.array(data)
μ = np.mean(data)
σ = np.std(data)
n = len(data)
print(f"Statistics for Numerical Variable (100-1000 range):")
print(f"Mean (μ): {μ:.2f}")
print(f"Standard Deviation (σ): {σ:.2f}")
print(f"Total Data Points (n): {n:,}")
print(f"Minimum Value: {np.min(data):.2f}")
print(f"Maximum Value: {np.max(data):.2f}")
plt.figure(figsize=(12, 8))
plt.hist(data, bins=50, density=True, alpha=0.6, color='blue', edgecolor='black',
         label=f'Actual Data (n={n:,})')
x = np.linspace(np.min(data) - 3*σ, np.max(data) + 3*σ, 1000)
y = norm.pdf(x, μ, σ)
plt.plot(x, y, 'r-', linewidth=2, label=f'Normal Distribution\nμ={μ:.2f}, σ={σ:.2f}')
plt.axvline(μ, color='red', linestyle='--', alpha=0.5, label=f'Mean = {μ:.2f}')
plt.axvspan(μ - σ, μ + σ, alpha=0.2, color='gray', label=f'±1σ ({μ-σ:.1f} to {μ+σ:.1f})')
plt.xlabel('Value', fontsize=12)
plt.ylabel('Density', fontsize=12)
plt.title(f'Histogram with Normal Distribution Overlay\nμ = {μ:.2f}, σ = {σ:.2f}, n = {n:,}', fontsize=14)
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)
textstr = '\n'.join((
    f'μ = {μ:.2f}',
    f'σ = {σ:.2f}',
    f'n = {n:,}',
    f'Range: {np.min(data):.1f} to {np.max(data):.1f}',
    f'68% within: {μ-σ:.1f} to {μ+σ:.1f}',
    f'95% within: {μ-2*σ:.1f} to {μ+2*σ:.1f}'))
props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)
plt.text(0.02, 0.98, textstr, transform=plt.gca().transAxes, fontsize=10,
         verticalalignment='top', bbox=props)
plt.tight_layout()
plt.show()
print("ADDITIONAL STATISTICS:")
print("="*50)
print(f"Variance (σ²): {σ**2:.2f}")
print(f"Coefficient of Variation: {(σ/μ)*100:.2f}%")
print(f"Skewness: {np.mean((data - μ)**3) / (σ**3):.4f}")
print(f"Kurtosis: {np.mean((data - μ)**4) / (σ**4) - 3:.4f}")
print(f"25th Percentile (Q1): {np.percentile(data, 25):.2f}")
print(f"50th Percentile (Median): {np.median(data):.2f}")
print(f"75th Percentile (Q3): {np.percentile(data, 75):.2f}")
print(f"IQR: {np.percentile(data, 75) - np.percentile(data, 25):.2f}")
expected_uniform_mean = (100 + 1000) / 2
expected_uniform_std = (1000 - 100) / np.sqrt(12)
print(f"\nExpected for Uniform Distribution [100, 1000]:")
print(f"  Expected Mean: {expected_uniform_mean:.2f}")
print(f"  Expected Std Dev: {expected_uniform_std:.2f}")
print(f"\nDifference from Uniform:")
print(f"  Mean difference: {μ - expected_uniform_mean:.2f}")
print(f"  Std Dev difference: {σ - expected_uniform_std:.2f}")

"""**G2. Normal Probability Questions**"""

import numpy as np
from scipy.stats import norm
labels = ["100.00 - 118.00", "118.00 - 136.00", "136.00 - 154.00", "154.00 - 172.00",
          "172.00 - 190.00", "190.00 - 208.00", "208.00 - 226.00", "226.00 - 244.00",
          "244.00 - 262.00", "262.00 - 280.00", "280.00 - 298.00", "298.00 - 316.00",
          "316.00 - 334.00", "334.00 - 352.00", "352.00 - 370.00", "370.00 - 388.00",
          "388.00 - 406.00", "406.00 - 424.00", "424.00 - 442.00", "442.00 - 460.00",
          "460.00 - 478.00", "478.00 - 496.00", "496.00 - 514.00", "514.00 - 532.00",
          "532.00 - 550.00", "550.00 - 568.00", "568.00 - 586.00", "586.00 - 604.00",
          "604.00 - 622.00", "622.00 - 640.00", "640.00 - 658.00", "658.00 - 676.00",
          "676.00 - 694.00", "694.00 - 712.00", "712.00 - 730.00", "730.00 - 748.00",
          "748.00 - 766.00", "766.00 - 784.00", "784.00 - 802.00", "802.00 - 820.00",
          "820.00 - 838.00", "838.00 - 856.00", "856.00 - 874.00", "874.00 - 892.00",
          "892.00 - 910.00", "910.00 - 928.00", "928.00 - 946.00", "946.00 - 964.00",
          "964.00 - 982.00", "982.00 - 1000.00"]
counts = [20220, 20109, 20008, 19900, 19867, 19830, 20237, 20063, 20050, 20174,
          19953, 19667, 20148, 19693, 19959, 19954, 20004, 20195, 19920, 19987,
          19947, 20006, 19927, 19917, 20115, 19844, 19971, 20117, 20051, 19889,
          20087, 19788, 20177, 20139, 20270, 20124, 20009, 19972, 19860, 19833,
          20035, 19917, 20226, 20085, 19894, 19784, 19891, 20012, 20329, 19846]
# Calculate midpoints
midpoints = []
for label in labels:
    start, end = label.split(" - ")
    start = float(start)
    end = float(end)
    midpoints.append((start + end) / 2)
# Create the full dataset
data = []
for midpoint, count in zip(midpoints, counts):
    data.extend([midpoint] * count)
data = np.array(data)
# Compute μ and σ
μ = np.mean(data)
σ = np.std(data)
print(f"Computed Statistics:")
print(f"Mean (μ) = {μ:.4f}")
print(f"Standard Deviation (σ) = {σ:.4f}")
print(f"Minimum value = {np.min(data):.2f}")
print(f"Maximum value = {np.max(data):.2f}")
print()
# 1. P(X > μ)
p_greater_than_mean = 1 - norm.cdf(μ, loc=μ, scale=σ)
print(f"1. P(X > μ) = P(X > {μ:.2f})")
print(f"   = 1 - Φ(({μ:.2f} - {μ:.2f})/{σ:.2f})")
print(f"   = 1 - Φ(0)")
print(f"   = 1 - 0.5")
print(f"   = 0.5 or 50%")
print()
# 2. P(μ - σ < X < μ + σ)
lower = μ - σ
upper = μ + σ
p_within_one_sigma = norm.cdf(upper, loc=μ, scale=σ) - norm.cdf(lower, loc=μ, scale=σ)
print(f"2. P(μ - σ < X < μ + σ)")
print(f"   = P({μ:.2f} - {σ:.2f} < X < {μ:.2f} + {σ:.2f})")
print(f"   = P({lower:.2f} < X < {upper:.2f})")
print(f"   = Φ(({upper:.2f} - {μ:.2f})/{σ:.2f}) - Φ(({lower:.2f} - {μ:.2f})/{σ:.2f})")
print(f"   = Φ(1) - Φ(-1)")
print(f"   = 0.8413 - 0.1587")
print(f"   = 0.6827 or 68.27%")
print()
# 3. P(X < μ - 2σ)
lower_bound = μ - 2*σ
p_below_two_sigma = norm.cdf(lower_bound, loc=μ, scale=σ)
print(f"3. P(X < μ - 2σ)")
print(f"   = P(X < {μ:.2f} - 2×{σ:.2f})")
print(f"   = P(X < {lower_bound:.2f})")
print(f"   = Φ(({lower_bound:.2f} - {μ:.2f})/{σ:.2f})")
print(f"   = Φ(-2)")
print(f"   = 0.0228 or 2.28%")

"""**G3. Are Your Data Normally Distributed?**"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm, skew, kurtosis
labels = ["100.00 - 118.00", "118.00 - 136.00", "136.00 - 154.00", "154.00 - 172.00",
          "172.00 - 190.00", "190.00 - 208.00", "208.00 - 226.00", "226.00 - 244.00",
          "244.00 - 262.00", "262.00 - 280.00", "280.00 - 298.00", "298.00 - 316.00",
          "316.00 - 334.00", "334.00 - 352.00", "352.00 - 370.00", "370.00 - 388.00",
          "388.00 - 406.00", "406.00 - 424.00", "424.00 - 442.00", "442.00 - 460.00",
          "460.00 - 478.00", "478.00 - 496.00", "496.00 - 514.00", "514.00 - 532.00",
          "532.00 - 550.00", "550.00 - 568.00", "568.00 - 586.00", "586.00 - 604.00",
          "604.00 - 622.00", "622.00 - 640.00", "640.00 - 658.00", "658.00 - 676.00",
          "676.00 - 694.00", "694.00 - 712.00", "712.00 - 730.00", "730.00 - 748.00",
          "748.00 - 766.00", "766.00 - 784.00", "784.00 - 802.00", "802.00 - 820.00",
          "820.00 - 838.00", "838.00 - 856.00", "856.00 - 874.00", "874.00 - 892.00",
          "892.00 - 910.00", "910.00 - 928.00", "928.00 - 946.00", "946.00 - 964.00",
          "964.00 - 982.00", "982.00 - 1000.00"]
counts = [20220, 20109, 20008, 19900, 19867, 19830, 20237, 20063, 20050, 20174,
          19953, 19667, 20148, 19693, 19959, 19954, 20004, 20195, 19920, 19987,
          19947, 20006, 19927, 19917, 20115, 19844, 19971, 20117, 20051, 19889,
          20087, 19788, 20177, 20139, 20270, 20124, 20009, 19972, 19860, 19833,
          20035, 19917, 20226, 20085, 19894, 19784, 19891, 20012, 20329, 19846]
midpoints = []
for label in labels:
    start, end = label.split(" - ")
    start = float(start)
    end = float(end)
    midpoints.append((start + end) / 2)
data = []
for midpoint, count in zip(midpoints, counts):
    data.extend([midpoint] * count)
data = np.array(data)
μ = np.mean(data)
σ = np.std(data)
median = np.median(data)
skewness = skew(data)
kurt = kurtosis(data)
print()
print("DESCRIPTIVE STATISTICS:")
print(f"Mean (μ): {μ:.4f}")
print(f"Median: {median:.4f}")
print(f"Standard Deviation (σ): {σ:.4f}")
print(f"Skewness: {skewness:.4f} (Normal ≈ 0)")
print(f"Kurtosis: {kurt:.4f} (Normal ≈ 0)")
print(f"Minimum: {np.min(data):.2f}")
print(f"Maximum: {np.max(data):.2f}")
print(f"Range: {np.max(data) - np.min(data):.2f}")
print()
print("NORMALITY INDICATORS:")
print("-" * 40)
print("1. MEAN vs MEDIAN:")
print(f"   Mean = {μ:.2f}, Median = {median:.2f}")
print(f"   Difference: {abs(μ - median):.2f}")
if abs(μ - median) < 0.01:
    print("   Mean ≈ Median (consistent with normality)")
else:
    print("   Mean ≠ Median (not consistent with normality)")
print()
print("2. SKEWNESS:")
print(f"   Value: {skewness:.4f}")
if abs(skewness) < 0.5:
    print(f"   Low skewness (|{skewness:.4f}| < 0.5)")
else:
    print(f"   Skewed distribution (|{skewness:.4f}| ≥ 0.5)")
print()
print("3. KURTOSIS:")
print(f"   Value: {kurt:.4f}")
if abs(kurt) < 0.5:
    print(f"   Normal kurtosis (|{kurt:.4f}| < 0.5)")
else:
    print(f"   Non-normal kurtosis (|{kurt:.4f}| ≥ 0.5)")
    if kurt < -0.5:
        print("   (Platykurtic - flatter than normal)")
    elif kurt > 0.5:
        print("   (Leptokurtic - more peaked than normal)")
print()
print("4. EMPIRICAL RULE CHECK:")
within_1σ = np.sum((data >= μ - σ) & (data <= μ + σ)) / len(data)
within_2σ = np.sum((data >= μ - 2*σ) & (data <= μ + 2*σ)) / len(data)
within_3σ = np.sum((data >= μ - 3*σ) & (data <= μ + 3*σ)) / len(data)
print("   Normal Distribution Expectation:")
print("   68% within μ ± σ, 95% within μ ± 2σ, 99.7% within μ ± 3σ")
print()
print(f"   Actual Data:")
print(f"   {within_1σ*100:.1f}% within μ ± σ ({μ-σ:.1f} to {μ+σ:.1f})")
print(f"   {within_2σ*100:.1f}% within μ ± 2σ ({μ-2*σ:.1f} to {μ+2*σ:.1f})")
print(f"   {within_3σ*100:.1f}% within μ ± 3σ ({μ-3*σ:.1f} to {μ+3*σ:.1f})")
print()
print("5. VISUAL ASSESSMENT:")
print("   Creating visualization...")
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
ax1 = axes[0, 0]
ax1.hist(data, bins=50, density=True, alpha=0.7, color='blue', edgecolor='black', label='Actual Data')
x = np.linspace(μ - 4*σ, μ + 4*σ, 1000)
y = norm.pdf(x, μ, σ)
ax1.plot(x, y, 'r-', linewidth=2, label='Normal Distribution')
ax1.axvline(μ, color='red', linestyle='--', alpha=0.5, label=f'Mean = {μ:.1f}')
ax1.set_xlabel('Value')
ax1.set_ylabel('Density')
ax1.set_title('Histogram vs Normal Distribution')
ax1.legend()
ax1.grid(True, alpha=0.3)
# Plot 2: Q-Q plot
ax2 = axes[0, 1]
from scipy.stats import probplot
probplot(data, dist="norm", plot=ax2)
ax2.set_title('Q-Q Plot (vs Normal Distribution)')
ax2.grid(True, alpha=0.3)
# Plot 3: Box plot
ax3 = axes[1, 0]
ax3.boxplot(data, vert=False, patch_artist=True)
ax3.set_xlabel('Value')
ax3.set_title('Box Plot (Check for outliers)')
ax3.grid(True, alpha=0.3)
# Plot 4: Cumulative distribution
ax4 = axes[1, 1]
sorted_data = np.sort(data)
y_vals = np.arange(1, len(sorted_data)+1) / len(sorted_data)
ax4.plot(sorted_data, y_vals, 'b-', linewidth=2, label='Empirical CDF')
# Theoretical normal CDF
x_norm = np.linspace(np.min(data), np.max(data), 1000)
y_norm = norm.cdf(x_norm, μ, σ)
ax4.plot(x_norm, y_norm, 'r--', linewidth=2, label='Normal CDF')
ax4.set_xlabel('Value')
ax4.set_ylabel('Cumulative Probability')
ax4.set_title('Cumulative Distribution Functions')
ax4.legend()
ax4.grid(True, alpha=0.3)
plt.suptitle(f'Normality Assessment for 100-1000 Variable (n={len(data):,})', fontsize=14)
plt.tight_layout()
plt.show()

"""**H. Task 6: Reflection**"""

import numpy as np
import pandas as pd
from scipy.stats import norm, chi2_contingency
import matplotlib.pyplot as plt
from scipy.stats import shapiro, kstest
sample_data = [
    {'Region': 'West', 'Soil': 'Sandy', 'Crop': 'Cotton'},{'Region': 'South', 'Soil': 'Clay', 'Crop': 'Rice'},{'Region': 'North', 'Soil': 'Loam', 'Crop': 'Barley'},
    {'Region': 'North', 'Soil': 'Sandy', 'Crop': 'Soybean'},{'Region': 'South', 'Soil': 'Silt', 'Crop': 'Wheat'},{'Region': 'South', 'Soil': 'Silt', 'Crop': 'Soybean'},
    {'Region': 'West', 'Soil': 'Clay', 'Crop': 'Wheat'},{'Region': 'South', 'Soil': 'Sandy', 'Crop': 'Rice'},{'Region': 'North', 'Soil': 'Silt', 'Crop': 'Wheat'},
    {'Region': 'West', 'Soil': 'Sandy', 'Crop': 'Wheat'},{'Region': 'North', 'Soil': 'Peaty', 'Crop': 'Wheat'} ]
df = pd.DataFrame(sample_data)
print("Marginal vs Conditional Probabilities:")
print()
print("Marginal (Overall) Probabilities:")
print(f"P(Rice) = {len(df[df['Crop'] == 'Rice'])/len(df):.2f}")
print(f"P(Wheat) = {len(df[df['Crop'] == 'Wheat'])/len(df):.2f}")
print(f"P(Clay) = {len(df[df['Soil'] == 'Clay'])/len(df):.2f}")
print()
print("Conditional Probabilities:")
print(f"P(Rice | Clay) = {len(df[(df['Crop'] == 'Rice') & (df['Soil'] == 'Clay')])/len(df[df['Soil'] == 'Clay']):.2f}")
print(f"P(Wheat | Silt) = {len(df[(df['Crop'] == 'Wheat') & (df['Soil'] == 'Silt')])/len(df[df['Soil'] == 'Silt']):.2f}")
print(f"P(Cotton | Sandy) = {len(df[(df['Crop'] == 'Cotton') & (df['Soil'] == 'Sandy')])/len(df[df['Soil'] == 'Sandy']):.2f}")
print()
print("Key Insight: Conditional probabilities differ significantly from marginal probabilities.")
print("This shows strong dependencies between variables.")
print()
print("2. INDEPENDENCE OF EVENTS")
print("-"*40)
contingency = pd.crosstab(df['Soil'], df['Crop'])
print("Contingency Table (Soil × Crop):")
print(contingency)
print()
chi2, p, dof, expected = chi2_contingency(contingency)
print(f"Chi-square test for independence (Soil vs Crop):")
print(f"  χ² = {chi2:.2f}, p-value = {p:.4f}")
print()
if p < 0.05:
    print("CONCLUSION: Events are NOT independent (p < 0.05)")
    print("   Soil type and crop choice are significantly dependent.")
else:
    print("CONCLUSION: Events are independent (p ≥ 0.05)")
print()
print("3. NORMAL DISTRIBUTION FIT")
print("-"*40)
print("Variable: -1.15 to 9.96 (appears bell-shaped)")
print()
labels_neg = [
    "-1.15 - -0.93", "-0.93 - -0.70", "-0.70 - -0.48", "-0.48 - -0.26",
    "-0.26 - -0.04", "-0.04 - 0.19", "0.19 - 0.41", "0.41 - 0.63",
    "0.63 - 0.85", "0.85 - 1.07", "1.07 - 1.30", "1.30 - 1.52",
    "1.52 - 1.74", "1.74 - 1.96", "1.96 - 2.19", "2.19 - 2.41",
    "2.41 - 2.63", "2.63 - 2.85", "2.85 - 3.07", "3.07 - 3.30",
    "3.30 - 3.52", "3.52 - 3.74", "3.74 - 3.96", "3.96 - 4.19",
    "4.19 - 4.41", "4.41 - 4.63", "4.63 - 4.85", "4.85 - 5.07",
    "5.07 - 5.30", "5.30 - 5.52", "5.52 - 5.74", "5.74 - 5.96",
    "5.96 - 6.19", "6.19 - 6.41", "6.41 - 6.63", "6.63 - 6.85",
    "6.85 - 7.07", "7.07 - 7.30", "7.30 - 7.52", "7.52 - 7.74",
    "7.74 - 7.96", "7.96 - 8.19", "8.19 - 8.41", "8.41 - 8.63",
    "8.63 - 8.85", "8.85 - 9.07", "9.07 - 9.30", "9.30 - 9.52",
    "9.52 - 9.74", "9.74 - 9.96" ]
counts_neg = [
    1, 5, 14, 40, 130, 363, 951, 1915, 3476, 5421,
    7607, 10234, 12739, 15401, 19148, 22759, 26915, 30814, 33967, 37015,
    39237, 41487, 44105, 46243, 47299, 47960, 48556, 47777, 46342, 44825,
    42102, 40001, 37250, 34439, 31402, 27882, 23376, 19459, 16117, 12986,
    10584, 8041, 5950, 3745, 2188, 1077, 439, 161, 47, 8 ]
neg_data = []
for label, count in zip(labels_neg, counts_neg):
    start, end = label.split(" - ")
    start = float(start)
    end = float(end)
    midpoint = (start + end) / 2
    neg_data.extend([midpoint] * count)
neg_data = np.array(neg_data)
neg_mean = np.mean(neg_data)
neg_std = np.std(neg_data)
neg_skew = (np.mean((neg_data - neg_mean)**3)) / (neg_std**3)
neg_kurt = (np.mean((neg_data - neg_mean)**4)) / (neg_std**4) - 3
print("Statistics:")
print(f"  Mean: {neg_mean:.4f}")
print(f"  Standard Deviation: {neg_std:.4f}")
print(f"  Skewness: {neg_skew:.4f} (Normal ≈ 0)")
print(f"  Kurtosis: {neg_kurt:.4f} (Normal ≈ 0)")
print()
sample_for_test = np.random.choice(neg_data, min(5000, len(neg_data)), replace=False)
stat, p_shapiro = shapiro(sample_for_test)
print(f"Shapiro-Wilk Test (normality):")
print(f"  Statistic = {stat:.4f}, p-value = {p_shapiro:.6f}")
if p_shapiro > 0.05:
    print("  Cannot reject normality (p > 0.05)")
else:
    print("  Reject normality (p ≤ 0.05)")
ks_stat, p_ks = kstest(neg_data, 'norm', args=(neg_mean, neg_std))
print(f"\nKolmogorov-Smirnov Test:")
print(f"  Statistic = {ks_stat:.4f}, p-value = {p_ks:.6f}")
if p_ks > 0.05:
    print("  Cannot reject normality (p > 0.05)")
else:
    print("  Reject normality (p ≤ 0.05)")
fig = plt.figure(figsize=(16, 10))
ax1 = plt.subplot(2, 2, 1)
ax1.hist(neg_data, bins=30, density=True, alpha=0.7, color='green', edgecolor='black')
x = np.linspace(neg_mean - 4*neg_std, neg_mean + 4*neg_std, 1000)
y = norm.pdf(x, neg_mean, neg_std)
ax1.plot(x, y, 'r-', linewidth=2)
ax1.set_xlabel('Value')
ax1.set_ylabel('Density')
ax1.set_title('Normal Distribution Fit (-1.15 to 9.96 variable)')
ax1.text(0.02, 0.98, f'μ = {neg_mean:.2f}\nσ = {neg_std:.2f}',
         transform=ax1.transAxes, verticalalignment='top',
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
ax2 = plt.subplot(2, 2, 2)
soil_crop_counts = contingency.values
crops = contingency.columns
soils = contingency.index
x_pos = np.arange(len(soils))
bar_width = 0.8/len(crops)
for i, crop in enumerate(crops):
    ax2.bar(x_pos + i*bar_width, soil_crop_counts[:, i], width=bar_width, label=crop)
ax2.set_xlabel('Soil Type')
ax2.set_ylabel('Count')
ax2.set_title('Soil-Crop Dependencies (Sample Data)')
ax2.set_xticks(x_pos + bar_width*(len(crops)-1)/2)
ax2.set_xticklabels(soils)
ax2.legend()
ax2.text(0.02, 0.98, f'χ² p-value = {p:.4f}', transform=ax2.transAxes,
         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
ax3 = plt.subplot(2, 2, 3)
decision_data = {
    'Optimal': [0.85, 0.70, 0.60],
    'Suboptimal': [0.60, 0.50, 0.40],
    'Poor': [0.30, 0.20, 0.10] }
conditions = ['Good Weather', 'Average', 'Poor Weather']
x = np.arange(len(conditions))
width = 0.25
multiplier = 0
for attribute, measurement in decision_data.items():
    offset = width * multiplier
    rects = ax3.bar(x + offset, measurement, width, label=attribute)
    multiplier += 1
ax3.set_ylabel('Crop Success Probability')
ax3.set_title('Decision Making: Weather Impact on Crop Success')
ax3.set_xticks(x + width)
ax3.set_xticklabels(conditions)
ax3.legend(loc='upper right')
ax3.set_ylim(0, 1)
# Plot 4: Risk assessment
ax4 = plt.subplot(2, 2, 4)
risk_levels = ['Low (< μ-σ)', 'Medium (μ±σ)', 'High (> μ+σ)']
probabilities = [0.1587, 0.6827, 0.1587]  # Normal distribution areas
colors = ['green', 'yellow', 'red']
ax4.bar(risk_levels, probabilities, color=colors, edgecolor='black')
ax4.set_ylabel('Probability')
ax4.set_title('Risk Assessment Using Normal Distribution')
ax4.set_ylim(0, 1)
for i, (level, prob) in enumerate(zip(risk_levels, probabilities)):
    ax4.text(i, prob + 0.02, f'{prob*100:.1f}%', ha='center')
plt.suptitle('Summary: Probability and Distribution Applications in Agriculture', fontsize=14)
plt.tight_layout()
plt.show()
sample_yields = [897.08, 992.67, 148.00, 986.87, 730.38, 797.47, 357.90, 441.13, 181.59, 395.05, 385.14]
expected_yield = np.mean(sample_yields)
yield_std = np.std(sample_yields)
yield_cv = (yield_std / expected_yield) * 100
print(f"Expected Yield (sample): {expected_yield:.2f} ± {yield_std:.2f}")
print(f"Coefficient of Variation: {yield_cv:.1f}% (High variability → High risk)")
high_yield_prob = len([y for y in sample_yields if y > 800]) / len(sample_yields)
print(f"Probability of High Yield (>800): {high_yield_prob:.2f}")
soil_yield_pairs = {
    'Sandy': [897.08, 986.87, 441.13, 395.05],
    'Clay': [992.67, 357.90],
    'Loam': [148.00],
    'Silt': [730.38, 797.47, 181.59],
    'Peaty': [385.14]
}
print("\nExpected Yield by Soil Type:")
for soil, yields in soil_yield_pairs.items():
    if yields:
        exp_yield = np.mean(yields)
        print(f"  {soil}: {exp_yield:.2f} (n={len(yields)})")

"""**I. Submission Guidelines**"""

from IPython.display import Image
Image(filename='/content/q.jpeg')

"""**[Milestone 7](https://)**

**A. Introduction**
"""

import numpy as np
x_data = [
    897.0772391101236,  # Row 1
    992.6732816189208,  # Row 2
    147.9980252926104,  # Row 3
    986.8663313367325,  # Row 4
    730.379174445627,   # Row 5
    797.4711823962564,  # Row 6
    357.90235724297685, # Row 7
    441.13115357285005, # Row 8
    181.5878606243205,  # Row 9
    385.13531445805074  # Row 10
    ]
y_data = [
    27.676966373377603,  # Row 1
    18.02614225436302,   # Row 2
    29.79404241557257,   # Row 3
    16.64419019137728,   # Row 4
    31.620687370805797,  # Row 5
    37.70497446941277,   # Row 6
    31.59343138976995,   # Row 7
    30.88710699523619,   # Row 8
    26.752728580811905,  # Row 9
    21.656191734884608   # Row 10
]
X = np.array(x_data)
Y = np.array(y_data)
n = len(X)
sum_x = np.sum(X)
sum_y = np.sum(Y)
sum_xy = np.sum(X * Y)
sum_x_squared = np.sum(X ** 2)
sum_y_squared = np.sum(Y ** 2)
x_mean = np.mean(X)
y_mean = np.mean(Y)
numerator = n * sum_xy - sum_x * sum_y
denominator = n * sum_x_squared - sum_x ** 2
beta = numerator / denominator
alpha = y_mean - beta * x_mean
ss_total = np.sum((Y - y_mean) ** 2)
ss_residual = np.sum((Y - (alpha + beta * X)) ** 2)
r_squared = 1 - (ss_residual / ss_total)
correlation = beta * (np.std(X) / np.std(Y))
print("SIMPLE LINEAR REGRESSION ANALYSIS")
print("=" * 50)
print(f"Number of data points: {n}")
print(f"X variable range: {min(X):.2f} to {max(X):.2f}")
print(f"Y variable range: {min(Y):.2f} to {max(Y):.2f}")
print("REGRESSION RESULTS:")
print("=" * 50)
print(f"Intercept (α): {alpha:.4f}")
print(f"Slope (β): {beta:.6f}")
print(f"\nRegression Equation: Y = {alpha:.4f} + ({beta:.6f}) * X")
print("GOODNESS OF FIT:")
print("=" * 50)
print(f"R-squared (R²): {r_squared:.6f}")
print(f"Correlation coefficient (r): {correlation:.6f}")
print(f"Standard deviation of X: {np.std(X):.4f}")
print(f"Standard deviation of Y: {np.std(Y):.4f}")
print("EXAMPLE PREDICTIONS:")
print("=" * 50)
test_x_values = [min(X), x_mean, max(X)]
for x_val in test_x_values:
    y_pred = alpha + beta * x_val
    print(f"X = {x_val:7.2f} \u2192 Predicted Y = {y_pred:6.2f}")
print("ADDITIONAL STATISTICS:")
print("=" * 50)
print(f"Sum of X: {sum_x:.2f}")
print(f"Sum of Y: {sum_y:.2f}")
print(f"Sum of XY: {sum_xy:.2f}")
print(f"Sum of X\u00b2: {sum_x_squared:.2f}")
print(f"Mean of X: {x_mean:.2f}")
print(f"Mean of Y: {y_mean:.2f}")
y_pred_all = alpha + beta * X
residuals = Y - y_pred_all
print(f"\nSum of squared residuals: {np.sum(residuals**2):.4f}")
print(f"Mean of residuals: {np.mean(residuals):.6f}")
print("DATA POINTS WITH PREDICTIONS:")
print(f"{'Index':<6} {'X':<12} {'Y':<12} {'Y_pred':<12} {'Residual':<12}")
print("-" * 60)
for i in range(n):
    y_pred = alpha + beta * X[i]
    residual = Y[i] - y_pred
    print(f"{i+1:<6} {X[i]:<12.2f} {Y[i]:<12.2f} {y_pred:<12.2f} {residual:<12.4f}")

"""**B. Knowledge Points: The Least Squares Method**"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List, Dict
class SimpleLinearRegression:
    """
    Manual implementation of Simple Linear Regression
    using the Least Squares method
    """
    def __init__(self):
        self.beta_0 = None  # Intercept
        self.beta_1 = None  # Slope
        self.X_mean = None
        self.Y_mean = None
        self.r_squared = None
        self.sse = None  # Sum of Squared Errors
        self.ssr = None  # Sum of Squared Regression
        self.sst = None  # Total Sum of Squares
    def fit(self, X: np.ndarray, Y: np.ndarray) -> None:
        """
        Fit the linear regression model using Least Squares
        Parameters:
        -----------
        X : Independent variable (1D array)
        Y : Dependent variable (1D array)
        """
        if len(X) != len(Y):
            raise ValueError("X and Y must have the same length")
        n = len(X)
        sum_X = np.sum(X)
        sum_Y = np.sum(Y)
        sum_XY = np.sum(X * Y)
        sum_X2 = np.sum(X ** 2)
        sum_Y2 = np.sum(Y ** 2)
        self.X_mean = np.mean(X)
        self.Y_mean = np.mean(Y)
        numerator = n * sum_XY - sum_X * sum_Y
        denominator = n * sum_X2 - sum_X ** 2

        if denominator == 0:
            raise ValueError("Denominator is zero, cannot compute slope")
        self.beta_1 = numerator / denominator
        self.beta_0 = self.Y_mean - self.beta_1 * self.X_mean
        self._calculate_goodness_of_fit(X, Y)
    def _calculate_goodness_of_fit(self, X: np.ndarray, Y: np.ndarray) -> None:
        """Calculate R-squared and other statistics"""
        Y_pred = self.predict(X)
        self.sst = np.sum((Y - self.Y_mean) ** 2)  # Total Sum of Squares
        self.ssr = np.sum((Y_pred - self.Y_mean) ** 2)  # Regression Sum of Squares
        self.sse = np.sum((Y - Y_pred) ** 2)  # Error Sum of Squares
        self.r_squared = self.ssr / self.sst if self.sst != 0 else 0
    def predict(self, X: np.ndarray) -> np.ndarray:
        """Make predictions using the fitted model"""
        if self.beta_0 is None or self.beta_1 is None:
            raise ValueError("Model must be fitted before prediction")
        return self.beta_0 + self.beta_1 * X
    def residuals(self, X: np.ndarray, Y: np.ndarray) -> np.ndarray:
        """Calculate residuals (errors)"""
        Y_pred = self.predict(X)
        return Y - Y_pred
    def get_parameters(self) -> Dict:
        """Return model parameters and statistics"""
        return {
            'intercept': self.beta_0,
            'slope': self.beta_1,
            'r_squared': self.r_squared,
            'sse': self.sse,
            'ssr': self.ssr,
            'sst': self.sst,
            'x_mean': self.X_mean,
            'y_mean': self.Y_mean
        }
    def standard_errors(self, X: np.ndarray, Y: np.ndarray) -> Dict:
        """
        Calculate standard errors for parameters
        assuming homoscedasticity
        """
        n = len(X)
        residuals = self.residuals(X, Y)
        mse = self.sse / (n - 2)  # Mean Squared Error
        x_variance = np.var(X, ddof=1)
        se_slope = np.sqrt(mse / ((n - 1) * x_variance))
        se_intercept = np.sqrt(mse * (1/n + self.X_mean**2 / ((n - 1) * x_variance)))
        return {
            'se_intercept': se_intercept,
            'se_slope': se_slope,
            'mse': mse,
            'rmse': np.sqrt(mse)
        }
def load_sample_data() -> Tuple[np.ndarray, np.ndarray]:
    """Load the sample data provided"""
    X = np.array([
        897.0772391101236,   # Row 1
        992.6732816189208,   # Row 2
        147.9980252926104,   # Row 3
        986.8663313367325,   # Row 4
        730.379174445627,    # Row 5
        797.4711823962564,   # Row 6
        357.90235724297685,  # Row 7
        441.13115357285005,  # Row 8
        181.5878606243205,   # Row 9
        385.13531445805074   # Row 10
    ])
    Y = np.array([
        27.676966373377603,  # Row 1
        18.02614225436302,   # Row 2
        29.79404241557257,   # Row 3
        16.64419019137728,   # Row 4
        31.620687370805797,  # Row 5
        37.70497446941277,   # Row 6
        31.59343138976995,   # Row 7
        30.88710699523619,   # Row 8
        26.752728580811905,  # Row 9
        21.656191734884608   # Row 10
    ])

    return X, Y
def manual_calculation_demo(X: np.ndarray, Y: np.ndarray) -> None:
    """Show step-by-step manual calculation"""
    print("MANUAL CALCULATION OF LINEAR REGRESSION PARAMETERS")
    print("=" * 60)
    n = len(X)
    print(f"n (number of observations) = {n}")
    print()
    sum_X = np.sum(X)
    sum_Y = np.sum(Y)
    sum_XY = np.sum(X * Y)
    sum_X2 = np.sum(X ** 2)
    sum_Y2 = np.sum(Y ** 2)
    print("Step 1: Calculate sums")
    print(f"  ΣX = {sum_X:.4f}")
    print(f"  ΣY = {sum_Y:.4f}")
    print(f"  ΣXY = {sum_XY:.4f}")
    print(f"  ΣX² = {sum_X2:.4f}")
    print(f"  ΣY² = {sum_Y2:.4f}")
    print()
    X_mean = np.mean(X)
    Y_mean = np.mean(Y)
    print("Step 2: Calculate means")
    print(f"  X̄ = ΣX/n = {sum_X:.4f}/{n} = {X_mean:.4f}")
    print(f"  Ȳ = ΣY/n = {sum_Y:.4f}/{n} = {Y_mean:.4f}")
    print()
    numerator = n * sum_XY - sum_X * sum_Y
    denominator = n * sum_X2 - sum_X ** 2
    beta_1 = numerator / denominator
    print("Step 3: Calculate slope (β1)")
    print(f"  β1 = (n·ΣXY - ΣX·ΣY) / (n·ΣX² - (ΣX)²)")
    print(f"     = ({n}×{sum_XY:.4f} - {sum_X:.4f}×{sum_Y:.4f}) / ({n}×{sum_X2:.4f} - {sum_X:.4f}²)")
    print(f"     = {numerator:.4f} / {denominator:.4f}")
    print(f"     = {beta_1:.6f}")
    print()
    beta_0 = Y_mean - beta_1 * X_mean
    print("Step 4: Calculate intercept (β0)")
    print(f"  β0 = Ȳ - β1·X̄")
    print(f"     = {Y_mean:.4f} - {beta_1:.6f}×{X_mean:.4f}")
    print(f"     = {beta_0:.4f}")
    print()
    Y_pred = beta_0 + beta_1 * X
    SST = np.sum((Y - Y_mean) ** 2)
    SSR = np.sum((Y_pred - Y_mean) ** 2)
    SSE = np.sum((Y - Y_pred) ** 2)
    r_squared = SSR / SST
    print("Step 5: Calculate R-squared")
    print(f"  SST = Σ(Y - Ȳ)² = {SST:.4f}")
    print(f"  SSR = Σ(Ŷ - Ȳ)² = {SSR:.4f}")
    print(f"  SSE = Σ(Y - Ŷ)² = {SSE:.4f}")
    print(f"  R² = SSR/SST = {SSR:.4f}/{SST:.4f} = {r_squared:.6f}")
    print()
    print(f"FINAL REGRESSION EQUATION: Ŷ = {beta_0:.4f} + {beta_1:.6f}X")
    print("=" * 60)
def main():
    X, Y = load_sample_data()
    manual_calculation_demo(X, Y)
    print("USING THE SIMPLE LINEAR REGRESSION CLASS")
    print("=" * 60)
    model = SimpleLinearRegression()
    model.fit(X, Y)
    params = model.get_parameters()
    print(f"Intercept (β0): {params['intercept']:.4f}")
    print(f"Slope (β1): {params['slope']:.6f}")
    print(f"R-squared: {params['r_squared']:.6f}")
    print(f"Sum of Squared Errors (SSE): {params['sse']:.4f}")
    print(f"Sum of Squared Regression (SSR): {params['ssr']:.4f}")
    print(f"Total Sum of Squares (SST): {params['sst']:.4f}")
    se = model.standard_errors(X, Y)
    print(f"\nStandard Error of Intercept: {se['se_intercept']:.4f}")
    print(f"Standard Error of Slope: {se['se_slope']:.6f}")
    print(f"Mean Squared Error (MSE): {se['mse']:.4f}")
    print(f"Root Mean Squared Error (RMSE): {se['rmse']:.4f}")
    print("PREDICTIONS FOR SAMPLE X VALUES")
    print("=" * 60)
    for i, x_val in enumerate(X):
        y_pred = model.predict(np.array([x_val]))
        residual = Y[i] - y_pred
        print(f"X = {x_val:8.2f}, Actual Y = {Y[i]:6.2f}, Predicted Y = {y_pred[0]:6.2f}, Residual = {residual[0]:6.2f}")
    plot_results(X, Y, model)
    print("ADDITIONAL ANALYSIS")
    print("=" * 60)
    check_assumptions(X, Y, model)
    print("\n95% Confidence Intervals:")
    t_critical = 2.306  # For n-2=8 degrees of freedom, alpha=0.05
    ci_intercept_lower = params['intercept'] - t_critical * se['se_intercept']
    ci_intercept_upper = params['intercept'] + t_critical * se['se_intercept']
    print(f"  Intercept: ({ci_intercept_lower:.4f}, {ci_intercept_upper:.4f})")
    ci_slope_lower = params['slope'] - t_critical * se['se_slope']
    ci_slope_upper = params['slope'] + t_critical * se['se_slope']
    print(f"  Slope: ({ci_slope_lower:.6f}, {ci_slope_upper:.6f})")
def plot_results(X: np.ndarray, Y: np.ndarray, model: SimpleLinearRegression) -> None:
    """Create visualization of the regression results"""
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.scatter(X, Y, color='blue', alpha=0.7, label='Actual Data')
    x_line = np.linspace(min(X), max(X), 100)
    y_line = model.predict(x_line)
    plt.plot(x_line, y_line, color='red', linewidth=2, label='Regression Line')
    params = model.get_parameters()
    plt.axhline(y=params['y_mean'], color='green', linestyle='--', alpha=0.5, label='Mean of Y')
    plt.axvline(x=params['x_mean'], color='orange', linestyle='--', alpha=0.5, label='Mean of X')
    plt.xlabel('Independent Variable (X)')
    plt.ylabel('Dependent Variable (Y)')
    plt.title(f'Simple Linear Regression\nŶ = {params["intercept"]:.2f} + {params["slope"]:.4f}X, R² = {params["r_squared"]:.3f}')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.subplot(1, 2, 2)
    residuals = model.residuals(X, Y)
    plt.scatter(model.predict(X), residuals, color='purple', alpha=0.7)
    plt.axhline(y=0, color='red', linestyle='--', alpha=0.5)
    plt.xlabel('Predicted Values')
    plt.ylabel('Residuals')
    plt.title('Residual Plot')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
def check_assumptions(X: np.ndarray, Y: np.ndarray, model: SimpleLinearRegression) -> None:
    """Check linear regression assumptions"""
    residuals = model.residuals(X, Y)
    y_pred = model.predict(X)
    print("Checking Regression Assumptions:")
    print("-" * 40)
    print("1. Linearity: Visual check (see scatter plot)")
    print("2. Independence of errors: Assumed (cross-sectional data)")
    residuals_variance = np.var(residuals)
    print(f"3. Homoscedasticity: Residual variance = {residuals_variance:.4f}")
    from scipy import stats
    shapiro_test = stats.shapiro(residuals)
    print(f"4. Normality of residuals (Shapiro-Wilk test):\n   Test statistic = {shapiro_test.statistic:.4f}, p-value = {shapiro_test.pvalue:.4f}")
    print("5. No multicollinearity: Only one predictor variable")
if __name__ == "__main__":
    main()